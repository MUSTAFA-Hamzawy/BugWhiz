{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m class_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowner\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual class column name\u001b[39;00m\n\u001b[1;32m     99\u001b[0m summary_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the actual summary column name\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[43mpreprocess_and_save_for_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_pickle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_column\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m, in \u001b[0;36mpreprocess_and_save_for_transformer\u001b[0;34m(input_csv, output_pickle, class_column, summary_column)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m     30\u001b[0m   \u001b[38;5;66;03m# Get class and summary text\u001b[39;00m\n\u001b[1;32m     31\u001b[0m   label \u001b[38;5;241m=\u001b[39m row[class_column]\n\u001b[0;32m---> 32\u001b[0m   summary_text \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43msummary_column\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     34\u001b[0m   \u001b[38;5;66;03m# Preprocess text\u001b[39;00m\n\u001b[1;32m     35\u001b[0m   processed_text \u001b[38;5;241m=\u001b[39m preprocess_text(summary_text, stop_words, stemmer)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Summary'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "def preprocess_and_save_for_transformer(input_csv, output_pickle, class_column, summary_column):\n",
    "  \"\"\"\n",
    "  Preprocesses text from a CSV file for a transformer model, \n",
    "  including tokenization, stemming, stop word removal, and saves to a pickle file.\n",
    "\n",
    "  Args:\n",
    "      input_csv (str): Path to the input CSV file.\n",
    "      output_pickle (str): Path to the output pickle file.\n",
    "      class_column (str): Name of the column containing class labels.\n",
    "      summary_column (str): Name of the column containing summaries.\n",
    "  \"\"\"\n",
    "\n",
    "  # Load stop words and stemmer\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  stemmer = PorterStemmer()\n",
    "\n",
    "  # Initialize empty lists for tokens and labels\n",
    "  tokens = []\n",
    "  labels = []\n",
    "\n",
    "  with open(input_csv, 'r', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "\n",
    "    for row in reader:\n",
    "      # Get class and summary text\n",
    "      label = row[class_column]\n",
    "      summary_text = row[summary_column]\n",
    "\n",
    "      # Preprocess text\n",
    "      processed_text = preprocess_text(summary_text, stop_words, stemmer)\n",
    "\n",
    "      # Tokenize using a BERT tokenizer\n",
    "      tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "      tokenized_text = tokenizer.encode(processed_text, add_special_tokens=True)\n",
    "\n",
    "      # Add tokens and label to lists\n",
    "      tokens.append(tokenized_text)\n",
    "      labels.append(label)\n",
    "\n",
    "  # Save preprocessed data to pickle file\n",
    "  data = {'tokens': tokens, 'labels': labels}\n",
    "  with open(output_pickle, 'wb') as handle:\n",
    "    import pickle\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "  print(f\"Preprocessed data saved to: {output_pickle}\")\n",
    "\n",
    "\n",
    "def preprocess_text(text, stop_words, stemmer):\n",
    "  \"\"\"\n",
    "  Preprocesses text for tokenization, including lowercasing, stemming, and stop word removal.\n",
    "\n",
    "  Args:\n",
    "      text (str): Text to be preprocessed.\n",
    "      stop_words (set): Set of stop words to remove.\n",
    "      stemmer (nltk.stem.PorterStemmer): Stemmer object for stemming words.\n",
    "\n",
    "  Returns:\n",
    "      str: Preprocessed text.\n",
    "  \"\"\"\n",
    "\n",
    "  # Lowercase text\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove punctuation\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "  # Tokenize text (split into words)\n",
    "  words = text.split()\n",
    "\n",
    "  # Remove stop words\n",
    "  filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "  # Apply stemming (optional)\n",
    "  stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "  # Join words back into text\n",
    "  processed_text = ' '.join(stemmed_words)\n",
    "\n",
    "  return processed_text\n",
    "\n",
    "\n",
    "# Get the base filename and path\n",
    "base_filename = \"merged_file\"\n",
    "\n",
    "# Define file extensions\n",
    "extensions = [\"_part_1.csv\", \"_part_2.csv\", \"_part_3.csv\", \"_part_4.csv\"]\n",
    "\n",
    "# Loop over the files\n",
    "for ext in extensions:\n",
    "    input_csv = f\"{base_filename}{ext}\"\n",
    "    output_pickle = f\"preprocessed_data_{ext[:-4]}.pickle\"  # Remove \".csv\" from output filename\n",
    "    class_column = \"owner\"  # Replace with the actual class column name\n",
    "    summary_column = \"Summary\"  # Replace with the actual summary column name\n",
    "\n",
    "    preprocess_and_save_for_transformer(input_csv, output_pickle, class_column, summary_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'merged_file_part_1.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV file:\", column_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
