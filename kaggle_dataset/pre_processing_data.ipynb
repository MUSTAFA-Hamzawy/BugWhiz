{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOUh0894Z2QM",
    "outputId": "8a05336a-818f-4882-b110-624020d0333e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the original file: 118644\n"
     ]
    }
   ],
   "source": [
    "file_path = 'classifier_data_0.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the original file:\", line_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the classifier_data_0 file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'classifier_data_0.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the classifier_data_0 file:\", column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique owners in classifier_data_0 file: 2564\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def get_unique_owner_count(csv_file):\n",
    "    unique_owners = set()\n",
    "    with open(csv_file, 'rb') as file:\n",
    "        try:\n",
    "            filtered_lines = (line.decode('utf-8-sig') for line in file if b'\\x00' not in line)\n",
    "            csv_reader = csv.DictReader(filtered_lines)\n",
    "            for row in csv_reader:\n",
    "                unique_owners.add(row['owner'])\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "    return len(unique_owners)\n",
    "\n",
    "csv_file = 'classifier_data_0.csv'\n",
    "unique_owner_count = get_unique_owner_count(csv_file)\n",
    "print(\"Number of unique owners in classifier_data_0 file:\", unique_owner_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 elements of the 'description' column:\n",
      "Product Version      : <see about:version>URLs (if applicable) :0.2.149.27Other browsers tested: Firefox / IEAdd OK or FAIL after other browsers where you have tested this issue:Safari 3:    Firefox 3: OK         IE 7:OKWhat steps will reproduce the problem?1. Open any webpage on compaq 6715s running vista.2. Try scrolling with the touchpad3. Scrolling down will work , but up will not.What is the expected result?The page to scroll up.What happens instead?The page doesn't move.Please provide any additional information below. Attach a screenshot if possible.Only a minor bug. \n",
      "Product Version      : 0.2.149.27 (1583)URLs (if applicable) : http://www.igoogle.com,http://code.google.com/p/chromiumOther browsers tested:Add OK or FAIL after other browsers where you have tested this issue:Safari 3:    Firefox 3: OK         IE 7: OKWhat steps will reproduce the problem?1. Load http://www.igoogle.com/ (or any other google account page)2. Click the Sign In link at top right3. Kaboom.What is the expected result?Should see the sign-in screenWhat happens instead?Don't see nuffin'.  Just a white screenPlease provide any additional information below. Attach a screenshot ifpossible.* Using Windows Vista Enterprise.* I am connecting to the internet through a proxy server (I'm at work);could possibly be that?  Not sure what else to configure since Chrome seemsto use Vista's standard Internet Options (and those are configuredcorrectly, at least as far as IE and FireFox are concerned).  I have noticed that I get a password prompt from my Proxy server whenever Istart Chrome.  Will try downloading Chrome at home tonight to see if the problem persists. \n",
      "Product Version      : chrome beta 1URLs (if applicable) :Other browsers tested:Add OK or FAIL after other browsers where you have tested this issue:     Safari 3: OK    Firefox 3: irrelevant         IE 7: irrelevantWhat steps will reproduce the problem?1. right-click on a web element2. click on \"inspect element\"3. click on \"dock to main window\"What is the expected result?To have the web-inspector get docked at the bottom of the current tab.What happens instead?Nothing.Please provide any additional information below. Attach a screenshot if possible. \n",
      "Product Version      : 0.2.149.27 (1583)URLs (if applicable) :Other browsers tested:Add OK or FAIL after other browsers where you have tested this issue:     Safari 3: OK    Firefox 3: OK         IE 7: N/AWhat steps will reproduce the problem?1. Log in to Habari's admin interface2. Notice the incorrect rendering of the rounded input boxes upon login3. Hover over the menu in the upper left corner and notice the opaque blur around it. Looks really strange.What is the expected result?Smooth graphics.What happens instead?Weird graphics.Please provide any additional information below. Attach a screenshot if possible.Screenshot attached. Black squares obviously for obfuscation. \n",
      "Product Version      : 0.2.149.27URLs (if applicable) :What steps will reproduce the problem?1. Open Chrome on the primary monitor which has a smaller resolution2. Move Chrome to the second larger resolution monitor.3. Try to maximize and will only maximize to the resolution of the first smaller monitorWhat is the expected result?Maximize to the full resolution of the monitor that it is on. \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def print_title_elements(csv_file, num_elements=5):\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        print(f\"First {num_elements} elements of the 'description' column:\")\n",
    "        for row in csv_reader:\n",
    "            print(row['description'])\n",
    "            num_elements -= 1\n",
    "            if num_elements == 0:\n",
    "                break\n",
    "\n",
    "csv_file = 'classifier_data_0.csv'\n",
    "print_title_elements(csv_file, num_elements=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 elements of the 'title' column:\n",
      "Scrolling with some scroll mice (touchpad, etc.) scrolls down but not up\n",
      "Proxy causes some or all network requests to fail\n",
      "Web inspector button \"dock to main window\" does nothing\n",
      "Habari admin interface is not rendered correctly\n",
      "Maximize on second larger monitor not working\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def print_title_elements(csv_file, num_elements=5):\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        print(f\"First {num_elements} elements of the 'title' column:\")\n",
    "        for row in csv_reader:\n",
    "            print(row['issue_title'])\n",
    "            num_elements -= 1\n",
    "            if num_elements == 0:\n",
    "                break\n",
    "\n",
    "csv_file = 'classifier_data_0.csv'\n",
    "print_title_elements(csv_file, num_elements=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows saved to 'output_csv_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def filter_non_empty_rows(input_csv, output_csv):\n",
    "    with open(input_csv, 'r', encoding='utf-8-sig') as file:\n",
    "        lines = file.readlines()\n",
    "        # Filter out lines containing NUL characters\n",
    "        lines = [line for line in lines if '\\0' not in line]\n",
    "        \n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        csv_writer = csv.writer(output_file)\n",
    "        csv_writer.writerow(['owner', 'issue_title'])  # Write header\n",
    "        \n",
    "        for line in lines:\n",
    "            row = line.strip().split(',')  # Assuming CSV is comma-separated\n",
    "            if row[0] and row[1]:  # Check if both columns are not empty\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "input_csv_file = 'classifier_data_0.csv'\n",
    "output_csv_file = 'output_csv_file.csv'\n",
    "\n",
    "filter_non_empty_rows(input_csv_file, output_csv_file)\n",
    "print(f\"Filtered rows saved to '{output_csv_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows saved to 'output_csv_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def filter_non_empty_rows(input_csv, output_csv, delimiter=','):\n",
    "    \"\"\"\n",
    "    Filters rows from a CSV file where the first two columns are not empty \n",
    "    and saves them to another CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to the output CSV file.\n",
    "        delimiter (str, optional): Delimiter used in the CSV file (default is comma ',').\n",
    "    \"\"\"\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8-sig') as file:\n",
    "        lines = file.readlines()\n",
    "        # Filter out lines containing NUL characters\n",
    "        lines = [line for line in lines if '\\0' not in line]\n",
    "\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        csv_writer = csv.writer(output_file, delimiter=delimiter)\n",
    "\n",
    "        # Write header row if it exists in the input file \n",
    "        # (assuming the first line is the header)\n",
    "        header_row = next(csv.reader(open(input_csv, 'r', encoding='utf-8-sig'), delimiter=delimiter))\n",
    "        if header_row:\n",
    "            csv_writer.writerow(header_row)\n",
    "\n",
    "        for line in lines:\n",
    "            row = line.strip().split(delimiter)  # Split based on delimiter\n",
    "\n",
    "            # Check if the first two columns are not empty\n",
    "            if row[0] and row[1]:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "input_csv_file = 'classifier_data_0.csv'\n",
    "output_csv_file = 'output_csv_file.csv'\n",
    "\n",
    "filter_non_empty_rows(input_csv_file, output_csv_file)\n",
    "print(f\"Filtered rows saved to '{output_csv_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the filtered file: 118567\n"
     ]
    }
   ],
   "source": [
    "file_path = 'output_csv_file.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the filtered file:\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV filtered_csv_file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'output_csv_file.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV filtered_csv_file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows saved to 'filtered_csv_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def filter_rows_by_owner_count(input_csv, output_csv, min_owner_count):\n",
    "    \"\"\"\n",
    "    Filters rows from a CSV file based on the minimum occurrence count of an owner.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to the output CSV file where filtered rows will be saved.\n",
    "        min_owner_count (int): Minimum number of times an owner must appear in the input file to be included in the output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count occurrences of each owner\n",
    "    owner_counts = Counter()\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8-sig') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            if len(row) == len(header):  # Ensure the row has the correct number of elements\n",
    "                owner_counts[row[0]] += 1  # Assuming 'owner' is the first column\n",
    "\n",
    "    # Filter rows with owners that occur at least min_owner_count times\n",
    "    filtered_rows = []\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8-sig') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)  # Skip header\n",
    "        filtered_rows.append(header)\n",
    "        for row in csv_reader:\n",
    "            if len(row) == len(header) and owner_counts[row[0]] >= min_owner_count:\n",
    "                filtered_rows.append(row)\n",
    "\n",
    "    # Write filtered rows to the output CSV file (including all columns)\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as file:\n",
    "        csv_writer = csv.writer(file)\n",
    "        csv_writer.writerows(filtered_rows)  # Write all rows, not just specific columns\n",
    "\n",
    "input_csv_file = 'output_csv_file.csv'\n",
    "output_csv_file = 'filtered_csv_file.csv'\n",
    "min_owner_count = 5  # Minimum count of occurrences for an owner to be kept\n",
    "\n",
    "filter_rows_by_owner_count(input_csv_file, output_csv_file, min_owner_count)\n",
    "print(f\"Filtered rows saved to '{output_csv_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV filtered_csv_file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'filtered_csv_file.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV filtered_csv_file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV classifier_data_20 file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'classifier_data_20.csv'\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV classifier_data_20 file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the filtered file: 35619\n"
     ]
    }
   ],
   "source": [
    "file_path = 'filtered_csv_file.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the filtered file:\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV deep_data file: ['id', 'issue_id', 'issue_title', 'reported_time', 'owner', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'deep_data.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV deep_data file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged files saved to 'merged_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def merge_csv_files(input_files, output_file):\n",
    "    # Open the output CSV file in write mode\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as output_csv:\n",
    "        csv_writer = csv.writer(output_csv)\n",
    "\n",
    "        # Iterate over each input file\n",
    "        for input_file in input_files:\n",
    "            # Open the input CSV file\n",
    "            with open(input_file, 'r', newline='', encoding='utf-8') as input_csv:\n",
    "                # Filter out lines containing NUL characters\n",
    "                filtered_lines = (line for line in input_csv if '\\0' not in line)\n",
    "                csv_reader = csv.reader(filtered_lines)\n",
    "                \n",
    "                # Write the rows from the input file to the output file\n",
    "                csv_writer.writerows(csv_reader)\n",
    "\n",
    "# List of input CSV files to merge\n",
    "input_files = ['filtered_csv_file.csv', 'classifier_data_5.csv', 'classifier_data_10.csv', 'classifier_data_20.csv']\n",
    "\n",
    "# Output CSV file where merged content will be written\n",
    "output_file = 'merged_file.csv'\n",
    "\n",
    "# Merge the CSV files\n",
    "merge_csv_files(input_files, output_file)\n",
    "\n",
    "print(f\"Merged files saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'merged_file.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the merged file: 377053\n"
     ]
    }
   ],
   "source": [
    "file_path = 'merged_file.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the merged file:\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered, merged, and cleaned CSV saved to: filtered_merged_cleaned_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "def merge_filter_save_csv(input_csv, output_csv, min_word_count=10):\n",
    "    \"\"\"\n",
    "    Merges 'issue_title' and 'description' columns into a new 'Summary' column,\n",
    "    removes special characters, newlines, and hyperlinks from the Summary,\n",
    "    filters rows where the Summary has at least min_word_count words,\n",
    "    and saves the results to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to the output CSV file.\n",
    "        min_word_count (int, optional): Minimum word count for Summary column. Defaults to 10.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(input_csv, 'r', newline='') as infile, open(output_csv, 'w', newline='') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # Read header row\n",
    "        header = next(reader)\n",
    "\n",
    "        # Identify indices of 'issue_title' and 'description' columns\n",
    "        title_index = header.index('issue_title')\n",
    "        desc_index = header.index('description')\n",
    "\n",
    "        # Update header with 'Summary'\n",
    "        header.insert(header.index('description') + 1, 'Summary')  # Insert before 'description'\n",
    "        del header[desc_index]  # Remove 'description'\n",
    "        del header[title_index]  # Remove 'issue_title'\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Define regular expressions for special characters, newlines, and hyperlinks\n",
    "        special_char_pattern = r\"[^\\w\\s]\"\n",
    "        url_pattern = r\"(http|https)?://[^\\s]+?\"  # Matches URLs with optional protocol (http/https)\n",
    "\n",
    "        # Process data rows\n",
    "        for row in reader:\n",
    "            summary = row[title_index] + \" \" + row[desc_index]  # Merge text with space\n",
    "\n",
    "            # Clean the Summary text\n",
    "            clean_summary = re.sub(special_char_pattern, \"\", summary)\n",
    "            clean_summary = clean_summary.replace('\\n', ' ')  # Replace newline with space\n",
    "            clean_summary = re.sub(url_pattern, \"\", clean_summary)  # Remove hyperlinks\n",
    "\n",
    "            word_count = len(clean_summary.split())\n",
    "\n",
    "            if word_count >= min_word_count:\n",
    "                row.insert(desc_index + 1, clean_summary)  # Insert cleaned Summary before 'description'\n",
    "                del row[desc_index]  # Remove 'description'\n",
    "                del row[title_index]  # Remove 'issue_title'\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Example usage\n",
    "input_csv_file = \"merged_file.csv\"\n",
    "output_csv_file = \"filtered_merged_cleaned_output.csv\"\n",
    "\n",
    "merge_filter_save_csv(input_csv_file, output_csv_file)\n",
    "print(f\"Filtered, merged, and cleaned CSV saved to: {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV file: ['owner', 'Summary']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'merged_file_after_filteration.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'merged_file_after_filteration.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_file_after_filteration.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Open the file in read mode\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Use a loop to iterate over each line in the file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     line_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of lines in the merged file after filteration:\u001b[39m\u001b[38;5;124m\"\u001b[39m, line_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'merged_file_after_filteration.csv'"
     ]
    }
   ],
   "source": [
    "file_path = 'merged_file_after_filteration.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the merged file after filteration:\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('merged_file_after_filteration.csv')\n",
    "\n",
    "# Calculate the split indices\n",
    "total_rows = len(df)\n",
    "split_indices = [0] + [total_rows * i // 4 for i in range(1, 4)] + [total_rows]\n",
    "\n",
    "# Split the DataFrame into four parts and save each part to a separate CSV file\n",
    "for i in range(4):\n",
    "    start_index = split_indices[i]\n",
    "    end_index = split_indices[i+1]\n",
    "    df_part = df.iloc[start_index:end_index]\n",
    "    df_part.to_csv(f'merged_file_part_{i+1}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
