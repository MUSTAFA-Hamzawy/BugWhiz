{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ['''Topic sentences are similar to mini thesis statements. Like a thesis statement', 'a topic sentence has a specific main point. Whereas the thesis is the main point of the essay, the topic sentence is the main point of the paragraph.               Like the thesis statement, a topic sentence has a unifying function. \n",
    "But a thesis statement or topic sentence alone doesnâ€™t guarantee unity.', 'An essay is unified if all the paragraphs relate to the thesis,'whereas a paragraph is unified if all the sentences relate to the topic sentence.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents:  1\n",
      "Total words:  36\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "word_set = []\n",
    "\n",
    "for sent in sample_text:\n",
    "    words = [word.lower() for word in word_tokenize(sent) if word.isalpha()]\n",
    "    sentences.append(words)\n",
    "    for word in words:\n",
    "        if word not in word_set:\n",
    "            word_set.append(word)\n",
    "# Set of words\n",
    "word_set = set(word_set)\n",
    "# total documents in our corpus\n",
    "total_docs = len(sample_text)\n",
    "print('Total documents: ', total_docs)\n",
    "print('Total words: ', len(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "for i, word in enumerate(word_set):\n",
    "    word_index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences': 2, 'essay': 2, 't': 1, 'doesn': 1, 'relate': 2, 'are': 1, 'mini': 1, 'a': 7, 'if': 2, 'to': 3, 'topic': 6, 'alone': 1, 'guarantee': 1, 'paragraph': 2, 'paragraphs': 1, 'main': 3, 'statements': 1, 'or': 1, 'has': 2, 'thesis': 6, 'point': 3, 'all': 2, 'function': 1, 'the': 11, 'specific': 1, 'unified': 2, 'similar': 1, 'sentence': 5, 'whereas': 1, 'like': 2, 'statement': 3, 'unity': 1, 'unifying': 1, 'is': 4, 'but': 1, 'of': 2}\n"
     ]
    }
   ],
   "source": [
    "def count_dict(sentences):\n",
    "    count_dict = {}\n",
    "    for word in word_set:\n",
    "        count_dict[word] = 0\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            count_dict[word] += 1\n",
    "    return count_dict\n",
    "word_count = count_dict(sentences)\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(total_docs / word_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(sentence):\n",
    "    vec = np.zeros((len(word_set),))\n",
    "    for word in sentence:\n",
    "        tf = term_frequency(sentence, word)\n",
    "        idf = inverse_document_frequency(word)\n",
    "        vec[word_index[word]] = tf * idf\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.02525545, -0.02525545, -0.00796721, -0.00796721, -0.02525545,\n",
      "       -0.00796721, -0.00796721, -0.16731139, -0.02525545, -0.04780325,\n",
      "       -0.1342007 , -0.00796721, -0.00796721, -0.02525545, -0.00796721,\n",
      "       -0.04780325, -0.00796721, -0.00796721, -0.02525545, -0.1342007 ,\n",
      "       -0.04780325, -0.02525545, -0.00796721, -0.3141836 , -0.00796721,\n",
      "       -0.02525545, -0.00796721, -0.10297468, -0.00796721, -0.02525545,\n",
      "       -0.04780325, -0.00796721, -0.00796721, -0.07399715, -0.00796721,\n",
      "       -0.02525545])]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for sent in sentences:\n",
    "    vectors.append(tf_idf(sent))\n",
    "\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
