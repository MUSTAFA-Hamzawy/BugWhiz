{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nimfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ-_-4bROUdX",
        "outputId": "a6d4b214-44d6-4eac-8006-d96d85d11e1a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nimfa\n",
            "  Downloading nimfa-1.4.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nimfa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from nimfa) (1.11.4)\n",
            "Installing collected packages: nimfa\n",
            "Successfully installed nimfa-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GfR4Yo-rtJZ",
        "outputId": "ba32ed60-c8ee-465a-9420-a4a12537862f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukDBCJvXqwYc",
        "outputId": "83081b6b-e762-4a5b-afce-41108c3887b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# remove the stop words from the preprocessed data using nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0rcr0kUBqwYe"
      },
      "outputs": [],
      "source": [
        "def convert_lower_case(data):\n",
        "    return str(data).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DqVvCOicqwYe"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in symbols:\n",
        "        data = np.char.replace(data, i, ' ')\n",
        "\n",
        "    return str(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q4-p9BLIqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B2RoS_VXqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(data):\n",
        "    return re.sub(r'\\d+', '', str(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kicK-_AdqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_single_characters(tokens):\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        if len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NFCFEuimqwYg"
      },
      "outputs": [],
      "source": [
        "def lemmatization(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(data)\n",
        "    data = remove_single_characters(tokens)\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "    return lemmatized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OGszj2RZqwYg"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_numbers(data)\n",
        "    data = lemmatization(data)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwwT_5_PqwYi",
        "outputId": "c269e430-8f46-457b-d3eb-27d24be68995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  for any event on my bookmarked project option ...    Backend\n",
            "1               switch to using full ln id in urlbar   Frontend\n",
            "2  consider removing hasicon property to simplify...   Frontend\n",
            "3  method to obtain current url from webbrowsered...   Frontend\n",
            "4                fix migration fails in m sql server    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_train_df = pd.read_csv('/content/preprocessed_train_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlvdLrF6qwYi",
        "outputId": "97499af5-7a25-43b5-fb0c-769f016fd4ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  rest api ability to list sub project for a pro...    Backend\n",
            "1  support selective text on right if set in gnom...   Frontend\n",
            "2  meta userstory ship v of pre populated topsite...   Frontend\n",
            "3  include updated on and passwd changed on colum...    Backend\n",
            "4         problem with email integration to m office    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_test_df = pd.read_csv('/content/preprocessed_test_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the stop words from the preprocessed data using nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoyHEvQRbQDl",
        "outputId": "0bbdccf1-58e0-4773-b12c-8e315a16044a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(data):\n",
        "    tokens = word_tokenize(data)\n",
        "    data = ' '.join([i for i in tokens if not i in stop_words])\n",
        "    return data\n",
        "\n",
        "# preprocess the first report of the training data\n",
        "print(preprocess(preprocessed_train_df['bug_description'][0]))\n",
        "\n",
        "# remove the stop words from the preprocessed data\n",
        "print(remove_stop_words(preprocess(preprocessed_train_df['bug_description'][0])))\n",
        "\n",
        "# preprocess the first report of the testing data\n",
        "print(preprocess(preprocessed_test_df['bug_description'][0]))\n",
        "\n",
        "# remove the stop words from the preprocessed data\n",
        "print(remove_stop_words(preprocess(preprocessed_test_df['bug_description'][0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFUU3HvrsIOl",
        "outputId": "a4591fbb-14e5-43fc-98cb-756e63af18c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "event bookmarked project option sending notification non member bookmarked project\n",
            "event bookmarked project option sending notification non member bookmarked project\n",
            "rest api ability list sub project project\n",
            "rest api ability list sub project project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1XWkNBPqwYi",
        "outputId": "3fcfb993-8602-43e4-a153-d2a98d0ed6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ],
      "source": [
        "# Convert non-string values to strings in 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: str(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: str(x))\n",
        "\n",
        "# Remove stop words from 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "\n",
        "# Show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the preprocessed testing data\n",
        "print(preprocessed_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7avA5KxfqwYi",
        "outputId": "08f0ebfe-d8a3-44d0-fdc2-6370b48dd42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Training Data:\n",
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "\n",
            "Filtered Testing Data:\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ],
      "source": [
        "# keep only the reports that has class_name of Frontend, Backend, Security, Documentation\n",
        "# Filter the training data\n",
        "filtered_train_df = preprocessed_train_df[\n",
        "    (preprocessed_train_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Security') |\n",
        "    (preprocessed_train_df['class_name'] == 'Documentation')\n",
        "]\n",
        "\n",
        "# Filter the testing data\n",
        "filtered_test_df = preprocessed_test_df[\n",
        "    (preprocessed_test_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Security') |\n",
        "    (preprocessed_test_df['class_name'] == 'Documentation')\n",
        "]\n",
        "\n",
        "# Show the first 5 rows of the filtered training data\n",
        "print(\"Filtered Training Data:\")\n",
        "print(filtered_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the filtered testing data\n",
        "print(\"\\nFiltered Testing Data:\")\n",
        "print(filtered_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYJ8iDYgqwYi",
        "outputId": "9128a4c3-037a-4744-ece7-d4db3d2eda04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Backend' 'Frontend' 'Security' 'Documentation']\n",
            "['Backend' 'Frontend' 'Documentation' 'Security']\n"
          ]
        }
      ],
      "source": [
        "# print the unique class names in the training data\n",
        "print(filtered_train_df['class_name'].unique())\n",
        "\n",
        "# print the unique class names in the testing data\n",
        "print(filtered_test_df['class_name'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esX-WwIwqwYj"
      },
      "source": [
        "## Feature Exraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(filtered_train_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZB99m8gvAcs",
        "outputId": "2bcf80fe-4b40-405d-c6a9-046776692857"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6I9uy_heqwYk"
      },
      "outputs": [],
      "source": [
        "# Define the mapping of class names to the desired order\n",
        "class_name_mapping = {\n",
        "    'Backend': 1,\n",
        "    'Frontend': 0,\n",
        "    'Security': 2,\n",
        "    'Documentation': 3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext.vocab as vocab\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove = vocab.GloVe(name='6B', dim=300)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGOLFs0fbwAh",
        "outputId": "cc19954a-7ea7-4baa-abf4-c48c6b49e2ec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [01:05<00:00, 6118.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize bug reports and map tokens to GloVe embeddings\n",
        "def tokenize_and_map_to_glove(text):\n",
        "    tokens = text.split()\n",
        "    embeddings = [glove[token.lower()] for token in tokens if token.lower() in glove.stoi]\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "5zJKEeYdbxUF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "tokenized_bug_reports_train = [tokenize_and_map_to_glove(text) for text in filtered_train_df['bug_description']]\n",
        "tokenized_bug_reports_test = [tokenize_and_map_to_glove(text) for text in filtered_test_df['bug_description']]"
      ],
      "metadata": {
        "id": "B8SrjU9Gbz6Q"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate token embeddings (e.g., by averaging)\n",
        "def aggregate_embeddings(embeddings):\n",
        "    if embeddings:\n",
        "        return torch.stack(embeddings).mean(dim=0)\n",
        "    else:\n",
        "        # Return a zero vector if no embeddings are found\n",
        "        return torch.zeros(glove.vectors.shape[1])"
      ],
      "metadata": {
        "id": "DyRbm1cxb0tu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example usage:\n",
        "X_train = torch.stack([aggregate_embeddings(embeddings) for embeddings in tokenized_bug_reports_train])\n",
        "X_test = torch.stack([aggregate_embeddings(embeddings) for embeddings in tokenized_bug_reports_test])\n",
        "\n",
        "# Now you can use X_train and X_test as features for your classification model\n"
      ],
      "metadata": {
        "id": "96JuaST_g87p"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Save X_train and X_test to files\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('X_test.npy', X_test)\n"
      ],
      "metadata": {
        "id": "zgt0qs7bNP9J"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load X_train and X_test from files\n",
        "X_train = np.load('X_train.npy')\n",
        "X_test = np.load('X_test.npy')\n"
      ],
      "metadata": {
        "id": "1y5wBKOPOsDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_clustering(data, class_name, num_clusters=4):\n",
        "    \"\"\"\n",
        "    Apply hierarchical clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - num_clusters: Number of clusters to be generated (default is 4).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply hierarchical clustering\n",
        "    clustering = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "    cluster_labels = clustering.fit_predict(data)\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        0: 'Backend',\n",
        "        1: 'Frontend',\n",
        "        2: 'Security',\n",
        "        3: 'Documentation'\n",
        "        # Add more mappings if needed\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define the number of clusters\n",
        "num_clusters = 4  # Adjust as needed\n",
        "\n",
        "# Apply clustering on training data\n",
        "apply_clustering(X_train, filtered_train_df['class_name'], num_clusters=num_clusters)\n",
        "\n",
        "# Apply clustering on test data\n",
        "apply_clustering(X_test, filtered_test_df['class_name'], num_clusters=num_clusters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwF2zikxgdeR",
        "outputId": "4471134c-360b-4b51-89fd-a31cd11b1d3b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Backend       0.65      0.41      0.50      7437\n",
            "Documentation       0.01      0.03      0.01       174\n",
            "     Frontend       0.77      0.38      0.51      5799\n",
            "     Security       0.04      0.52      0.07       367\n",
            "\n",
            "     accuracy                           0.39     13777\n",
            "    macro avg       0.37      0.33      0.27     13777\n",
            " weighted avg       0.68      0.39      0.49     13777\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3012  938  555 2932]\n",
            " [  84    5   30   55]\n",
            " [1458   20 2228 2093]\n",
            " [ 108    3   66  190]]\n",
            "Accuracy: 0.3944980765043188\n",
            "Precision: 0.6755257556522494\n",
            "Recall: 0.3944980765043188\n",
            "F1 Score: 0.48680963728651483\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Backend       0.61      0.54      0.57      1345\n",
            "Documentation       0.01      0.38      0.02        21\n",
            "     Frontend       0.36      0.15      0.21       987\n",
            "     Security       0.01      0.01      0.01        70\n",
            "\n",
            "     accuracy                           0.36      2423\n",
            "    macro avg       0.25      0.27      0.20      2423\n",
            " weighted avg       0.49      0.36      0.40      2423\n",
            "\n",
            "Confusion Matrix:\n",
            "[[725 263 236 121]\n",
            " [  7   8   6   0]\n",
            " [415 426 144   2]\n",
            " [ 35  23  11   1]]\n",
            "Accuracy: 0.3623607098638052\n",
            "Precision: 0.4885603535507735\n",
            "Recall: 0.3623607098638052\n",
            "F1 Score: 0.4037669470406613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_gmm_clustering(data, class_name, num_clusters=4, random_state=42):\n",
        "    \"\"\"\n",
        "    Apply Gaussian Mixture Model clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - num_clusters: Number of clusters to be generated (default is 4).\n",
        "    - random_state: Random seed for reproducibility (default is 42).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply Gaussian Mixture Model clustering\n",
        "    gmm = GaussianMixture(n_components=num_clusters, random_state=random_state)\n",
        "    cluster_labels = gmm.fit_predict(data)\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        0: 'Backend',\n",
        "        1: 'Frontend',\n",
        "        2: 'Security',\n",
        "        3: 'Documentation'\n",
        "        # Add more mappings if needed\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define the number of clusters\n",
        "num_clusters = 4  # Adjust as needed\n",
        "\n",
        "# Apply GMM clustering on training data\n",
        "apply_gmm_clustering(X_train, filtered_train_df['class_name'], num_clusters=num_clusters)\n",
        "\n",
        "# Apply GMM clustering on test data\n",
        "apply_gmm_clustering(X_test, filtered_test_df['class_name'], num_clusters=num_clusters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAdMgXEhhfYt",
        "outputId": "06f6b9b1-f5fe-4516-8ddd-f784b97b451f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Backend       0.25      0.18      0.21      7437\n",
            "Documentation       0.01      0.03      0.01       174\n",
            "     Frontend       0.40      0.27      0.32      5799\n",
            "     Security       0.01      0.09      0.02       367\n",
            "\n",
            "     accuracy                           0.21     13777\n",
            "    macro avg       0.17      0.14      0.14     13777\n",
            " weighted avg       0.30      0.21      0.25     13777\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1330  977 2103 3027]\n",
            " [  19    6  102   47]\n",
            " [3804   26 1544  425]\n",
            " [ 178    5  152   32]]\n",
            "Accuracy: 0.21136677070479784\n",
            "Precision: 0.3015888718062606\n",
            "Recall: 0.21136677070479784\n",
            "F1 Score: 0.24702568549861834\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Backend       0.58      0.44      0.50      1345\n",
            "Documentation       0.01      0.05      0.01        21\n",
            "     Frontend       0.63      0.42      0.50       987\n",
            "     Security       0.04      0.27      0.06        70\n",
            "\n",
            "     accuracy                           0.43      2423\n",
            "    macro avg       0.31      0.30      0.27      2423\n",
            " weighted avg       0.58      0.43      0.48      2423\n",
            "\n",
            "Confusion Matrix:\n",
            "[[594 177 229 345]\n",
            " [  6   1   5   9]\n",
            " [398   4 417 168]\n",
            " [ 34   1  16  19]]\n",
            "Accuracy: 0.4255055716054478\n",
            "Precision: 0.5752332279029058\n",
            "Recall: 0.4255055716054478\n",
            "F1 Score: 0.48471053563347194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_spectral_clustering(data, class_name, num_clusters=4, affinity='rbf', random_state=42):\n",
        "    \"\"\"\n",
        "    Apply Spectral Clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - num_clusters: Number of clusters to be generated (default is 4).\n",
        "    - affinity: The affinity matrix to use (default is 'rbf').\n",
        "    - random_state: Random seed for reproducibility (default is 42).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply Spectral Clustering\n",
        "    spectral = SpectralClustering(n_clusters=num_clusters, affinity=affinity, random_state=random_state)\n",
        "    cluster_labels = spectral.fit_predict(data)\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        label: f'Cluster_{label}' for label in np.unique(cluster_labels)\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define the number of clusters and affinity metric\n",
        "num_clusters = 4  # Adjust as needed\n",
        "affinity = 'rbf'  # Options: {'nearest_neighbors', 'rbf', 'precomputed'}\n",
        "\n",
        "# Apply Spectral Clustering on training data\n",
        "apply_spectral_clustering(X_train, filtered_train_df['class_name'], num_clusters=num_clusters, affinity=affinity)\n",
        "\n",
        "# Apply Spectral Clustering on test data\n",
        "apply_spectral_clustering(X_test, filtered_test_df['class_name'], num_clusters=num_clusters, affinity=affinity)\n"
      ],
      "metadata": {
        "id": "pxZWsXqohxG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_affinity_propagation(data, class_name, damping=0.5, random_state=None):\n",
        "    \"\"\"\n",
        "    Apply Affinity Propagation to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - damping: Damping factor (between 0.5 and 1) (default is 0.5).\n",
        "    - random_state: Random seed for reproducibility (default is None).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply Affinity Propagation\n",
        "    affinity_propagation = AffinityPropagation(damping=damping, random_state=random_state)\n",
        "    cluster_labels = affinity_propagation.fit_predict(data)\n",
        "\n",
        "    # Number of clusters\n",
        "    num_clusters = len(np.unique(cluster_labels))\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        label: f'Cluster_{label}' for label in np.unique(cluster_labels)\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Number of Clusters:\", num_clusters)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define damping factor and random state\n",
        "damping = 0.5  # Adjust as needed\n",
        "random_state = 42  # Adjust as needed\n",
        "\n",
        "# Apply Affinity Propagation on training data\n",
        "apply_affinity_propagation(X_train, filtered_train_df['class_name'], damping=damping, random_state=random_state)\n",
        "\n",
        "# Apply Affinity Propagation on test data\n",
        "apply_affinity_propagation(X_test, filtered_test_df['class_name'], damping=damping, random_state=random_state)\n"
      ],
      "metadata": {
        "id": "g8HE9HMJiCuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_optics(data, class_name, min_samples=5, xi=0.05, min_cluster_size=0.1):\n",
        "    \"\"\"\n",
        "    Apply OPTICS clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - min_samples: The number of samples in a neighborhood for a data point to be considered a core point (default is 5).\n",
        "    - xi: Determines the minimum steepness of the cluster hierarchy that will be returned (default is 0.05).\n",
        "    - min_cluster_size: The minimum number of samples in a cluster (default is 0.1).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply OPTICS clustering\n",
        "    optics = OPTICS(min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)\n",
        "    cluster_labels = optics.fit_predict(data)\n",
        "\n",
        "    # Number of clusters\n",
        "    num_clusters = len(np.unique(cluster_labels))\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        label: f'Cluster_{label}' for label in np.unique(cluster_labels)\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Number of Clusters:\", num_clusters)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define parameters for OPTICS\n",
        "min_samples = 5  # Adjust as needed\n",
        "xi = 0.05  # Adjust as needed\n",
        "min_cluster_size = 0.1  # Adjust as needed\n",
        "\n",
        "# Apply OPTICS on training data\n",
        "apply_optics(X_train, filtered_train_df['class_name'], min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)\n",
        "\n",
        "# Apply OPTICS on test data\n",
        "apply_optics(X_test, filtered_test_df['class_name'], min_samples=min_samples, xi=xi, min_cluster_size=min_cluster_size)\n"
      ],
      "metadata": {
        "id": "WC4YyebPiY0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_mean_shift(data, class_name, bandwidth=None):\n",
        "    \"\"\"\n",
        "    Apply Mean Shift clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - bandwidth: Bandwidth parameter for the Mean Shift algorithm (optional).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply Mean Shift clustering\n",
        "    mean_shift = MeanShift(bandwidth=bandwidth)\n",
        "    cluster_labels = mean_shift.fit_predict(data)\n",
        "\n",
        "    # Number of clusters\n",
        "    num_clusters = len(np.unique(cluster_labels))\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        label: f'Cluster_{label}' for label in np.unique(cluster_labels)\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Number of Clusters:\", num_clusters)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define bandwidth parameter for Mean Shift\n",
        "bandwidth = 0.1  # Adjust as needed\n",
        "\n",
        "# Apply Mean Shift clustering on training data\n",
        "apply_mean_shift(X_train, filtered_train_df['class_name'], bandwidth=bandwidth)\n",
        "\n",
        "# Apply Mean Shift clustering on test data\n",
        "apply_mean_shift(X_test, filtered_test_df['class_name'], bandwidth=bandwidth)\n"
      ],
      "metadata": {
        "id": "Un1vjVPjias6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Birch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_birch(data, class_name, threshold=0.5, branching_factor=50):\n",
        "    \"\"\"\n",
        "    Apply Birch clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - threshold: The branching factor threshold (optional).\n",
        "    - branching_factor: The number of subclusters in each node (optional).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply Birch clustering\n",
        "    birch = Birch(threshold=threshold, branching_factor=branching_factor)\n",
        "    cluster_labels = birch.fit_predict(data)\n",
        "\n",
        "    # Number of clusters\n",
        "    num_clusters = len(np.unique(cluster_labels))\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        label: f'Cluster_{label}' for label in np.unique(cluster_labels)\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Number of Clusters:\", num_clusters)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define Birch parameters\n",
        "threshold = 0.5  # Adjust as needed\n",
        "branching_factor = 50  # Adjust as needed\n",
        "\n",
        "# Apply Birch clustering on training data\n",
        "apply_birch(X_train, filtered_train_df['class_name'], threshold=threshold, branching_factor=branching_factor)\n",
        "\n",
        "# Apply Birch clustering on test data\n",
        "apply_birch(X_test, filtered_test_df['class_name'], threshold=threshold, branching_factor=branching_factor)\n"
      ],
      "metadata": {
        "id": "Sh5m2uxFijRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_mini_batch_kmeans(data, class_name, num_clusters=4, random_state=42):\n",
        "    \"\"\"\n",
        "    Apply MiniBatchKMeans clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - num_clusters: Number of clusters to be generated (default is 4).\n",
        "    - random_state: Random seed for reproducibility (default is 42).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply MiniBatchKMeans clustering\n",
        "    mbkmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=random_state)\n",
        "    cluster_labels = mbkmeans.fit_predict(data)\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        label: f'Cluster_{label}' for label in np.unique(cluster_labels)\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Number of Clusters:\", num_clusters)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define the number of clusters\n",
        "num_clusters = 4  # Adjust as needed\n",
        "\n",
        "# Apply MiniBatchKMeans clustering on training data\n",
        "apply_mini_batch_kmeans(X_train, filtered_train_df['class_name'], num_clusters=num_clusters)\n",
        "\n",
        "# Apply MiniBatchKMeans clustering on test data\n",
        "apply_mini_batch_kmeans(X_test, filtered_test_df['class_name'], num_clusters=num_clusters)\n"
      ],
      "metadata": {
        "id": "3_xwXU0TirhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_dbscan(data, class_name, eps=0.5, min_samples=5):\n",
        "    \"\"\"\n",
        "    Apply DBSCAN clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other (default is 0.5).\n",
        "    - min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point (default is 5).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply DBSCAN clustering\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    cluster_labels = dbscan.fit_predict(data)\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        label: f'Cluster_{label}' for label in np.unique(cluster_labels)\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"DBSCAN Parameters:\")\n",
        "    print(\"Epsilon:\", eps)\n",
        "    print(\"Min Samples:\", min_samples)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define DBSCAN parameters\n",
        "eps = 0.5  # Maximum distance between two samples\n",
        "min_samples = 5  # Minimum number of samples in a neighborhood\n",
        "\n",
        "# Apply DBSCAN clustering on training data\n",
        "apply_dbscan(X_train, filtered_train_df['class_name'], eps=eps, min_samples=min_samples)\n",
        "\n",
        "# Apply DBSCAN clustering on test data\n",
        "apply_dbscan(X_test, filtered_test_df['class_name'], eps=eps, min_samples=min_samples)\n"
      ],
      "metadata": {
        "id": "Gx404II7i5-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_pca_kmeans(data, class_name, num_clusters=4, pca_components=50, random_state=42):\n",
        "    \"\"\"\n",
        "    Apply PCA followed by K-means clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - num_clusters: Number of clusters to be generated by K-means (default is 4).\n",
        "    - pca_components: Number of principal components to retain (default is 50).\n",
        "    - random_state: Random seed for reproducibility (default is 42).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply PCA for dimensionality reduction\n",
        "    pca = PCA(n_components=pca_components, random_state=random_state)\n",
        "    reduced_data = pca.fit_transform(data)\n",
        "\n",
        "    # Apply K-means clustering on the reduced data\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=random_state)\n",
        "    cluster_labels = kmeans.fit_predict(reduced_data)\n",
        "\n",
        "    # Map cluster labels to class names\n",
        "    cluster_class_mapping = {\n",
        "        label: f'Cluster_{label}' for label in np.unique(cluster_labels)\n",
        "    }\n",
        "    predicted_class_names = [cluster_class_mapping[label] for label in cluster_labels]\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"PCA Components:\", pca_components)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, predicted_class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, predicted_class_names))\n",
        "\n",
        "    accuracy = np.mean(class_name == predicted_class_names)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define the number of clusters and PCA components\n",
        "num_clusters = 4  # Adjust as needed\n",
        "pca_components = 50  # Adjust as needed\n",
        "\n",
        "# Apply PCA followed by K-means clustering on training data\n",
        "apply_pca_kmeans(X_train, filtered_train_df['class_name'], num_clusters=num_clusters, pca_components=pca_components)\n",
        "\n",
        "# Apply PCA followed by K-means clustering on test data\n",
        "apply_pca_kmeans(X_test, filtered_test_df['class_name'], num_clusters=num_clusters, pca_components=pca_components)\n"
      ],
      "metadata": {
        "id": "5J1micjpjMC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install MiniSom"
      ],
      "metadata": {
        "id": "KF2vezz-kGjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from minisom import MiniSom\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_som(data, class_name, grid_size=(10, 10), num_iterations=100, random_seed=42):\n",
        "    \"\"\"\n",
        "    Apply Self-Organizing Maps (SOM) clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - grid_size: Tuple specifying the size of the SOM grid (default is (10, 10)).\n",
        "    - num_iterations: Number of iterations for training the SOM (default is 100).\n",
        "    - random_seed: Random seed for reproducibility (default is 42).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Initialize the SOM\n",
        "    som = MiniSom(grid_size[0], grid_size[1], data.shape[1], sigma=1.0, learning_rate=0.5, random_seed=random_seed)\n",
        "\n",
        "    # Train the SOM\n",
        "    som.train_random(data, num_iterations)\n",
        "\n",
        "    # Find the best-matching units (BMUs) for each data point\n",
        "    bmu_indices = np.array([som.winner(x) for x in data])\n",
        "\n",
        "    # Map BMUs to cluster labels\n",
        "    cluster_labels = np.ravel_multi_index(bmu_indices.T, grid_size)\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, cluster_labels))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, cluster_labels))\n",
        "\n",
        "    accuracy = np.mean(class_name == cluster_labels)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, cluster_labels, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define the grid size and number of iterations\n",
        "grid_size = (10, 10)  # Adjust as needed\n",
        "num_iterations = 100   # Adjust as needed\n",
        "\n",
        "# Apply SOM clustering on training data\n",
        "apply_som(X_train, filtered_train_df['class_name'], grid_size=grid_size, num_iterations=num_iterations)\n",
        "\n",
        "# Apply SOM clustering on test data\n",
        "apply_som(X_test, filtered_test_df['class_name'], grid_size=grid_size, num_iterations=num_iterations)\n"
      ],
      "metadata": {
        "id": "5OMSAC2Nj01L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install hdbscan\n"
      ],
      "metadata": {
        "id": "-KI61g43kShu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_hdbscan(data, class_name, min_cluster_size=5, min_samples=5):\n",
        "    \"\"\"\n",
        "    Apply HDBSCAN clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - min_cluster_size: The minimum size of clusters (default is 5).\n",
        "    - min_samples: The number of samples in a neighborhood for a point to be considered as a core point (default is 5).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply HDBSCAN clustering\n",
        "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
        "    cluster_labels = clusterer.fit_predict(data)\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, cluster_labels))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, cluster_labels))\n",
        "\n",
        "    accuracy = np.mean(class_name == cluster_labels)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, cluster_labels, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define the minimum cluster size and minimum samples\n",
        "min_cluster_size = 5  # Adjust as needed\n",
        "min_samples = 5       # Adjust as needed\n",
        "\n",
        "# Apply HDBSCAN clustering on training data\n",
        "apply_hdbscan(X_train, filtered_train_df['class_name'], min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
        "\n",
        "# Apply HDBSCAN clustering on test data\n",
        "apply_hdbscan(X_test, filtered_test_df['class_name'], min_cluster_size=min_cluster_size, min_samples=min_samples)\n"
      ],
      "metadata": {
        "id": "9laX-89_kJBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn-extra\n"
      ],
      "metadata": {
        "id": "6nNgc9K2kTYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn_extra.cluster import OPTICS\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "def apply_optics_hierarchical(data, class_name, min_samples=5):\n",
        "    \"\"\"\n",
        "    Apply OPTICS clustering with hierarchical clustering to GloVe embeddings and evaluate the performance.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the GloVe embeddings.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - min_samples: The number of samples in a neighborhood for a point to be considered as a core point (default is 5).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Apply OPTICS clustering\n",
        "    optics = OPTICS(min_samples=min_samples)\n",
        "    optics.fit(data)\n",
        "\n",
        "    # Apply hierarchical clustering to reachability plot\n",
        "    hierarchical_clusterer = AgglomerativeClustering(n_clusters=None, distance_threshold=0)\n",
        "    hierarchical_clusterer.fit(optics.reachability_)\n",
        "\n",
        "    # Evaluate clustering performance\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(class_name, hierarchical_clusterer.labels_))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(class_name, hierarchical_clusterer.labels_))\n",
        "\n",
        "    accuracy = np.mean(class_name == hierarchical_clusterer.labels_)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, hierarchical_clusterer.labels_, average='weighted')\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1_score)\n",
        "\n",
        "# Assuming you have obtained GloVe embeddings and stored them in X_train and X_test\n",
        "# Define the minimum number of samples\n",
        "min_samples = 5  # Adjust as needed\n",
        "\n",
        "# Apply OPTICS with hierarchical clustering on training data\n",
        "apply_optics_hierarchical(X_train, filtered_train_df['class_name'], min_samples=min_samples)\n",
        "\n",
        "# Apply OPTICS with hierarchical clustering on test data\n",
        "apply_optics_hierarchical(X_test, filtered_test_df['class_name'], min_samples=min_samples)\n"
      ],
      "metadata": {
        "id": "RVWnFh-_ke3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SmH256dCkgwA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}