{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'  # Specify the desired BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1024\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenized_bug_reports = [tokenizer.encode(text, add_special_tokens=True) for text in filtered_train_df['bug_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a batch and obtain embeddings\n",
    "def process_batch(batch):\n",
    "    # Pad sequences to a fixed length within the batch\n",
    "    max_length = max(len(seq) for seq in batch)\n",
    "    padded_sequences = [seq + [0]*(max_length-len(seq)) for seq in batch]\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    batch_tensors = torch.tensor(padded_sequences)\n",
    "    \n",
    "    # Obtain the embeddings\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = model(batch_tensors)[0]\n",
    "    \n",
    "    # Convert embeddings to numpy array\n",
    "    return embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pad sequences within a batch\n",
    "def pad_batch(batch):\n",
    "    # Find the maximum length within the batch\n",
    "    max_length = max(len(seq) for seq in batch)\n",
    "    # Pad sequences to the maximum length\n",
    "    padded_sequences = [seq + [0]*(max_length-len(seq)) for seq in batch]\n",
    "    return padded_sequences\n",
    "\n",
    "# Process the data in batches\n",
    "num_batches = len(tokenized_bug_reports) // batch_size\n",
    "if len(tokenized_bug_reports) % batch_size != 0:\n",
    "    num_batches += 1\n",
    "\n",
    "X_train_batches = []\n",
    "for i in range(num_batches):\n",
    "    print(i)\n",
    "    batch_start = i * batch_size\n",
    "    batch_end = min((i + 1) * batch_size, len(tokenized_bug_reports))\n",
    "    batch = tokenized_bug_reports[batch_start:batch_end]\n",
    "    # Pad the last batch if it has fewer samples\n",
    "    if len(batch) < batch_size:\n",
    "        batch = pad_batch(batch)\n",
    "    batch_embeddings = process_batch(batch)\n",
    "    X_train_batches.append(batch_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the batches \n",
    "# x_train_batches has different number of samples in each batch and this produces error in the next step\n",
    "for i in range(len(X_train_batches)):\n",
    "    if i == 0:\n",
    "        X_train = X_train_batches[i]\n",
    "    else:\n",
    "        X_train = np.concatenate((X_train, X_train_batches[i]), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def nmf_topic_modeling(X_train, X_test, class_name, n_components=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform Non-negative Matrix Factorization (NMF) topic modeling on the given data and evaluate on test data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Array containing the BERT embeddings for training data.\n",
    "    - X_test: Array containing the BERT embeddings for test data.\n",
    "    - class_name: Series containing the class names corresponding to the data.\n",
    "    - n_components: Number of topics to be generated (default is 2).\n",
    "    - random_state: Random seed for reproducibility (default is 42).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Reshape X_train and X_test to have two dimensions\n",
    "    num_samples_train, num_tokens_train, embedding_size_train = X_train.shape\n",
    "    X_train_2d = X_train.reshape(num_samples_train * num_tokens_train, embedding_size_train)\n",
    "    \n",
    "    num_samples_test, num_tokens_test, embedding_size_test = X_test.shape\n",
    "    X_test_2d = X_test.reshape(num_samples_test * num_tokens_test, embedding_size_test)\n",
    "\n",
    "\n",
    "    # Scale the input data to make it non-negative\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_2d)\n",
    "    X_test_scaled = scaler.transform(X_test_2d)\n",
    "\n",
    "    # Initialize the NMF model\n",
    "    nmf_model = NMF(n_components=n_components, random_state=random_state)\n",
    "\n",
    "    # Fit the NMF model on the training data\n",
    "    nmf_model.fit(X_train_scaled)\n",
    "\n",
    "    # Predict the topics for the test data\n",
    "    topic_predictions_test = nmf_model.transform(X_test_scaled)\n",
    "\n",
    "    # Reshape the predictions back to the shape of the original test data\n",
    "    topic_predictions_test = topic_predictions_test.reshape(num_samples_test, num_tokens_test, n_components)\n",
    "\n",
    "    # Take only the first 1024 samples from the class_name series\n",
    "    class_name_subset = class_name[:num_samples_test]\n",
    "\n",
    "    # Flatten the predictions and class names to align for evaluation\n",
    "    flat_topic_predictions_test = topic_predictions_test.reshape(-1, n_components)\n",
    "    flat_class_name_subset = class_name_subset.repeat(num_tokens_test)\n",
    "\n",
    "    # Map numerical indices to class names\n",
    "    predicted_class_names_test = flat_class_name_subset.unique()[flat_topic_predictions_test.argmax(axis=1)]\n",
    "\n",
    "    # Print the classification report for the test data\n",
    "    print(\"Classification Report for Test Data:\")\n",
    "    print(classification_report(flat_class_name_subset, predicted_class_names_test))\n",
    "\n",
    "    # Print the confusion matrix for the test data\n",
    "    print(\"Confusion Matrix for Test Data:\")\n",
    "    print(confusion_matrix(flat_class_name_subset, predicted_class_names_test))\n",
    "\n",
    "    # Calculate and print the accuracy for the test data\n",
    "    accuracy_test = np.mean(flat_class_name_subset == predicted_class_names_test)\n",
    "    print(\"Accuracy for Test Data:\", accuracy_test)\n",
    "\n",
    "    # Calculate and print the precision, recall, and F1-score for the test data\n",
    "    precision_test, recall_test, f1_score_test, _ = precision_recall_fscore_support(flat_class_name_subset, predicted_class_names_test, average='weighted')\n",
    "    print(\"Precision for Test Data:\", precision_test)\n",
    "    print(\"Recall for Test Data:\", recall_test)\n",
    "    print(\"F1 Score for Test Data:\", f1_score_test)\n",
    "\n",
    "# Usage example:\n",
    "nmf_topic_modeling(X_train, X_test, filtered_test_df['class_name'], n_components=2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def lda_topic_modeling(X_train, X_test, class_name, n_components=2, random_state=42, model_save_path='lda_model.pkl'):\n",
    "    \"\"\"\n",
    "    Perform Latent Dirichlet Allocation (LDA) topic modeling on the given data and evaluate on test data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Array containing the BERT embeddings for training data.\n",
    "    - X_test: Array containing the BERT embeddings for test data.\n",
    "    - class_name: Series containing the class names corresponding to the data.\n",
    "    - n_components: Number of topics to be generated (default is 2).\n",
    "    - random_state: Random seed for reproducibility (default is 42).\n",
    "    - model_save_path: Path to save the trained LDA model (default is 'lda_model.pkl').\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Reshape X_train and X_test to have two dimensions\n",
    "    num_samples_train, num_tokens_train, embedding_size_train = X_train.shape\n",
    "    X_train_2d = X_train.reshape(num_samples_train * num_tokens_train, embedding_size_train)\n",
    "    \n",
    "    num_samples_test, num_tokens_test, embedding_size_test = X_test.shape\n",
    "    X_test_2d = X_test.reshape(num_samples_test * num_tokens_test, embedding_size_test)\n",
    "\n",
    "    # Apply the Softplus function to X_train and X_test\n",
    "    X_train_2d_softplus = np.log1p(np.exp(X_train_2d))\n",
    "    X_test_2d_softplus = np.log1p(np.exp(X_test_2d))\n",
    "\n",
    "    # Scale the input data to make it non-negative\n",
    "    scaler = MinMaxScaler()  \n",
    "    X_train_scaled = scaler.fit_transform(X_train_2d_softplus)\n",
    "    X_test_scaled = scaler.transform(X_test_2d_softplus)\n",
    "\n",
    "    # Create an LDA model with desired parameters\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=n_components,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Fit the LDA model on the training data\n",
    "    lda_model.fit(X_train_scaled)\n",
    "\n",
    "    # Save the trained LDA model\n",
    "    joblib.dump(lda_model, model_save_path)\n",
    "    print(\"Trained LDA model saved at:\", model_save_path)\n",
    "\n",
    "    # Predict the topics for the test data\n",
    "    topic_predictions_test = lda_model.transform(X_test_scaled)\n",
    "\n",
    "    # Map numerical indices to class names\n",
    "    predicted_class_names_test = [cluster_class_mapping[prediction] for prediction in np.argmax(topic_predictions_test, axis=1)]\n",
    "\n",
    "    # Print the classification report for the test data\n",
    "    print(\"Classification Report for Test Data:\")\n",
    "    print(classification_report(class_name, predicted_class_names_test))\n",
    "\n",
    "    # Print the confusion matrix for the test data\n",
    "    print(\"Confusion Matrix for Test Data:\")\n",
    "    print(confusion_matrix(class_name, predicted_class_names_test))\n",
    "\n",
    "    # Calculate and print the accuracy for the test data\n",
    "    accuracy_test = np.mean(class_name == predicted_class_names_test)\n",
    "    print(\"Accuracy for Test Data:\", accuracy_test)\n",
    "\n",
    "    # Calculate and print the precision, recall, and F1-score for the test data\n",
    "    precision_test, recall_test, f1_score_test, _ = precision_recall_fscore_support(class_name, predicted_class_names_test, average='weighted')\n",
    "    print(\"Precision for Test Data:\", precision_test)\n",
    "    print(\"Recall for Test Data:\", recall_test)\n",
    "    print(\"F1 Score for Test Data:\", f1_score_test)\n",
    "\n",
    "# Usage example:\n",
    "lda_topic_modeling(X_train, X_test, filtered_test_df['class_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate to cover all possible combinations of ngram_range:\n",
    "# 1- (1, 1) , (1, 2) , (1, 3)and so on till (1,15)\n",
    "# 2- (2, 2) , (2, 3) , (2, 4)and so on till (2,15)\n",
    "# 3- (3, 3) , (3, 4) , (3, 5)and so on till (3,15)\n",
    "# and so on till (15, 15)\n",
    "\n",
    "# Create a list of tuples containing the range of n-grams to consider\n",
    "ngram_ranges = [(i, j) for i in range(1, 16) for j in range(i, 16)]\n",
    "\n",
    "# Iterate over each n-gram range\n",
    "for ngram_range in ngram_ranges:\n",
    "    print(\"N-gram Range:\", ngram_range)\n",
    "    # Perform topic modeling using LDA\n",
    "    lda_topic_modeling(X_train, X_test, filtered_test_df['class_name'], n_components=2, random_state=42, model_save_path='lda_model.pkl')\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
