{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model using pickle\n",
    "with open('word2vec_model.pkl', 'wb') as f:\n",
    "    pickle.dump(word2vec_model, f)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the saved Word2Vec model\n",
    "word2vec_model = KeyedVectors.load('/content/word2vec_model.pkl')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "# import \n",
    "\n",
    "# Get the vocabulary from the pre-trained model\n",
    "vocabulary = word2vec_model.wv.vocab\n",
    "\n",
    "# Fine-tune Word2Vec model using your data\n",
    "filtered_sentences = [[word for word in word_tokenize(description) if word in vocabulary] \n",
    "                      for description in filtered_train_df['bug_description']]\n",
    "\n",
    "\n",
    "# Create and train the Word2Vec model\n",
    "fine_tuned_word2vec_model = Word2Vec(size=300, min_count=1)\n",
    "fine_tuned_word2vec_model.build_vocab(filtered_sentences, update=True)\n",
    "fine_tuned_word2vec_model.train(filtered_sentences, total_examples=len(filtered_sentences), epochs=10)\n",
    "\n",
    "# Function to get average word vector for a description\n",
    "def get_average_word_vector(description, word_embeddings_model, embedding_size):\n",
    "    words = word_tokenize(description)\n",
    "    vectors = [word_embeddings_model[word] for word in words if word in word_embeddings_model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_size)  # Return zero vector if no words are found in the model\n",
    "\n",
    "# Tokenize and get document vectors for training data\n",
    "train_embeddings = np.array([get_average_word_vector(description, fine_tuned_word2vec_model, 300) \n",
    "                             for description in filtered_train_df['bug_description']])\n",
    "\n",
    "# Tokenize and get document vectors for testing data\n",
    "test_embeddings = np.array([get_average_word_vector(description, fine_tuned_word2vec_model, 300) \n",
    "                            for description in filtered_test_df['bug_description']])\n",
    "\n",
    "# Perform clustering (K-means) on training data\n",
    "num_clusters = len(class_name_mapping)\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "train_clusters = kmeans.fit_predict(train_embeddings)\n",
    "\n",
    "# Train SVM model with tuned hyperparameters\n",
    "svm_model = SVC(kernel='linear', C=100)\n",
    "svm_model.fit(train_embeddings, train_clusters)\n",
    "\n",
    "# Make predictions on the entire test set\n",
    "test_clusters_pred = svm_model.predict(test_embeddings)\n",
    "\n",
    "# Define a mapping function to map labels\n",
    "def map_labels(label):\n",
    "    if label == 'Frontend':\n",
    "        return 1\n",
    "    elif label == 'Backend':\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the mapping function to filtered_test_df['class_name']\n",
    "mapped_labels = filtered_test_df['class_name'].map(map_labels)\n",
    "\n",
    "# Calculate accuracy with mapped labels\n",
    "accuracy = accuracy_score(mapped_labels, test_clusters_pred)\n",
    "print(\"Accuracy of SVM model:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
