{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GfR4Yo-rtJZ",
        "outputId": "e39a3b15-2d3c-42d2-871c-63fdb2bfd7af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukDBCJvXqwYc",
        "outputId": "5ecbab2d-05eb-430d-a3f6-fea365605ba7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# remove the stop words from the preprocessed data using nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5BXZfCGyqwYd"
      },
      "outputs": [],
      "source": [
        "# path of training data\n",
        "train_path = 'train.xlsx'\n",
        "\n",
        "# path of testing data\n",
        "test_path = 'test.xlsx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTa0KIKRqwYd",
        "outputId": "d7f2c46c-e1df-483b-afb0-4a3a7c288dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              report class_name  class_index\n",
            "0  \"For any event on my bookmarked projects\" opti...    Backend            1\n",
            "1           Switch to using full l10n id's in urlbar   Frontend            2\n",
            "2  Consider removing hasicon property to simplify...   Frontend            2\n",
            "3  Method to obtain current URL from WebBrowserEd...   Frontend            2\n",
            "4              Fix: migration fails in MS SQL-Server    Backend            1\n",
            "                                              report class_name  class_index\n",
            "0  REST API - ability to list sub projects for a ...    Backend            1\n",
            "1  support selective text on right if set in GNOM...   Frontend            2\n",
            "2  [meta][userstory] Ship v1 of Pre-populated top...   Frontend            2\n",
            "3  Include updated_on and passwd_changed_on colum...    Backend            1\n",
            "4    Problem with email integration to MS Office 365    Backend            1\n"
          ]
        }
      ],
      "source": [
        "# show the first 5 rows of the training data\n",
        "train_df = pd.read_excel(train_path)\n",
        "print(train_df.head())\n",
        "\n",
        "# show the first 5 rows of the testing data\n",
        "test_df = pd.read_excel(test_path)\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0rcr0kUBqwYe"
      },
      "outputs": [],
      "source": [
        "def convert_lower_case(data):\n",
        "    return str(data).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DqVvCOicqwYe"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in symbols:\n",
        "        data = np.char.replace(data, i, ' ')\n",
        "\n",
        "    return str(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q4-p9BLIqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B2RoS_VXqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(data):\n",
        "    return re.sub(r'\\d+', '', str(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kicK-_AdqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_single_characters(tokens):\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        if len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NFCFEuimqwYg"
      },
      "outputs": [],
      "source": [
        "def lemmatization(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(data)\n",
        "    data = remove_single_characters(tokens)\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "    return lemmatized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OGszj2RZqwYg"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_numbers(data)\n",
        "    data = lemmatization(data)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abOxDwffqwYg",
        "outputId": "44b580da-5234-4470-f9b2-c111a38fbcee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"For any event on my bookmarked projects\" option not sending notifications for non-member bookmarked projects\n"
          ]
        }
      ],
      "source": [
        "# print the first report of the training data\n",
        "print(train_df['report'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3tPIbcjqwYh",
        "outputId": "058ea26b-e9de-44c8-b737-bf8ebd07ab5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"For any event on my bookmarked projects\" option not sending notifications for non-member bookmarked projects\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming train_df is your DataFrame containing training data\n",
        "# Assuming train_df has columns 'id', 'report', and 'class_name'\n",
        "\n",
        "# Initialize counter\n",
        "counter = 1\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "with open('preprocessed_train_data.csv', 'w', encoding='utf-8') as f:\n",
        "    f.write('bug_description , class_name\\n')  # Write header\n",
        "\n",
        "    for _, row in train_df.iterrows():\n",
        "        # Preprocess the 'report' column\n",
        "        preprocessed_report = preprocess(row['report'])\n",
        "\n",
        "        # Write data to the file with incremented counter\n",
        "        f.write(f\"{preprocessed_report},{row['class_name']}\\n\")\n",
        "\n",
        "# Print the first preprocessed report\n",
        "print(train_df['report'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELBhql3SqwYh",
        "outputId": "5ad19d94-5812-4eee-94d2-ae1d25552b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REST API - ability to list sub projects for a project\n"
          ]
        }
      ],
      "source": [
        "# FOR TESTING DATA\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "with open('preprocessed_test_data.csv', 'w', encoding='utf-8') as f:\n",
        "    f.write('bug_description , class_name\\n')  # Write header\n",
        "\n",
        "    for _, row in test_df.iterrows():\n",
        "        # Preprocess the 'report' column\n",
        "        preprocessed_report = preprocess(row['report'])\n",
        "\n",
        "        # Write data to the file with incremented counter\n",
        "        f.write(f\"{preprocessed_report},{row['class_name']}\\n\")\n",
        "\n",
        "# Print the first preprocessed report\n",
        "print(test_df['report'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTg4VYxXqwYh"
      },
      "outputs": [],
      "source": [
        "# Open the preprocessed data file for reading\n",
        "with open('preprocessed_train_data.csv', 'r', encoding='utf-8') as f:\n",
        "    # Open the new file for writing preprocessed data with 2 columns\n",
        "    with open('preprocessed_train_data2.csv', 'w', encoding='utf-8') as f_out:\n",
        "        # Iterate over each line in the file\n",
        "        for i, line in enumerate(f):\n",
        "            # Skip the header\n",
        "            if i == 0:\n",
        "                continue\n",
        "\n",
        "            # Split the line into columns based on comma delimiter\n",
        "            columns = line.strip().split(',')\n",
        "\n",
        "            # Check the number of columns\n",
        "            if len(columns) == 2:\n",
        "                # Get the preprocessed report and class name\n",
        "                preprocessed_report = columns[0]\n",
        "                class_name = columns[1]\n",
        "\n",
        "                # Write the preprocessed report and class name to the new file\n",
        "                f_out.write(f\"{preprocessed_report},{class_name}\\n\")\n",
        "            else:\n",
        "                # Skip the line if it doesn't have exactly 2 columns\n",
        "                continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiXB0GixqwYh"
      },
      "outputs": [],
      "source": [
        "# FOR TESTING DATA\n",
        "\n",
        "# Open the preprocessed data file for reading\n",
        "with open('preprocessed_test_data.csv', 'r', encoding='utf-8') as f:\n",
        "    # Open the new file for writing preprocessed data with 2 columns\n",
        "    with open('preprocessed_test_data2.csv', 'w', encoding='utf-8') as f_out:\n",
        "        # Iterate over each line in the file\n",
        "        for i, line in enumerate(f):\n",
        "            # Skip the header\n",
        "            if i == 0:\n",
        "                continue\n",
        "\n",
        "            # Split the line into columns based on comma delimiter\n",
        "            columns = line.strip().split(',')\n",
        "\n",
        "            # Check the number of columns\n",
        "            if len(columns) == 2:\n",
        "                # Get the preprocessed report and class name\n",
        "                preprocessed_report = columns[0]\n",
        "                class_name = columns[1]\n",
        "\n",
        "                # Write the preprocessed report and class name to the new file\n",
        "                f_out.write(f\"{preprocessed_report},{class_name}\\n\")\n",
        "            else:\n",
        "                # Skip the line if it doesn't have exactly 2 columns\n",
        "                continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwwT_5_PqwYi",
        "outputId": "ba8250c1-3e42-462c-a18b-4f1a02589e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     bug_description class_name\n",
            "0  for any event on my bookmarked project option ...    Backend\n",
            "1               switch to using full ln id in urlbar   Frontend\n",
            "2  consider removing hasicon property to simplify...   Frontend\n",
            "3  method to obtain current url from webbrowsered...   Frontend\n",
            "4                fix migration fails in m sql server    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_train_df = pd.read_csv('preprocessed_train_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlvdLrF6qwYi",
        "outputId": "8b560870-11e6-41ae-e7b1-69cb35f06c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     bug_description class_name\n",
            "0  rest api ability to list sub project for a pro...    Backend\n",
            "1  support selective text on right if set in gnom...   Frontend\n",
            "2  meta userstory ship v of pre populated topsite...   Frontend\n",
            "3  include updated on and passwd changed on colum...    Backend\n",
            "4         problem with email integration to m office    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_test_df = pd.read_csv('preprocessed_test_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFUU3HvrsIOl",
        "outputId": "69641f9f-c49e-4fc0-bd00-a9e4b22249fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwvzxTrqqwYi",
        "outputId": "86618e85-bca8-4af2-8797-b72b26385592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for any event on my bookmarked project option not sending notification for non member bookmarked project\n",
            "event bookmarked project option sending notification non member bookmarked project\n",
            "rest api ability to list sub project for a project\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stop_words(data):\n",
        "    tokens = word_tokenize(data)\n",
        "    data = ' '.join([i for i in tokens if not i in stop_words])\n",
        "    return data\n",
        "\n",
        "# preprocess the first report of the training data\n",
        "print(preprocess(train_df['report'][0]))\n",
        "\n",
        "# remove the stop words from the preprocessed data\n",
        "print(remove_stop_words(preprocess(train_df['report'][0])))\n",
        "\n",
        "# preprocess the first report of the testing data\n",
        "print(preprocess(test_df['report'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1XWkNBPqwYi",
        "outputId": "b24736f1-23fc-4a6d-acaf-1af720fefebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ],
      "source": [
        "# Convert non-string values to strings in 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: str(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: str(x))\n",
        "\n",
        "# Remove stop words from 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "\n",
        "# Show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the preprocessed testing data\n",
        "print(preprocessed_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7avA5KxfqwYi",
        "outputId": "b8e6bc49-1a7e-4fb3-cdad-c6402e7607ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Training Data:\n",
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "\n",
            "Filtered Testing Data:\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ],
      "source": [
        "# keep only the reports that has class_name of Frontend, Backend\n",
        "# Filter the training data\n",
        "filtered_train_df = preprocessed_train_df[\n",
        "    (preprocessed_train_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Backend')\n",
        "]\n",
        "\n",
        "# Filter the testing data\n",
        "filtered_test_df = preprocessed_test_df[\n",
        "    (preprocessed_test_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Backend')\n",
        "]\n",
        "\n",
        "# Show the first 5 rows of the filtered training data\n",
        "print(\"Filtered Training Data:\")\n",
        "print(filtered_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the filtered testing data\n",
        "print(\"\\nFiltered Testing Data:\")\n",
        "print(filtered_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYJ8iDYgqwYi",
        "outputId": "2aff1874-38df-4498-cc78-d8a43b1b21f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Backend' 'Frontend']\n",
            "['Backend' 'Frontend']\n"
          ]
        }
      ],
      "source": [
        "# print the unique class names in the training data\n",
        "print(filtered_train_df['class_name'].unique())\n",
        "\n",
        "# print the unique class names in the testing data\n",
        "print(filtered_test_df['class_name'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esX-WwIwqwYj"
      },
      "source": [
        "## Feature Exraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZB99m8gvAcs",
        "outputId": "6d51d91a-23b8-4612-e360-acaece7fd5af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13236\n"
          ]
        }
      ],
      "source": [
        "print(len(filtered_train_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OSrS3j1Etzf7"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "M6vB5Br5t07q"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'  # Specify the desired BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 1024\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenized_bug_reports = [tokenizer.encode(text, add_special_tokens=True) for text in filtered_train_df['bug_description']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EKzfJblIt28S"
      },
      "outputs": [],
      "source": [
        "# Function to process a batch and obtain embeddings\n",
        "def process_batch(batch):\n",
        "    # Pad sequences to a fixed length within the batch\n",
        "    max_length = max(len(seq) for seq in batch)\n",
        "    padded_sequences = [seq + [0]*(max_length-len(seq)) for seq in batch]\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    batch_tensors = torch.tensor(padded_sequences)\n",
        "\n",
        "    # Obtain the embeddings\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        embeddings = model(batch_tensors)[0]\n",
        "\n",
        "    # Convert embeddings to numpy array\n",
        "    return embeddings.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "uq-vFvV11VWG",
        "outputId": "d6ec5309-e56a-457f-96ce-899a64f7ce85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 33 and the array at index 1 has size 46",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-2c1c147fb6af>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Concatenate batch embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Print the shape of X_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 33 and the array at index 1 has size 46"
          ]
        }
      ],
      "source": [
        "# Define a function to pad sequences within a batch\n",
        "def pad_batch(batch):\n",
        "    # Find the maximum length within the batch\n",
        "    max_length = max(len(seq) for seq in batch)\n",
        "    # Pad sequences to the maximum length\n",
        "    padded_sequences = [seq + [0]*(max_length-len(seq)) for seq in batch]\n",
        "    return padded_sequences\n",
        "\n",
        "# Process the data in batches\n",
        "num_batches = len(tokenized_bug_reports) // batch_size\n",
        "if len(tokenized_bug_reports) % batch_size != 0:\n",
        "    num_batches += 1\n",
        "\n",
        "X_train_batches = []\n",
        "for i in range(num_batches):\n",
        "    print(i)\n",
        "    batch_start = i * batch_size\n",
        "    batch_end = min((i + 1) * batch_size, len(tokenized_bug_reports))\n",
        "    batch = tokenized_bug_reports[batch_start:batch_end]\n",
        "    # Pad the last batch if it has fewer samples\n",
        "    if len(batch) < batch_size:\n",
        "        batch = pad_batch(batch)\n",
        "    batch_embeddings = process_batch(batch)\n",
        "    X_train_batches.append(batch_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBelwrNnqwYk",
        "outputId": "9e345388-fa18-4204-f633-efa3b69a8a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Array 0 size along axis 1: 28\n",
            "Array 1 size along axis 1: 28\n",
            "Array 2 size along axis 1: 28\n",
            "Array 3 size along axis 1: 28\n",
            "Array 4 size along axis 1: 28\n",
            "Array 5 size along axis 1: 28\n",
            "Array 6 size along axis 1: 28\n",
            "Array 7 size along axis 1: 28\n",
            "Array 8 size along axis 1: 28\n",
            "Array 9 size along axis 1: 28\n",
            "Array 10 size along axis 1: 28\n",
            "Array 11 size along axis 1: 28\n",
            "Array 12 size along axis 1: 28\n"
          ]
        }
      ],
      "source": [
        "# Find the minimum size along axis 1\n",
        "min_size = min(arr.shape[1] for arr in truncated_batches)\n",
        "\n",
        "# Truncate arrays to have the same size along axis 1\n",
        "truncated_batches = [arr[:, :min_size] for arr in truncated_batches]\n",
        "\n",
        "# Print the new sizes of each array\n",
        "for i, arr in enumerate(truncated_batches):\n",
        "    print(f\"Array {i} size along axis 1:\", arr.shape[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtvfMihJ_7bH",
        "outputId": "a9330cc1-37dd-43bb-d990-9f2b3cf42796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train: (12324, 28, 768)\n"
          ]
        }
      ],
      "source": [
        "# Concatenate the truncated arrays along axis 0\n",
        "X_train = np.concatenate(truncated_batches, axis=0)\n",
        "\n",
        "# Print the shape of the concatenated array\n",
        "print(\"Shape of X_train:\", X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jr0yLcQqwYk",
        "outputId": "85041821-e7f1-4a1b-aefa-9b588def11ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "['Backend' 'Frontend']\n",
            "class_name\n",
            "Backend     7437\n",
            "Frontend    5799\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "  # Print the number of unique class_name in the training data\n",
        "print(filtered_train_df['class_name'].nunique())\n",
        "\n",
        "# print their unique values\n",
        "print(filtered_train_df['class_name'].unique())\n",
        "\n",
        "# print the number of reports in each class\n",
        "print(filtered_train_df['class_name'].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6I9uy_heqwYk"
      },
      "outputs": [],
      "source": [
        "# Create a mapping between cluster labels and class names\n",
        "cluster_class_mapping = {\n",
        "    1: 'Backend',  # Example mapping, adjust based on your actual clusters\n",
        "    0: 'Frontend',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQVQ72NeqwYl"
      },
      "outputs": [],
      "source": [
        "# I want to train a classifier to predict the class_name based on the bug_description\n",
        "# I want the predictions to be with probabilities\n",
        "# I will use the Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define the features (X) and target (y)\n",
        "X = X_train\n",
        "y = filtered_train_df['class_name'].map(cluster_class_mapping)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = rf_classifier.predict(X_val)\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_val, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
