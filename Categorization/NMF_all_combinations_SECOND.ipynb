{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of training data\n",
    "train_path = 'train.xlsx'\n",
    "\n",
    "# path of testing data\n",
    "test_path = 'test.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 5 rows of the training data\n",
    "train_df = pd.read_excel(train_path)\n",
    "print(train_df.head())\n",
    "\n",
    "# show the first 5 rows of the testing data\n",
    "test_df = pd.read_excel(test_path)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return str(data).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in symbols:\n",
    "        data = np.char.replace(data, i, ' ')\n",
    "\n",
    "    return str(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(data):\n",
    "    return re.sub(r'\\d+', '', str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_characters(tokens):\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(data)\n",
    "    data = remove_single_characters(tokens)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_numbers(data)\n",
    "    data = lemmatization(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first report of the training data\n",
    "print(train_df['report'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the first report of the training data\n",
    "print(preprocess(train_df['report'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed data from the new file\n",
    "preprocessed_train_df = pd.read_csv('preprocessed_train_data2.csv')\n",
    "\n",
    "# show the first 5 rows of the preprocessed training data\n",
    "print(preprocessed_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the preprocessed data from the new file\n",
    "preprocessed_test_df = pd.read_csv('preprocessed_test_data2.csv')\n",
    "\n",
    "# show the first 5 rows of the preprocessed training data\n",
    "print(preprocessed_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stop words from the preprocessed data using nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    tokens = word_tokenize(data)\n",
    "    data = ' '.join([i for i in tokens if not i in stop_words])\n",
    "    return data\n",
    "\n",
    "# preprocess the first report of the training data\n",
    "print(preprocess(preprocessed_train_df['bug_description'][0]))\n",
    "\n",
    "# remove the stop words from the preprocessed data\n",
    "print(remove_stop_words(preprocess(preprocessed_train_df['bug_description'][0])))\n",
    "\n",
    "# preprocess the first report of the testing data\n",
    "print(preprocess(preprocessed_test_df['bug_description'][0]))\n",
    "\n",
    "# remove the stop words from the preprocessed data\n",
    "print(remove_stop_words(preprocess(preprocessed_test_df['bug_description'][0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert non-string values to strings in 'bug_description' column\n",
    "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: str(x))\n",
    "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: str(x))\n",
    "\n",
    "# Remove stop words from 'bug_description' column\n",
    "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
    "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
    "\n",
    "# Show the first 5 rows of the preprocessed training data\n",
    "print(preprocessed_train_df.head())\n",
    "\n",
    "# Show the first 5 rows of the preprocessed testing data\n",
    "print(preprocessed_test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the reports that has class_name of Frontend, Backend, Security, Documentation\n",
    "# Filter the training data\n",
    "filtered_train_df = preprocessed_train_df[\n",
    "    (preprocessed_train_df['class_name'] == 'Frontend') | \n",
    "    (preprocessed_train_df['class_name'] == 'Backend') |\n",
    "    (preprocessed_train_df['class_name'] == 'Security')\n",
    "]\n",
    "\n",
    "# Filter the testing data\n",
    "filtered_test_df = preprocessed_test_df[\n",
    "    (preprocessed_test_df['class_name'] == 'Frontend') | \n",
    "    (preprocessed_test_df['class_name'] == 'Backend') |\n",
    "    (preprocessed_test_df['class_name'] == 'Security') \n",
    "]\n",
    "\n",
    "# Show the first 5 rows of the filtered training data\n",
    "print(\"Filtered Training Data:\")\n",
    "print(filtered_train_df.head())\n",
    "\n",
    "# Show the first 5 rows of the filtered testing data\n",
    "print(\"\\nFiltered Testing Data:\")\n",
    "print(filtered_test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping of class names to the desired order\n",
    "class_name_mapping = {\n",
    "    'Backend': 1,\n",
    "    'Frontend': 0,\n",
    "    'Security': 2\n",
    "}\n",
    "\n",
    "# Map class names in both training and testing data to the desired order\n",
    "filtered_train_df['class_label'] = filtered_train_df['class_name'].map(class_name_mapping)\n",
    "filtered_test_df['class_label'] = filtered_test_df['class_name'].map(class_name_mapping)\n",
    "\n",
    "# order them based on the number of class_label\n",
    "filtered_train_df = filtered_train_df.sort_values(by=['class_label'])\n",
    "filtered_test_df = filtered_test_df.sort_values(by=['class_label'])\n",
    "\n",
    "# Print the unique class names in the training data\n",
    "print(filtered_train_df['class_name'].unique())\n",
    "\n",
    "# Print the unique class names in the testing data\n",
    "print(filtered_test_df['class_name'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Exraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "preprocessed_train_df['bug_description'].fillna('', inplace=True)\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed data\n",
    "X_train = vectorizer.fit_transform(filtered_train_df['bug_description'])\n",
    "\n",
    "# Print the shape of X_train\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the vector representation of the first report\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of unique class_name in the training data\n",
    "print(filtered_train_df['class_name'].nunique())\n",
    "\n",
    "# print their unique values\n",
    "print(filtered_train_df['class_name'].unique())\n",
    "\n",
    "# print the number of reports in each class\n",
    "print(filtered_train_df['class_name'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between cluster labels and class names\n",
    "cluster_class_mapping = {\n",
    "    1: 'Backend',  # Example mapping, adjust based on your actual clusters\n",
    "    0: 'Frontend',\n",
    "    2: 'Security'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_cluster_mapping(data, class_name, cluster_class_mappings, n_components=2, ngram_range=(3, 3), random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of NMF topic modeling with different cluster mappings.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the text data to be modeled.\n",
    "    - class_name: Series containing the class names corresponding to the data.\n",
    "    - cluster_class_mappings: Dictionary containing different cluster mappings.\n",
    "    - n_components: Number of topics to be generated (default is 2).\n",
    "    - ngram_range: Tuple specifying the range of n-grams to consider (default is (1, 2)).\n",
    "    - random_state: Random seed for reproducibility (default is 42).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Initialize the TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "\n",
    "    # Fit and transform the data\n",
    "    X_train = vectorizer.fit_transform(data)\n",
    "\n",
    "    for mapping_name, cluster_class_mapping in cluster_class_mappings.items():\n",
    "        print(f\"Evaluating cluster mapping: {mapping_name}\")\n",
    "\n",
    "        # Initialize the NMF model\n",
    "        nmf_model = NMF(n_components=n_components, random_state=random_state)\n",
    "\n",
    "        # Fit the NMF model\n",
    "        nmf_model.fit(X_train)\n",
    "\n",
    "        # Transform the preprocessed test data\n",
    "        X_test = vectorizer.transform(filtered_test_df['bug_description'])\n",
    "\n",
    "        # Predict the topics for the test data\n",
    "        topic_predictions = nmf_model.transform(X_test)\n",
    "\n",
    "        # Map numerical indices to class names\n",
    "        predicted_class_names = [cluster_class_mapping[prediction] for prediction in np.argmax(topic_predictions, axis=1)]\n",
    "\n",
    "        # Print evaluation metrics\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(class_name, predicted_class_names))\n",
    "\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(class_name, predicted_class_names))\n",
    "\n",
    "        accuracy = np.mean(class_name == predicted_class_names)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "        precision, recall, f1_score, _ = precision_recall_fscore_support(class_name, predicted_class_names, average='weighted')\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 Score:\", f1_score)\n",
    "\n",
    "# Define all possible mappings\n",
    "cluster_class_mappings = {\n",
    "    \"Mapping 1\": {0: 'Frontend', 1: 'Backend', 2: 'Security'},\n",
    "    # Add more mappings if needed\n",
    "}\n",
    "\n",
    "# Usage example:\n",
    "evaluate_cluster_mapping(filtered_train_df['bug_description'], filtered_test_df['class_name'], cluster_class_mappings, n_components=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate to cover all possible combinations of ngram_range:\n",
    "# 1- (2, 2) , (2, 3) , (2, 4)and so on till (2,9)\n",
    "# 2- (3, 3) , (3, 4) , (3, 5)and so on till (3,9)\n",
    "# and so on till (3, 9)\n",
    "# 1 => Backend , 0 => Frontend , 2 => Security\n",
    "\n",
    "# Define all possible mappings\n",
    "cluster_class_mappings = {\n",
    "    \"Mapping 2\": {0: 'Frontend', 1: 'Backend', 2: 'Security'},\n",
    "}\n",
    "\n",
    "# Define all possible n-gram ranges\n",
    "ngram_ranges = [(3, i) for i in range(3, 10)]\n",
    "\n",
    "# Iterate over all possible n-gram ranges\n",
    "for ngram_range in ngram_ranges:\n",
    "    print(f\"Evaluating ngram_range: {ngram_range}\")\n",
    "    evaluate_cluster_mapping(filtered_train_df['bug_description'], filtered_test_df['class_name'], cluster_class_mappings, n_components=2, ngram_range=ngram_range)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
