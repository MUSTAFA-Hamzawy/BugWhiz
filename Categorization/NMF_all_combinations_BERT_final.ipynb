{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nimfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ-_-4bROUdX",
        "outputId": "42a8256a-887f-46f2-8836-b966b17b641e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nimfa in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nimfa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from nimfa) (1.11.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GfR4Yo-rtJZ",
        "outputId": "68610db3-c4a2-4f33-eb66-b3797d929b71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukDBCJvXqwYc",
        "outputId": "6143e23e-ebfe-4e00-898a-cb4ca058e309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# remove the stop words from the preprocessed data using nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5BXZfCGyqwYd"
      },
      "outputs": [],
      "source": [
        "# path of training data\n",
        "train_path = '/content/train.xlsx'\n",
        "\n",
        "# path of testing data\n",
        "test_path = '/content/test.xlsx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTa0KIKRqwYd",
        "outputId": "dc649494-2c89-4c7a-85f1-c8f4afacc7c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              report class_name  class_index\n",
            "0  \"For any event on my bookmarked projects\" opti...    Backend            1\n",
            "1           Switch to using full l10n id's in urlbar   Frontend            2\n",
            "2  Consider removing hasicon property to simplify...   Frontend            2\n",
            "3  Method to obtain current URL from WebBrowserEd...   Frontend            2\n",
            "4              Fix: migration fails in MS SQL-Server    Backend            1\n",
            "                                              report class_name  class_index\n",
            "0  REST API - ability to list sub projects for a ...    Backend            1\n",
            "1  support selective text on right if set in GNOM...   Frontend            2\n",
            "2  [meta][userstory] Ship v1 of Pre-populated top...   Frontend            2\n",
            "3  Include updated_on and passwd_changed_on colum...    Backend            1\n",
            "4    Problem with email integration to MS Office 365    Backend            1\n"
          ]
        }
      ],
      "source": [
        "# show the first 5 rows of the training data\n",
        "train_df = pd.read_excel(train_path)\n",
        "print(train_df.head())\n",
        "\n",
        "# show the first 5 rows of the testing data\n",
        "test_df = pd.read_excel(test_path)\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0rcr0kUBqwYe"
      },
      "outputs": [],
      "source": [
        "def convert_lower_case(data):\n",
        "    return str(data).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DqVvCOicqwYe"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in symbols:\n",
        "        data = np.char.replace(data, i, ' ')\n",
        "\n",
        "    return str(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q4-p9BLIqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B2RoS_VXqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(data):\n",
        "    return re.sub(r'\\d+', '', str(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kicK-_AdqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_single_characters(tokens):\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        if len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NFCFEuimqwYg"
      },
      "outputs": [],
      "source": [
        "def lemmatization(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(data)\n",
        "    data = remove_single_characters(tokens)\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "    return lemmatized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OGszj2RZqwYg"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_numbers(data)\n",
        "    data = lemmatization(data)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abOxDwffqwYg",
        "outputId": "0d61e75c-d21b-4393-8fa2-49acfa060b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"For any event on my bookmarked projects\" option not sending notifications for non-member bookmarked projects\n"
          ]
        }
      ],
      "source": [
        "# print the first report of the training data\n",
        "print(train_df['report'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwwT_5_PqwYi",
        "outputId": "9ac67e64-5314-43f5-9edd-79fcff87f350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  for any event on my bookmarked project option ...    Backend\n",
            "1               switch to using full ln id in urlbar   Frontend\n",
            "2  consider removing hasicon property to simplify...   Frontend\n",
            "3  method to obtain current url from webbrowsered...   Frontend\n",
            "4                fix migration fails in m sql server    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_train_df = pd.read_csv('/content/preprocessed_train_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlvdLrF6qwYi",
        "outputId": "2137af14-2f49-4cbb-e258-e534dba137da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  rest api ability to list sub project for a pro...    Backend\n",
            "1  support selective text on right if set in gnom...   Frontend\n",
            "2  meta userstory ship v of pre populated topsite...   Frontend\n",
            "3  include updated on and passwd changed on colum...    Backend\n",
            "4         problem with email integration to m office    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_test_df = pd.read_csv('/content/preprocessed_test_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFUU3HvrsIOl",
        "outputId": "137dedc3-ce4f-401a-9ad5-0517888ea9c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwvzxTrqqwYi",
        "outputId": "4f7dbac1-0ad4-4c36-e919-b1c185febaa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for any event on my bookmarked project option not sending notification for non member bookmarked project\n",
            "event bookmarked project option sending notification non member bookmarked project\n",
            "rest api ability to list sub project for a project\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stop_words(data):\n",
        "    tokens = word_tokenize(data)\n",
        "    data = ' '.join([i for i in tokens if not i in stop_words])\n",
        "    return data\n",
        "\n",
        "# preprocess the first report of the training data\n",
        "print(preprocess(train_df['report'][0]))\n",
        "\n",
        "# remove the stop words from the preprocessed data\n",
        "print(remove_stop_words(preprocess(train_df['report'][0])))\n",
        "\n",
        "# preprocess the first report of the testing data\n",
        "print(preprocess(test_df['report'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1XWkNBPqwYi",
        "outputId": "e0d1f0db-d5bf-4364-8402-1b3b5ee5cb5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ],
      "source": [
        "# Convert non-string values to strings in 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: str(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: str(x))\n",
        "\n",
        "# Remove stop words from 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "\n",
        "# Show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the preprocessed testing data\n",
        "print(preprocessed_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7avA5KxfqwYi",
        "outputId": "209411ae-4264-4c6e-926b-2cd177c95340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Training Data:\n",
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "\n",
            "Filtered Testing Data:\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ],
      "source": [
        "# keep only the reports that has class_name of Frontend, Backend\n",
        "# Filter the training data\n",
        "filtered_train_df = preprocessed_train_df[\n",
        "    (preprocessed_train_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Backend')\n",
        "]\n",
        "\n",
        "# Filter the testing data\n",
        "filtered_test_df = preprocessed_test_df[\n",
        "    (preprocessed_test_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Backend')\n",
        "]\n",
        "\n",
        "# Show the first 5 rows of the filtered training data\n",
        "print(\"Filtered Training Data:\")\n",
        "print(filtered_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the filtered testing data\n",
        "print(\"\\nFiltered Testing Data:\")\n",
        "print(filtered_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYJ8iDYgqwYi",
        "outputId": "6598c500-08c7-441d-aaf1-176e7530cc84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Backend' 'Frontend']\n",
            "['Backend' 'Frontend']\n"
          ]
        }
      ],
      "source": [
        "# print the unique class names in the training data\n",
        "print(filtered_train_df['class_name'].unique())\n",
        "\n",
        "# print the unique class names in the testing data\n",
        "print(filtered_test_df['class_name'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esX-WwIwqwYj"
      },
      "source": [
        "## Feature Exraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(filtered_train_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZB99m8gvAcs",
        "outputId": "2bd36568-7433-4dd3-aded-4b4cbb6cd254"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6I9uy_heqwYk"
      },
      "outputs": [],
      "source": [
        "cluster_class_mapping = {\n",
        "    1: 'Frontend',\n",
        "    0: 'Backend',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'  # Specify the desired BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 512\n",
        "\n",
        "# Tokenize and pad sequences for training data\n",
        "tokenized_bug_reports_train = [tokenizer.encode(text, add_special_tokens=True) for text in filtered_train_df['bug_description']]\n",
        "\n",
        "# Tokenize and pad sequences for test data\n",
        "tokenized_bug_reports_test = [tokenizer.encode(text, add_special_tokens=True) for text in filtered_test_df['bug_description']]\n",
        "\n",
        "# Function to process a batch and obtain embeddings\n",
        "def process_batch(batch):\n",
        "    # Pad sequences to a fixed length within the batch\n",
        "    max_length = max(len(seq) for seq in batch)\n",
        "    padded_sequences = [seq + [0]*(max_length-len(seq)) for seq in batch]\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    batch_tensors = torch.tensor(padded_sequences)\n",
        "\n",
        "    # Obtain the embeddings\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        embeddings = model(batch_tensors)[0]\n",
        "\n",
        "    # Apply ReLU activation to make embeddings non-negative\n",
        "    embeddings = torch.relu(embeddings)\n",
        "\n",
        "    # Convert embeddings to numpy array\n",
        "    return embeddings.numpy()\n",
        "\n",
        "# Define a function to pad sequences within a batch\n",
        "def pad_batch(batch):\n",
        "    # Find the maximum length within the batch\n",
        "    max_length = max(len(seq) for seq in batch)\n",
        "    # Pad sequences to the maximum length\n",
        "    padded_sequences = [seq + [0]*(max_length-len(seq)) for seq in batch]\n",
        "    return padded_sequences\n",
        "\n",
        "# Process the training data in batches\n",
        "num_batches_train = len(tokenized_bug_reports_train) // batch_size\n",
        "if len(tokenized_bug_reports_train) % batch_size != 0:\n",
        "    num_batches_train += 1\n",
        "\n",
        "X_train_batches = []\n",
        "for i in range(num_batches_train):\n",
        "    print(\"Processing training batch\", i)\n",
        "    batch_start = i * batch_size\n",
        "    batch_end = min((i + 1) * batch_size, len(tokenized_bug_reports_train))\n",
        "    batch = tokenized_bug_reports_train[batch_start:batch_end]\n",
        "    # Pad the last batch if it has fewer samples\n",
        "    if len(batch) < batch_size:\n",
        "        batch = pad_batch(batch)\n",
        "    batch_embeddings = process_batch(batch)\n",
        "    X_train_batches.append(batch_embeddings)\n",
        "\n",
        "# Print the shapes of arrays in X_train_batches\n",
        "for i, arr in enumerate(X_train_batches):\n",
        "    print(f\"Shape of array {i}: {arr.shape}\")\n",
        "\n",
        "# Find the maximum size along axis 1\n",
        "max_size_train = max(arr.shape[1] for arr in X_train_batches)\n",
        "\n",
        "# Pad arrays to have the same size along axis 1 for training data\n",
        "padded_batches_train = [np.pad(arr, ((0, 0), (0, max_size_train - arr.shape[1]), (0, 0)), mode='constant', constant_values=0) for arr in X_train_batches]\n",
        "\n",
        "# Concatenate the padded arrays along axis 0 for training data\n",
        "X_train = np.concatenate(padded_batches_train, axis=0)\n",
        "\n",
        "# Print the shape of the concatenated training array\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "\n",
        "# Process the test data in batches\n",
        "num_batches_test = len(tokenized_bug_reports_test) // batch_size\n",
        "if len(tokenized_bug_reports_test) % batch_size != 0:\n",
        "    num_batches_test += 1\n",
        "\n",
        "X_test_batches = []\n",
        "for i in range(num_batches_test):\n",
        "    print(\"Processing test batch\", i)\n",
        "    batch_start = i * batch_size\n",
        "    batch_end = min((i + 1) * batch_size, len(tokenized_bug_reports_test))\n",
        "    batch = tokenized_bug_reports_test[batch_start:batch_end]\n",
        "    # Pad the last batch if it has fewer samples\n",
        "    if len(batch) < batch_size:\n",
        "        batch = pad_batch(batch)\n",
        "    batch_embeddings = process_batch(batch)\n",
        "    X_test_batches.append(batch_embeddings)\n",
        "\n",
        "# Print the shapes of arrays in X_test_batches\n",
        "for i, arr in enumerate(X_test_batches):\n",
        "    print(f\"Shape of array {i}: {arr.shape}\")\n",
        "\n",
        "# Find the maximum size along axis 1 for test data\n",
        "max_size_test = max(arr.shape[1] for arr in X_test_batches)\n",
        "\n",
        "# Pad arrays to have the same size along axis 1 for test data\n",
        "padded_batches_test = [np.pad(arr, ((0, 0), (0, max_size_test - arr.shape[1]), (0, 0)), mode='constant', constant_values=0) for arr in X_test_batches]\n",
        "\n",
        "# Concatenate the padded arrays along axis 0 for test data\n",
        "X_test = np.concatenate(padded_batches_test, axis=0)\n",
        "\n",
        "# Print the shape of the concatenated test array\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "\n",
        "# Use the same cluster class mapping as before\n",
        "cluster_class_mapping = {\n",
        "    1: 'Frontend',\n",
        "    0: 'Backend',\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96JuaST_g87p",
        "outputId": "760c7042-cc65-47a2-d8cc-2e5a4228d442"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training batch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing training batch 1\n",
            "Processing training batch 2\n",
            "Processing training batch 3\n",
            "Processing training batch 4\n",
            "Processing training batch 5\n",
            "Processing training batch 6\n",
            "Processing training batch 7\n",
            "Processing training batch 8\n",
            "Processing training batch 9\n",
            "Processing training batch 10\n",
            "Processing training batch 11\n",
            "Processing training batch 12\n",
            "Processing training batch 13\n",
            "Processing training batch 14\n",
            "Processing training batch 15\n",
            "Processing training batch 16\n",
            "Processing training batch 17\n",
            "Processing training batch 18\n",
            "Processing training batch 19\n",
            "Processing training batch 20\n",
            "Processing training batch 21\n",
            "Processing training batch 22\n",
            "Processing training batch 23\n",
            "Processing training batch 24\n",
            "Processing training batch 25\n",
            "Shape of array 0: (512, 30, 768)\n",
            "Shape of array 1: (512, 33, 768)\n",
            "Shape of array 2: (512, 46, 768)\n",
            "Shape of array 3: (512, 31, 768)\n",
            "Shape of array 4: (512, 37, 768)\n",
            "Shape of array 5: (512, 30, 768)\n",
            "Shape of array 6: (512, 38, 768)\n",
            "Shape of array 7: (512, 29, 768)\n",
            "Shape of array 8: (512, 36, 768)\n",
            "Shape of array 9: (512, 25, 768)\n",
            "Shape of array 10: (512, 28, 768)\n",
            "Shape of array 11: (512, 24, 768)\n",
            "Shape of array 12: (512, 28, 768)\n",
            "Shape of array 13: (512, 27, 768)\n",
            "Shape of array 14: (512, 28, 768)\n",
            "Shape of array 15: (512, 27, 768)\n",
            "Shape of array 16: (512, 25, 768)\n",
            "Shape of array 17: (512, 49, 768)\n",
            "Shape of array 18: (512, 30, 768)\n",
            "Shape of array 19: (512, 27, 768)\n",
            "Shape of array 20: (512, 30, 768)\n",
            "Shape of array 21: (512, 25, 768)\n",
            "Shape of array 22: (512, 29, 768)\n",
            "Shape of array 23: (512, 27, 768)\n",
            "Shape of array 24: (512, 28, 768)\n",
            "Shape of array 25: (436, 30, 768)\n",
            "Shape of X_train: (13236, 49, 768)\n",
            "Processing test batch 0\n",
            "Processing test batch 1\n",
            "Processing test batch 2\n",
            "Processing test batch 3\n",
            "Processing test batch 4\n",
            "Shape of array 0: (512, 43, 768)\n",
            "Shape of array 1: (512, 26, 768)\n",
            "Shape of array 2: (512, 41, 768)\n",
            "Shape of array 3: (512, 31, 768)\n",
            "Shape of array 4: (284, 27, 768)\n",
            "Shape of X_test: (2332, 43, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Save X_train and X_test to files\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('X_test.npy', X_test)\n"
      ],
      "metadata": {
        "id": "zgt0qs7bNP9J"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load X_train and X_test from files\n",
        "X_train = np.load('X_train.npy')\n",
        "X_test = np.load('X_test.npy')\n"
      ],
      "metadata": {
        "id": "1y5wBKOPOsDd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.decomposition import NMF\n",
        "import joblib\n",
        "\n",
        "def nmf_topic_modeling(X_train, X_test, class_name, n_components=2, random_state=42, save_path=None):\n",
        "    \"\"\"\n",
        "    Perform Non-negative Matrix Factorization (NMF) topic modeling on the given data and evaluate on test data.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: Array containing the BERT embeddings for training data.\n",
        "    - X_test: Array containing the BERT embeddings for test data.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - n_components: Number of topics to be generated (default is 2).\n",
        "    - random_state: Random seed for reproducibility (default is 42).\n",
        "    - save_path: Path to save the trained model (default is None).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Reshape X_train and X_test to have two dimensions\n",
        "    num_samples_train, num_tokens_train, embedding_size_train = X_train.shape\n",
        "    X_train_2d = X_train.reshape(num_samples_train * num_tokens_train, embedding_size_train)\n",
        "\n",
        "    num_samples_test, num_tokens_test, embedding_size_test = X_test.shape\n",
        "    X_test_2d = X_test.reshape(num_samples_test * num_tokens_test, embedding_size_test)\n",
        "\n",
        "    # Scale the input data to make it non-negative\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_2d)\n",
        "    X_test_scaled = scaler.transform(X_test_2d)\n",
        "\n",
        "    # Initialize the NMF model\n",
        "    nmf_model = NMF(n_components=n_components, random_state=random_state)\n",
        "\n",
        "    # Fit the NMF model on the training data\n",
        "    nmf_model.fit(X_train_scaled)\n",
        "\n",
        "    # Save the trained model if save_path is provided\n",
        "    if save_path:\n",
        "        joblib.dump(nmf_model, save_path)\n",
        "\n",
        "    # Predict the topics for the test data\n",
        "    topic_predictions_test = nmf_model.transform(X_test_scaled)\n",
        "\n",
        "    # Reshape the predictions back to the shape of the original test data\n",
        "    topic_predictions_test = topic_predictions_test.reshape(num_samples_test, num_tokens_test, n_components)\n",
        "\n",
        "    # Take only the first 1024 samples from the class_name series\n",
        "    class_name_subset = class_name[:num_samples_test]\n",
        "\n",
        "    # Flatten the predictions and class names to align for evaluation\n",
        "    flat_topic_predictions_test = topic_predictions_test.reshape(-1, n_components)\n",
        "    flat_class_name_subset = class_name_subset.repeat(num_tokens_test)\n",
        "\n",
        "    # Map numerical indices to class names\n",
        "    predicted_class_names_test = flat_class_name_subset.unique()[flat_topic_predictions_test.argmax(axis=1)]\n",
        "\n",
        "    # Print the classification report for the test data\n",
        "    print(\"Classification Report for Test Data:\")\n",
        "    print(classification_report(flat_class_name_subset, predicted_class_names_test))\n",
        "\n",
        "    # Print the confusion matrix for the test data\n",
        "    print(\"Confusion Matrix for Test Data:\")\n",
        "    print(confusion_matrix(flat_class_name_subset, predicted_class_names_test))\n",
        "\n",
        "    # Calculate and print the accuracy for the test data\n",
        "    accuracy_test = np.mean(flat_class_name_subset == predicted_class_names_test)\n",
        "    print(\"Accuracy for Test Data:\", accuracy_test)\n",
        "\n",
        "    # Calculate and print the precision, recall, and F1-score for the test data\n",
        "    precision_test, recall_test, f1_score_test, _ = precision_recall_fscore_support(flat_class_name_subset, predicted_class_names_test, average='weighted')\n",
        "    print(\"Precision for Test Data:\", precision_test)\n",
        "    print(\"Recall for Test Data:\", recall_test)\n",
        "    print(\"F1 Score for Test Data:\", f1_score_test)\n",
        "\n",
        "# Usage example:\n",
        "nmf_topic_modeling(X_train, X_test, filtered_test_df['class_name'], n_components=2, random_state=42, save_path=\"nmf_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnXWvUK0FQ9a",
        "outputId": "0b5354a3-e4e6-4039-de75-9a2d39cc27d7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Backend       0.59      0.76      0.66     57835\n",
            "    Frontend       0.46      0.28      0.35     42441\n",
            "\n",
            "    accuracy                           0.56    100276\n",
            "   macro avg       0.53      0.52      0.51    100276\n",
            "weighted avg       0.54      0.56      0.53    100276\n",
            "\n",
            "Confusion Matrix for Test Data:\n",
            "[[43937 13898]\n",
            " [30520 11921]]\n",
            "Accuracy for Test Data: 0.5570425625274243\n",
            "Precision for Test Data: 0.5357611873317091\n",
            "Recall for Test Data: 0.5570425625274243\n",
            "F1 Score for Test Data: 0.530938236720551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "def lda_topic_modeling(X_train, X_test, class_name, n_components=2, random_state=42):\n",
        "    \"\"\"\n",
        "    Perform Latent Dirichlet Allocation (LDA) topic modeling on the given data and evaluate on test data.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: Array containing the BERT embeddings for training data.\n",
        "    - X_test: Array containing the BERT embeddings for test data.\n",
        "    - class_name: Series containing the class names corresponding to the data.\n",
        "    - n_components: Number of topics to be generated (default is 2).\n",
        "    - random_state: Random seed for reproducibility (default is 42).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Reshape X_train and X_test to have two dimensions\n",
        "    num_samples_train, num_tokens_train, embedding_size_train = X_train.shape\n",
        "    X_train_2d = X_train.reshape(num_samples_train * num_tokens_train, embedding_size_train)\n",
        "\n",
        "    num_samples_test, num_tokens_test, embedding_size_test = X_test.shape\n",
        "    X_test_2d = X_test.reshape(num_samples_test * num_tokens_test, embedding_size_test)\n",
        "\n",
        "    # Scale the input data to make it non-negative\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_2d)\n",
        "    X_test_scaled = scaler.transform(X_test_2d)\n",
        "\n",
        "    # Initialize the LDA model\n",
        "    lda_model = LatentDirichletAllocation(n_components=n_components, random_state=random_state)\n",
        "\n",
        "    # Fit the LDA model on the training data\n",
        "    lda_model.fit(X_train_scaled)\n",
        "\n",
        "    # Predict the topics for the test data\n",
        "    topic_predictions_test = lda_model.transform(X_test_scaled)\n",
        "\n",
        "    # Reshape the predictions back to the shape of the original test data\n",
        "    topic_predictions_test = topic_predictions_test.reshape(num_samples_test, num_tokens_test, n_components)\n",
        "\n",
        "    # Take only the first 1024 samples from the class_name series\n",
        "    class_name_subset = class_name[:num_samples_test]\n",
        "\n",
        "    # Flatten the predictions and class names to align for evaluation\n",
        "    flat_topic_predictions_test = topic_predictions_test.reshape(-1, n_components)\n",
        "    flat_class_name_subset = class_name_subset.repeat(num_tokens_test)\n",
        "\n",
        "    # Map numerical indices to class names\n",
        "    predicted_class_names_test = flat_class_name_subset.unique()[flat_topic_predictions_test.argmax(axis=1)]\n",
        "\n",
        "    # Print the classification report for the test data\n",
        "    print(\"Classification Report for Test Data:\")\n",
        "    print(classification_report(flat_class_name_subset, predicted_class_names_test))\n",
        "\n",
        "    # Print the confusion matrix for the test data\n",
        "    print(\"Confusion Matrix for Test Data:\")\n",
        "    print(confusion_matrix(flat_class_name_subset, predicted_class_names_test))\n",
        "\n",
        "    # Calculate and print the accuracy for the test data\n",
        "    accuracy_test = np.mean(flat_class_name_subset == predicted_class_names_test)\n",
        "    print(\"Accuracy for Test Data:\", accuracy_test)\n",
        "\n",
        "    # Calculate and print the precision, recall, and F1-score for the test data\n",
        "    precision_test, recall_test, f1_score_test, _ = precision_recall_fscore_support(flat_class_name_subset, predicted_class_names_test, average='weighted')\n",
        "    print(\"Precision for Test Data:\", precision_test)\n",
        "    print(\"Recall for Test Data:\", recall_test)\n",
        "    print(\"F1 Score for Test Data:\", f1_score_test)\n",
        "\n",
        "# Usage example:\n",
        "lda_topic_modeling(X_train, X_test, filtered_test_df['class_name'], n_components=2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "2CocbntJn_tJ",
        "outputId": "9d385e59-9e12-454a-ac31-be60a3dcc5c4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-a74478eb8aa0>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Usage example:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mlda_topic_modeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_test_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-a74478eb8aa0>\u001b[0m in \u001b[0;36mlda_topic_modeling\u001b[0;34m(X_train, X_test, class_name, n_components, random_state)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Fit the LDA model on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Predict the topics for the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                     \u001b[0;31m# batch update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                     self._em_step(\n\u001b[0m\u001b[1;32m    669\u001b[0m                         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;31m# E-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         _, suff_stats = self._e_step(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparallel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         results = parallel(\n\u001b[0m\u001b[1;32m    461\u001b[0m             delayed(_update_doc_distribution)(\n\u001b[1;32m    462\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# The optimal phi_{dwk} is proportional to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mnorm_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_doc_topic_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_topic_word_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mdoc_topic_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_doc_topic_d\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_topic_word_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "QE4ymXUrqwYk",
        "outputId": "60500c6b-e226-4824-f562-364d0dbc3587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-gram Range: (1, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'tuple' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-2a0d424a7a2b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"N-gram Range:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Perform topic modeling using LDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnmf_topic_modeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_test_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ],
      "source": [
        "# Create a list of tuples containing the range of n-grams to consider\n",
        "ngram_ranges = [(i, j) for i in range(1, 16) for j in range(i, 16)]\n",
        "\n",
        "# Iterate over each n-gram range\n",
        "for ngram_range in ngram_ranges:\n",
        "    print(\"N-gram Range:\", ngram_range)\n",
        "    # Perform topic modeling using LDA\n",
        "    nmf_topic_modeling(X_train, X_test, filtered_test_df['class_name'], n_components=2, random_state=42)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTa3eCnuoZMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}