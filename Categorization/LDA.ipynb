{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of training data\n",
    "train_path = 'train.xlsx'\n",
    "\n",
    "# path of testing data\n",
    "test_path = 'test.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              report class_name  class_index\n",
      "0  \"For any event on my bookmarked projects\" opti...    Backend            1\n",
      "1           Switch to using full l10n id's in urlbar   Frontend            2\n",
      "2  Consider removing hasicon property to simplify...   Frontend            2\n",
      "3  Method to obtain current URL from WebBrowserEd...   Frontend            2\n",
      "4              Fix: migration fails in MS SQL-Server    Backend            1\n",
      "                                              report class_name  class_index\n",
      "0  REST API - ability to list sub projects for a ...    Backend            1\n",
      "1  support selective text on right if set in GNOM...   Frontend            2\n",
      "2  [meta][userstory] Ship v1 of Pre-populated top...   Frontend            2\n",
      "3  Include updated_on and passwd_changed_on colum...    Backend            1\n",
      "4    Problem with email integration to MS Office 365    Backend            1\n"
     ]
    }
   ],
   "source": [
    "# show the first 5 rows of the training data\n",
    "train_df = pd.read_excel(train_path)\n",
    "print(train_df.head())\n",
    "\n",
    "# show the first 5 rows of the testing data\n",
    "test_df = pd.read_excel(test_path)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return str(data).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in symbols:\n",
    "        data = np.char.replace(data, i, ' ')\n",
    "\n",
    "    return str(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(data):\n",
    "    return re.sub(r'\\d+', '', str(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_characters(tokens):\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(data)\n",
    "    data = remove_single_characters(tokens)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_numbers(data)\n",
    "    data = lemmatization(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"For any event on my bookmarked projects\" option not sending notifications for non-member bookmarked projects\n"
     ]
    }
   ],
   "source": [
    "# print the first report of the training data\n",
    "print(train_df['report'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for any event on my bookmarked project option not sending notification for non member bookmarked project\n"
     ]
    }
   ],
   "source": [
    "# preprocess the first report of the training data\n",
    "print(preprocess(train_df['report'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"For any event on my bookmarked projects\" option not sending notifications for non-member bookmarked projects\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_df is your DataFrame containing training data\n",
    "# Assuming train_df has columns 'id', 'report', and 'class_name'\n",
    "\n",
    "# Initialize counter\n",
    "counter = 1\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "with open('preprocessed_train_data.csv', 'w', encoding='utf-8') as f:\n",
    "    f.write('bug_description , class_name\\n')  # Write header\n",
    "\n",
    "    for _, row in train_df.iterrows():\n",
    "        # Preprocess the 'report' column\n",
    "        preprocessed_report = preprocess(row['report'])\n",
    "\n",
    "        # Write data to the file with incremented counter\n",
    "        f.write(f\"{preprocessed_report},{row['class_name']}\\n\")\n",
    "\n",
    "# Print the first preprocessed report\n",
    "print(train_df['report'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REST API - ability to list sub projects for a project\n"
     ]
    }
   ],
   "source": [
    "# FOR TESTING DATA\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "with open('preprocessed_test_data.csv', 'w', encoding='utf-8') as f:\n",
    "    f.write('bug_description , class_name\\n')  # Write header\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        # Preprocess the 'report' column\n",
    "        preprocessed_report = preprocess(row['report'])\n",
    "\n",
    "        # Write data to the file with incremented counter\n",
    "        f.write(f\"{preprocessed_report},{row['class_name']}\\n\")\n",
    "\n",
    "# Print the first preprocessed report\n",
    "print(test_df['report'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the preprocessed data file for reading\n",
    "with open('preprocessed_train_data.csv', 'r', encoding='utf-8') as f:\n",
    "    # Open the new file for writing preprocessed data with 2 columns\n",
    "    with open('preprocessed_train_data2.csv', 'w', encoding='utf-8') as f_out:\n",
    "        # Iterate over each line in the file\n",
    "        for i, line in enumerate(f):\n",
    "            # Skip the header\n",
    "            if i == 0:\n",
    "                continue\n",
    "            \n",
    "            # Split the line into columns based on comma delimiter\n",
    "            columns = line.strip().split(',')\n",
    "            \n",
    "            # Check the number of columns\n",
    "            if len(columns) == 2:\n",
    "                # Get the preprocessed report and class name\n",
    "                preprocessed_report = columns[0]\n",
    "                class_name = columns[1]\n",
    "                \n",
    "                # Write the preprocessed report and class name to the new file\n",
    "                f_out.write(f\"{preprocessed_report},{class_name}\\n\")\n",
    "            else:\n",
    "                # Skip the line if it doesn't have exactly 2 columns\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TESTING DATA\n",
    "\n",
    "# Open the preprocessed data file for reading\n",
    "with open('preprocessed_test_data.csv', 'r', encoding='utf-8') as f:\n",
    "    # Open the new file for writing preprocessed data with 2 columns\n",
    "    with open('preprocessed_test_data2.csv', 'w', encoding='utf-8') as f_out:\n",
    "        # Iterate over each line in the file\n",
    "        for i, line in enumerate(f):\n",
    "            # Skip the header\n",
    "            if i == 0:\n",
    "                continue\n",
    "            \n",
    "            # Split the line into columns based on comma delimiter\n",
    "            columns = line.strip().split(',')\n",
    "            \n",
    "            # Check the number of columns\n",
    "            if len(columns) == 2:\n",
    "                # Get the preprocessed report and class name\n",
    "                preprocessed_report = columns[0]\n",
    "                class_name = columns[1]\n",
    "                \n",
    "                # Write the preprocessed report and class name to the new file\n",
    "                f_out.write(f\"{preprocessed_report},{class_name}\\n\")\n",
    "            else:\n",
    "                # Skip the line if it doesn't have exactly 2 columns\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     bug_description class_name\n",
      "0  for any event on my bookmarked project option ...    Backend\n",
      "1               switch to using full ln id in urlbar   Frontend\n",
      "2  consider removing hasicon property to simplify...   Frontend\n",
      "3  method to obtain current url from webbrowsered...   Frontend\n",
      "4                fix migration fails in m sql server    Backend\n"
     ]
    }
   ],
   "source": [
    "# read the preprocessed data from the new file\n",
    "preprocessed_train_df = pd.read_csv('preprocessed_train_data2.csv')\n",
    "\n",
    "# show the first 5 rows of the preprocessed training data\n",
    "print(preprocessed_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     bug_description class_name\n",
      "0  rest api ability to list sub project for a pro...    Backend\n",
      "1  support selective text on right if set in gnom...   Frontend\n",
      "2  meta userstory ship v of pre populated topsite...   Frontend\n",
      "3  include updated on and passwd changed on colum...    Backend\n",
      "4         problem with email integration to m office    Backend\n"
     ]
    }
   ],
   "source": [
    "# read the preprocessed data from the new file\n",
    "preprocessed_test_df = pd.read_csv('preprocessed_test_data2.csv')\n",
    "\n",
    "# show the first 5 rows of the preprocessed training data\n",
    "print(preprocessed_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Exraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13866, 7663)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "preprocessed_train_df['bug_description'].fillna('', inplace=True)\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed data\n",
    "X_train = vectorizer.fit_transform(preprocessed_train_df['bug_description'])\n",
    "\n",
    "# Print the shape of the transformed data\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3991)\t0.2538660078657135\n",
      "  (0, 4326)\t0.22188781122260037\n",
      "  (0, 4360)\t0.16200943672815057\n",
      "  (0, 5895)\t0.26840990743892756\n",
      "  (0, 4344)\t0.12359962887671232\n",
      "  (0, 4562)\t0.18653498271477986\n",
      "  (0, 5086)\t0.3360730020928219\n",
      "  (0, 692)\t0.6032988290912195\n",
      "  (0, 4234)\t0.2431528345726809\n",
      "  (0, 4482)\t0.13533242661691539\n",
      "  (0, 2189)\t0.24482610182777964\n",
      "  (0, 299)\t0.25719129168101634\n",
      "  (0, 2527)\t0.2398225552777558\n"
     ]
    }
   ],
   "source": [
    "# print the vector representation of the first report\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['Backend' 'Frontend' 'Security' 'Documentation' 'Performance']\n",
      "Backend          7437\n",
      "Frontend         5799\n",
      "Security          367\n",
      "Documentation     174\n",
      "Performance        89\n",
      "Name: class_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the number of unique class_name in the training data\n",
    "print(preprocessed_train_df['class_name'].nunique())\n",
    "\n",
    "# print their unique values\n",
    "print(preprocessed_train_df['class_name'].unique())\n",
    "\n",
    "# print the number of reports in each class\n",
    "print(preprocessed_train_df['class_name'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['field', 'on', 'api', 'the', 'custom', 'to', 'issue', 'in', 'notification', 'email']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['eclipse', 'java', 'on', 'not', 'with', 'for', 'in', 'to', 'error', 'project']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['to', 'is', 'not', 'rest', 'user', 'for', 'issue', 'the', 'in', 'api']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['bar', 'and', 'when', 'search', 'is', 'menu', 'to', 'in', 'the', 'tab']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['redmine', 'user', 'email', 'of', 'notification', 'not', 'in', 'for', 'the', 'to']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets try topic modeling using LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Initialize the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "\n",
    "# Fit the LDA model on the training data\n",
    "lda.fit(X_train)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([vectorizer.get_feature_names_out()[index] for index in topic.argsort()[-10:]])\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend          7437\n",
      "Frontend         5799\n",
      "Security          367\n",
      "Documentation     174\n",
      "Performance        89\n",
      "Name: class_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print each class and the number of reports in that class from class_name column\n",
    "print(preprocessed_train_df['class_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of transformed test data: (2441, 7663)\n",
      "Shape of predictions: (2441, 5)\n",
      "First prediction: [0.05777235 0.77220967 0.05812538 0.05529076 0.05660184]\n",
      "Predicted class for the first test report: 1\n",
      "First 5 predicted classes: [1, 0, 3, 2, 4]\n",
      "First 5 actual classes: 0     Backend\n",
      "1    Frontend\n",
      "2    Frontend\n",
      "3     Backend\n",
      "4     Backend\n",
      "Name: class_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace NaN values with an empty string\n",
    "preprocessed_test_df['bug_description'].fillna('', inplace=True)\n",
    "\n",
    "# Transform the preprocessed test data\n",
    "X_test = vectorizer.transform(preprocessed_test_df['bug_description'])\n",
    "\n",
    "# Print the shape of the transformed test data\n",
    "print(\"Shape of transformed test data:\", X_test.shape)\n",
    "\n",
    "# Predict the class of the test data\n",
    "predictions = lda.transform(X_test)\n",
    "\n",
    "# Print the shape of the predictions\n",
    "print(\"Shape of predictions:\", predictions.shape)\n",
    "\n",
    "# Print the first prediction\n",
    "print(\"First prediction:\", predictions[0])\n",
    "\n",
    "# Get the predicted class for each test report\n",
    "predicted_class = [np.argmax(prediction) for prediction in predictions]\n",
    "\n",
    "# Print the predicted class for the first test report\n",
    "print(\"Predicted class for the first test report:\", predicted_class[0])\n",
    "\n",
    "# Print the first 5 predicted classes\n",
    "print(\"First 5 predicted classes:\", predicted_class[:5])\n",
    "\n",
    "# Print the first 5 actual classes\n",
    "print(\"First 5 actual classes:\", preprocessed_test_df['class_name'][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend          1345\n",
      "Frontend          987\n",
      "Security           70\n",
      "Documentation      21\n",
      "Performance        18\n",
      "Name: class_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print each class and the number of reports in that class after training the model\n",
    "print(preprocessed_test_df['class_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend          1345\n",
      "Frontend          987\n",
      "Security           70\n",
      "Documentation      21\n",
      "Performance        18\n",
      "Name: class_name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print each class and the number of reports in that class\n",
    "print(preprocessed_test_df['class_name'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test report: when choosing a different application to open a file the chosen application is ignored\n",
      "Actual class: Backend\n",
      "Predicted class: 3\n"
     ]
    }
   ],
   "source": [
    "# take a random test report and its predicted class\n",
    "random_index = 11\n",
    "print(\"Test report:\", preprocessed_test_df['bug_description'][random_index])\n",
    "print(\"Actual class:\", preprocessed_test_df['class_name'][random_index])\n",
    "print(\"Predicted class:\", predicted_class[random_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between cluster labels and class names\n",
    "cluster_class_mapping = {\n",
    "    0: 'Backend',  # Example mapping, adjust based on your actual clusters\n",
    "    3: 'Frontend',\n",
    "    2: 'Security',\n",
    "    1: 'Documentation',\n",
    "    4: 'Performance'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Backend       0.69      0.15      0.25      1345\n",
      "Documentation       0.01      0.19      0.02        21\n",
      "     Frontend       0.63      0.48      0.54       987\n",
      "  Performance       0.01      0.17      0.01        18\n",
      "     Security       0.03      0.24      0.06        70\n",
      "\n",
      "     accuracy                           0.29      2441\n",
      "    macro avg       0.27      0.25      0.18      2441\n",
      " weighted avg       0.64      0.29      0.36      2441\n",
      "\n",
      "Confusion Matrix:\n",
      "[[203 266 248 329 299]\n",
      " [  3   4   5   5   4]\n",
      " [ 80 144 469 132 162]\n",
      " [  4   4   3   3   4]\n",
      " [  4   7  23  19  17]]\n",
      "Accuracy: 0.28512904547316675\n",
      "Precision: 0.28512904547316675\n",
      "Recall: 0.28512904547316675\n",
      "F1 Score: 0.28512904547316675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Map numerical indices to class names\n",
    "predicted_class_names = [cluster_class_mapping[label] for label in predicted_class]\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(preprocessed_test_df['class_name'], predicted_class_names))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(preprocessed_test_df['class_name'], predicted_class_names))\n",
    "\n",
    "# Print the accuracy\n",
    "accuracy = np.mean(preprocessed_test_df['class_name'] == predicted_class_names)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print the precision\n",
    "precision = np.mean([1 if actual == predicted else 0 for actual, predicted in zip(preprocessed_test_df['class_name'], predicted_class_names)])\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Print the recall\n",
    "recall = np.mean([1 if actual == predicted else 0 for actual, predicted in zip(preprocessed_test_df['class_name'], predicted_class_names)])\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Print the F1 score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
