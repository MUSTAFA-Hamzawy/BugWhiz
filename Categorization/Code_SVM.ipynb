{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me6NI_YUQg6D",
        "outputId": "c7377946-f047-47c1-b22b-4063d56930f7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import csv\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from collections import defaultdict\n",
        "\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "za89WztBQg6E"
      },
      "outputs": [],
      "source": [
        "def convert_lower_case(data):\n",
        "    \"\"\"\n",
        "        Converts all characters in the input text data to lowercase.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : str\n",
        "            The input text data to be converted to lowercase.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            The text data with all characters in lowercase.\n",
        "    \"\"\"\n",
        "\n",
        "    return str(data).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VpUXDgXmQg6F"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(data):\n",
        "    \"\"\"\n",
        "        Removes punctuation from the input text data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : str\n",
        "            The input text data from which punctuation will be removed.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            The text data with punctuation replaced by spaces.\n",
        "    \"\"\"\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in symbols:\n",
        "        data = np.char.replace(data, i, ' ')\n",
        "\n",
        "    return str(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "XMER2MBSQg6F"
      },
      "outputs": [],
      "source": [
        "def remove_apostrophe(data):\n",
        "    \"\"\"\n",
        "        Removes apostrophes from the input text data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : str\n",
        "            The input text data from which apostrophes will be removed.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            The text data with apostrophes removed.\n",
        "\n",
        "    \"\"\"\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "YABKQ2WSQg6F"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_numbers(data):\n",
        "    \"\"\"\n",
        "        Removes all numerical digits from the input text data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : str\n",
        "            The input text data from which numbers will be removed.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            The text data with all numerical digits removed.\n",
        "    \"\"\"\n",
        "    return re.sub(r'\\d+', '', str(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "p7H_7YoQQg6F"
      },
      "outputs": [],
      "source": [
        "def remove_single_characters(tokens):\n",
        "    \"\"\"\n",
        "        Removes single-character tokens from a list of tokens.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        tokens : list of str\n",
        "            A list of tokens (words) from which single-character tokens will be removed.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            A string with single-character tokens removed.\n",
        "    \"\"\"\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        if len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "CP8xMpcYQg6F"
      },
      "outputs": [],
      "source": [
        "def lemmatization(data):\n",
        "    \"\"\"\n",
        "        Performs lemmatization on the input text data, reducing words to their base or root form.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : str\n",
        "            The input text data to be lemmatized.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            The lemmatized text data.\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(data)\n",
        "    data = remove_single_characters(tokens)\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "    return lemmatized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "thx4H0iZQg6F"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    \"\"\"\n",
        "        Preprocesses the input text data by applying a series of transformations:\n",
        "        converting to lowercase, removing punctuation, removing apostrophes,\n",
        "        removing numbers, and lemmatizing.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : str\n",
        "            The input text data to be preprocessed.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            The preprocessed text data.\n",
        "    \"\"\"\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_numbers(data)\n",
        "    data = lemmatization(data)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "TZgpJN9_Qg6G"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(data):\n",
        "    \"\"\"\n",
        "        Removes common stop words from the input text data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : str\n",
        "            The input text data from which stop words will be removed.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            The text data with stop words removed.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(data)\n",
        "    data = ' '.join([i for i in tokens if not i in stop_words])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYeLeVodQg6F",
        "outputId": "5dbe4041-7ffe-4def-d606-8c7e08519160"
      },
      "outputs": [],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_train_df = pd.read_csv('train.csv')\n",
        "preprocessed_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())\n",
        "print(preprocessed_test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InUxEgprQg6G",
        "outputId": "7234588c-1250-4d07-f2c7-35400d0d720c"
      },
      "outputs": [],
      "source": [
        "# Convert non-string values to strings in 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: str(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: str(x))\n",
        "\n",
        "# Remove stop words from 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "\n",
        "print( preprocessed_test_df['bug_description'][0] )\n",
        "print( preprocessed_train_df['bug_description'][0] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSTdLFuvPnj0",
        "outputId": "0b51878c-dda1-4c3f-a278-509d62aefe60"
      },
      "outputs": [],
      "source": [
        "# keep only the reports that has class_name of Frontend, Backend, Security, Documentation\n",
        "# Filter the training data\n",
        "filtered_train_df = preprocessed_train_df[\n",
        "    (preprocessed_train_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Security') |\n",
        "    (preprocessed_train_df['class_name'] == 'Documentation')\n",
        "]\n",
        "\n",
        "# Filter the testing data\n",
        "filtered_test_df = preprocessed_test_df[\n",
        "    (preprocessed_test_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Security') |\n",
        "    (preprocessed_test_df['class_name'] == 'Documentation')\n",
        "]\n",
        "\n",
        "# Show the first 5 rows of the filtered training data\n",
        "print(\"Filtered Training Data:\")\n",
        "print(filtered_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the filtered testing data\n",
        "print(\"\\nFiltered Testing Data:\")\n",
        "print(filtered_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu0kjTg9Qg6G",
        "outputId": "6adeac3f-61d1-413a-a8d0-2a12da029b48"
      },
      "outputs": [],
      "source": [
        "# Define the mapping of class names to the desired order\n",
        "class_name_mapping = {\n",
        "    'Frontend': 0,\n",
        "    'Backend': 1,\n",
        "    'Security': 2,\n",
        "    'Documentation' : 3,\n",
        "}\n",
        "\n",
        "# Map class names in both training and testing data to the desired order\n",
        "filtered_train_df['class_label'] = filtered_train_df['class_name'].map(class_name_mapping)\n",
        "filtered_test_df['class_label'] = filtered_test_df['class_name'].map(class_name_mapping)\n",
        "\n",
        "# order them based on the number of class_label\n",
        "filtered_train_df = filtered_train_df.sort_values(by=['class_label'])\n",
        "filtered_test_df = filtered_test_df.sort_values(by=['class_label'])\n",
        "\n",
        "# Print the unique class names in the training data\n",
        "print(filtered_train_df['class_name'].unique())\n",
        "\n",
        "# print(filtered_train_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiu4-Rl8bbyE",
        "outputId": "907f3637-2877-4b6e-e28e-e8893297bfec"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Specify the path to your preprocessed CSV file\n",
        "input_file = 'train.csv'\n",
        "\n",
        "# Dictionary to store counts of each category\n",
        "category_counts = defaultdict(int)\n",
        "\n",
        "# Read the CSV file and count occurrences of each category\n",
        "with open(input_file, 'r', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    for row in csv_reader:\n",
        "        if len(row) == 2:  # Ensure the row has both report and category\n",
        "            _, category = row\n",
        "            category_counts[category] += 1\n",
        "\n",
        "# Print the counts of each category\n",
        "for category, count in category_counts.items():\n",
        "    print(f\"Category: {category}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfEqyVOaQg6G"
      },
      "source": [
        "## Feature Exraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "0jvI21lEgZAd"
      },
      "outputs": [],
      "source": [
        "def try_ngram_combinations(data, ngram_range):\n",
        "    \"\"\"\n",
        "        Generates TF-IDF weighted n-gram combinations from the input text data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        data : list of str\n",
        "            A list of textual data (documents) to be transformed into n-gram combinations.\n",
        "\n",
        "        ngram_range : tuple (min_n, max_n)\n",
        "            The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        X_transformed : sparse matrix of shape (n_samples, n_features)\n",
        "            The transformed data as a TF-IDF weighted term-document matrix.\n",
        "\n",
        "        vectorizer : TfidfVectorizer\n",
        "            The TfidfVectorizer instance that was used to perform the transformation, which includes\n",
        "            the fitted vocabulary and IDF values.\n",
        "        \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Initialize the TfidfVectorizer\n",
        "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
        "\n",
        "    # Fit and transform the data\n",
        "    X_transformed = vectorizer.fit_transform(data)\n",
        "\n",
        "\n",
        "    return X_transformed, vectorizer\n",
        "\n",
        "\n",
        "data1, vectorizer1 = try_ngram_combinations(filtered_train_df['bug_description'], (1, 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmxrjc63SMms"
      },
      "source": [
        "## ***SVM from scratch***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "MUTYooVtPulA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance  # to compute the Gaussian kernel\n",
        "import cvxopt                       # to solve the dual optimization problem\n",
        "import copy\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class SVM:\n",
        "    linear = lambda x, x_dash , c=0: x @ x_dash .T\n",
        "    polynomial = lambda x, x_dash , Q=5: (1 + x @ x_dash.T)**Q\n",
        "    rbf = lambda x, x_dash , gamma=10: np.exp(-gamma * distance.cdist(x, x_dash,'sqeuclidean'))\n",
        "    kernel_functions = {'linear': linear, 'polynomial': polynomial, 'rbf': rbf}\n",
        "\n",
        "    def __init__(self, kernel='linear', C=1, k=2):\n",
        "        # setting the hyperparameters\n",
        "        self.kernel_str = kernel\n",
        "        self.kernel = SVM.kernel_functions[kernel]\n",
        "        self.C = C                  # regularization parameter\n",
        "        self.k = k                  # kernel hyperparameter\n",
        "\n",
        "        # training data and support vectors\n",
        "        self.X, y = None, None\n",
        "        self.alpha = None\n",
        "        self.multiclass = False\n",
        "        self.classifiers = []\n",
        "\n",
        "    def fit(self, X, y, eval_train=False):\n",
        "      if len(np.unique(y)) > 2:\n",
        "          self.multiclass = True\n",
        "          return self.multi_fit(X, y, eval_train)\n",
        "\n",
        "      # relabel if needed\n",
        "      if set(np.unique(y)) == {0, 1}: y[y == 0] = -1\n",
        "\n",
        "\n",
        "      # ensure y has dimensions Nx1\n",
        "      self.y = y.reshape(-1, 1).astype(np.double) # Has to be a column vector\n",
        "\n",
        "      self.X = X\n",
        "      N = X.shape[0]\n",
        "\n",
        "      # compute the kernel over all possible pairs of (x, x') in the data\n",
        "      self.K = self.kernel(X, X, self.k)\n",
        "\n",
        "      # For 1/2 x^T P x + q^T x\n",
        "      P = cvxopt.matrix(self.y @ self.y.T * self.K)\n",
        "      q = cvxopt.matrix(-np.ones((N, 1)))\n",
        "\n",
        "      # For Ax = b\n",
        "      A = cvxopt.matrix(self.y.T)\n",
        "      b = cvxopt.matrix(np.zeros(1))\n",
        "\n",
        "      # For Gx <= h\n",
        "      G = cvxopt.matrix(np.vstack((-np.identity(N), np.identity(N))))\n",
        "      h = cvxopt.matrix(np.vstack((np.zeros((N,1)), np.ones((N,1)) * self.C)))\n",
        "\n",
        "      # Solve\n",
        "      cvxopt.solvers.options['show_progress'] = False\n",
        "      sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
        "      self.alpha = np.array(sol[\"x\"])\n",
        "\n",
        "      # Maps into support vectors\n",
        "      self.isSupportVector = ((self.alpha > 1e-3) & (self.alpha <= self.C)).squeeze()\n",
        "      self.marginSupportVector = np.argmax((1e-3 < self.alpha) & (self.alpha < self.C - 1e-3))\n",
        "\n",
        "    def multi_fit(self, X, y, eval_train=False):\n",
        "        self.k = len(np.unique(y))      # number of classes\n",
        "        y = np.array(y)\n",
        "        # for each pair of classes\n",
        "        for i in range(self.k):\n",
        "            # get the data for the pair\n",
        "            Xs, Ys = X, copy.copy(y)\n",
        "\n",
        "            # change the labels to -1 and 1\n",
        "            Ys[Ys!=i], Ys[Ys==i] = -1, +1\n",
        "\n",
        "            # fit the classifier\n",
        "            classifier = SVM(kernel=self.kernel_str, C=self.C, k=self.k)\n",
        "            classifier.fit(Xs, Ys)\n",
        "\n",
        "            # save the classifier\n",
        "            self.classifiers.append(classifier)\n",
        "\n",
        "\n",
        "    def predict(self, X_t):\n",
        "        if self.multiclass: return self.multi_predict(X_t)\n",
        "        x_s, y_s = self.X[self.marginSupportVector, np.newaxis], self.y[self.marginSupportVector]\n",
        "        alpha, y, X= self.alpha[self.isSupportVector], self.y[self.isSupportVector], self.X[self.isSupportVector]\n",
        "\n",
        "        b = y_s - np.sum(alpha * y * self.kernel(X, x_s, self.k), axis=0)\n",
        "        score = np.sum(alpha * y * self.kernel(X, X_t, self.k), axis=0) + b\n",
        "        return np.sign(score).astype(int), score\n",
        "\n",
        "    def multi_predict(self, X):\n",
        "        # get the predictions from all classifiers\n",
        "        preds = np.zeros((X.shape[0], self.k))\n",
        "        for i, classifier in enumerate(self.classifiers):\n",
        "            _, preds[:, i] = classifier.predict(X)\n",
        "\n",
        "        # get the argmax and the corresponding score\n",
        "        return np.argmax(preds, axis=1)\n",
        "\n",
        "\n",
        "# Initialize the SVM model\n",
        "model = SVM(C = 100)\n",
        "\n",
        "\n",
        "# Fit the model on the entire training data\n",
        "model.fit(data1, filtered_train_df['class_label'])\n",
        "\n",
        "# Predict the class labels for the testing data\n",
        "X_test_transformed = vectorizer1.transform(filtered_test_df['bug_description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2_yrRajaTAQ",
        "outputId": "7f52d8dd-74a5-4285-cb33-91aee8eb6b56"
      },
      "outputs": [],
      "source": [
        "# Predict the class labels for the test data\n",
        "predictions = model.predict(X_test_transformed)\n",
        "\n",
        "# Get the true labels for the test data\n",
        "true_labels = filtered_test_df['class_label']\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "precision = precision_score(true_labels, predictions, average='weighted')\n",
        "recall = recall_score(true_labels, predictions, average='weighted')\n",
        "f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIJlvjftg4JI",
        "outputId": "9c44541b-66f0-4d58-a8ba-e0cf3d2b3bc9"
      },
      "outputs": [],
      "source": [
        "# Save the model and the vectorizer\n",
        "joblib.dump(model, 'svm_model.pkl')\n",
        "joblib.dump(vectorizer1, 'vectorizer.pkl')\n",
        "print(\"Model and vectorizer saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iScN82MdhAgY",
        "outputId": "32232808-0dd5-4350-b34e-4ae6ef06b416"
      },
      "outputs": [],
      "source": [
        "# Load the model and the vectorizer\n",
        "loaded_model = joblib.load('svm_model.pkl')\n",
        "loaded_vectorizer = joblib.load('vectorizer.pkl')\n",
        "\n",
        "# Example new data for prediction\n",
        "new_data = [\n",
        "    \"We have some problems in api and it slows down the system.\",       # Backend\n",
        "    \"Manual guide of the installation is very bad.\",           # Documentation\n",
        "    \"customer wants to add button on the main page to show products\",   # Frontend\n",
        "    \"add warning when there is an error within the certificate\"         # Security\n",
        "    ]\n",
        "\n",
        "# Transform the new data using the loaded vectorizer\n",
        "new_data_transformed = loaded_vectorizer.transform(new_data)\n",
        "\n",
        "# Predict the class label for the new data\n",
        "new_pred = loaded_model.predict(new_data_transformed)\n",
        "\n",
        "# Print the prediction\n",
        "print(f\"Predicted class for the new input: {new_pred}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
