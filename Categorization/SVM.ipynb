{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me6NI_YUQg6D",
        "outputId": "7019675c-703e-43b6-9406-cddb647bcffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from itertools import combinations\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import joblib\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "za89WztBQg6E"
      },
      "outputs": [],
      "source": [
        "def convert_lower_case(data):\n",
        "    return str(data).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "VpUXDgXmQg6F"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in symbols:\n",
        "        data = np.char.replace(data, i, ' ')\n",
        "\n",
        "    return str(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "XMER2MBSQg6F"
      },
      "outputs": [],
      "source": [
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "YABKQ2WSQg6F"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_numbers(data):\n",
        "    return re.sub(r'\\d+', '', str(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "p7H_7YoQQg6F"
      },
      "outputs": [],
      "source": [
        "def remove_single_characters(tokens):\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        if len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "CP8xMpcYQg6F"
      },
      "outputs": [],
      "source": [
        "def lemmatization(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(data)\n",
        "    data = remove_single_characters(tokens)\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "    return lemmatized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "thx4H0iZQg6F"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_numbers(data)\n",
        "    data = lemmatization(data)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "TZgpJN9_Qg6G"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(data):\n",
        "    tokens = word_tokenize(data)\n",
        "    data = ' '.join([i for i in tokens if not i in stop_words])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYeLeVodQg6F",
        "outputId": "f9fcc455-5be5-4697-b402-49d8664d0bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  for any event on my bookmarked project option ...    Backend\n",
            "1               switch to using full ln id in urlbar   Frontend\n",
            "2  consider removing hasicon property to simplify...   Frontend\n",
            "3  method to obtain current url from webbrowsered...   Frontend\n",
            "4                fix migration fails in m sql server    Backend\n",
            "                                     bug_description class_name\n",
            "0  rest api ability to list sub project for a pro...    Backend\n",
            "1  support selective text on right if set in gnom...   Frontend\n",
            "2  meta userstory ship v of pre populated topsite...   Frontend\n",
            "3  include updated on and passwd changed on colum...    Backend\n",
            "4         problem with email integration to m office    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_train_df = pd.read_csv('train.csv')\n",
        "preprocessed_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())\n",
        "print(preprocessed_test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "input_file = 'test.txt'\n",
        "output_file = 'test_backend_preprocessed.csv'\n",
        "\n",
        "# Read the contents of the text file\n",
        "with open(input_file, 'r') as txt_file:\n",
        "    lines = txt_file.read().splitlines()\n",
        "\n",
        "# Preprocess each line and write to a CSV file\n",
        "with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    for line in lines:\n",
        "        preprocessed_data = preprocess(line)\n",
        "        csv_writer.writerow([preprocessed_data, 'Backend'])"
      ],
      "metadata": {
        "id": "J_-eGVGaZujz"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InUxEgprQg6G",
        "outputId": "83e1a6df-0db2-4b9e-c7f0-0e1914b24617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rest api ability list sub project project\n",
            "event bookmarked project option sending notification non member bookmarked project\n"
          ]
        }
      ],
      "source": [
        "# Convert non-string values to strings in 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: str(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: str(x))\n",
        "\n",
        "# Remove stop words from 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "\n",
        "print( preprocessed_test_df['bug_description'][0] )\n",
        "print( preprocessed_train_df['bug_description'][0] )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keep only the reports that has class_name of Frontend, Backend, Security, Documentation\n",
        "# Filter the training data\n",
        "filtered_train_df = preprocessed_train_df[\n",
        "    (preprocessed_train_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Security') |\n",
        "    (preprocessed_train_df['class_name'] == 'Documentation')\n",
        "]\n",
        "\n",
        "# Filter the testing data\n",
        "filtered_test_df = preprocessed_test_df[\n",
        "    (preprocessed_test_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Security') |\n",
        "    (preprocessed_test_df['class_name'] == 'Documentation')\n",
        "]\n",
        "\n",
        "# Show the first 5 rows of the filtered training data\n",
        "print(\"Filtered Training Data:\")\n",
        "print(filtered_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the filtered testing data\n",
        "print(\"\\nFiltered Testing Data:\")\n",
        "print(filtered_test_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSTdLFuvPnj0",
        "outputId": "b806132e-cb2a-4637-d27e-ee833abf0a74"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Training Data:\n",
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "\n",
            "Filtered Testing Data:\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu0kjTg9Qg6G",
        "outputId": "a762d8d5-26ea-4526-cc6a-9ddc8bb4851a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Frontend' 'Backend' 'Security' 'Documentation']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-808bf4f43219>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_train_df['class_label'] = filtered_train_df['class_name'].map(class_name_mapping)\n",
            "<ipython-input-68-808bf4f43219>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_df['class_label'] = filtered_test_df['class_name'].map(class_name_mapping)\n"
          ]
        }
      ],
      "source": [
        "# Define the mapping of class names to the desired order\n",
        "class_name_mapping = {\n",
        "    'Frontend': 0,\n",
        "    'Backend': 1,\n",
        "    'Security': 2,\n",
        "    'Documentation' : 3,\n",
        "}\n",
        "\n",
        "# Map class names in both training and testing data to the desired order\n",
        "filtered_train_df['class_label'] = filtered_train_df['class_name'].map(class_name_mapping)\n",
        "filtered_test_df['class_label'] = filtered_test_df['class_name'].map(class_name_mapping)\n",
        "\n",
        "# order them based on the number of class_label\n",
        "filtered_train_df = filtered_train_df.sort_values(by=['class_label'])\n",
        "filtered_test_df = filtered_test_df.sort_values(by=['class_label'])\n",
        "\n",
        "# Print the unique class names in the training data\n",
        "print(filtered_train_df['class_name'].unique())\n",
        "\n",
        "# print(filtered_train_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "# Specify the path to your preprocessed CSV file\n",
        "input_file = 'train.csv'\n",
        "\n",
        "# Dictionary to store counts of each category\n",
        "category_counts = defaultdict(int)\n",
        "\n",
        "# Read the CSV file and count occurrences of each category\n",
        "with open(input_file, 'r', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    for row in csv_reader:\n",
        "        if len(row) == 2:  # Ensure the row has both report and category\n",
        "            _, category = row\n",
        "            category_counts[category] += 1\n",
        "\n",
        "# Print the counts of each category\n",
        "for category, count in category_counts.items():\n",
        "    print(f\"Category: {category}, Count: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiu4-Rl8bbyE",
        "outputId": "11d93dff-cdf0-4175-963f-7a9e4125e4a2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category: class_name, Count: 1\n",
            "Category: Backend, Count: 8788\n",
            "Category: Frontend, Count: 6792\n",
            "Category: Security, Count: 3573\n",
            "Category: Documentation, Count: 3148\n",
            "Category: Performance, Count: 89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfEqyVOaQg6G"
      },
      "source": [
        "## Feature Exraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = 1000  # Number of features to select\n",
        "\n",
        "def try_ngram_combinations(data, ngram_range):\n",
        "    results = []\n",
        "\n",
        "    # Initialize the TfidfVectorizer\n",
        "    vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
        "\n",
        "    # Fit and transform the data\n",
        "    X_transformed = vectorizer.fit_transform(data)\n",
        "\n",
        "    # Apply TruncatedSVD to reduce to 1000 dimensions\n",
        "    # svd = TruncatedSVD(n_components=num_features)\n",
        "    # X_transformed = svd.fit_transform(X_transformed)\n",
        "\n",
        "\n",
        "    return X_transformed, vectorizer\n",
        "\n",
        "\n",
        "data1, vectorizer1 = try_ngram_combinations(filtered_train_df['bug_description'], (1, 2))\n"
      ],
      "metadata": {
        "id": "0jvI21lEgZAd"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygN6SwZ92LPn",
        "outputId": "27417132-a0f8-443e-d57c-d355d2f69f01"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22301, 61416)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM model\n",
        "model = SVC(C = 100)\n",
        "\n",
        "\n",
        "print(f\"Model trained using n-gram range: {vectorizer1.ngram_range}\")\n",
        "\n",
        "# Perform cross-validation to evaluate the model\n",
        "scores = cross_val_score(model, data1, filtered_train_df['class_name'], cv=5)\n",
        "print(f\"Cross-Validation Scores: {scores}\")\n",
        "\n",
        "# Fit the model on the entire training data\n",
        "model.fit(data1, filtered_train_df['class_name'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "K1fBI4MmderA",
        "outputId": "71de0394-737b-4e3f-93bb-5750fef94c47"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained using n-gram range: (1, 2)\n",
            "Cross-Validation Scores: [0.92154226 0.90717489 0.89484305 0.85403587 0.91928251]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=100)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the class labels for the testing data\n",
        "X_test_transformed = vectorizer1.transform(filtered_test_df['bug_description'])\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(filtered_test_df['class_name'], y_pred, target_names=filtered_test_df['class_name'].unique()))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(pd.crosstab(filtered_test_df['class_name'], y_pred, rownames=['Actual'], colnames=['Predicted']))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikveeDkSRlgS",
        "outputId": "e6bb72b5-43a1-4cf0-b821-69ca39da3c2d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Frontend       0.94      0.94      0.94      1794\n",
            "      Backend       1.00      0.95      0.97       449\n",
            "     Security       0.90      0.93      0.92      1244\n",
            "Documentation       0.98      0.96      0.97       704\n",
            "\n",
            "     accuracy                           0.94      4191\n",
            "    macro avg       0.96      0.95      0.95      4191\n",
            " weighted avg       0.94      0.94      0.94      4191\n",
            "\n",
            "Predicted      Backend  Documentation  Frontend  Security\n",
            "Actual                                                   \n",
            "Backend           1679              1       105         9\n",
            "Documentation       13            427         9         0\n",
            "Frontend            79              0      1162         3\n",
            "Security            11              0        15       678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the accuracy\n",
        "accuracy = model.score(X_test_transformed, filtered_test_df['class_name'])\n",
        "print(f\"Accuracy: {round(accuracy, 3) * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0v3OkAcTfvC",
        "outputId": "c418fcbf-7e48-4ef4-8bac-7baa672213ba"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 94.19999999999999%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and the vectorizer\n",
        "joblib.dump(model, 'svm_model.pkl')\n",
        "joblib.dump(vectorizer1, 'vectorizer.pkl')\n",
        "print(\"Model and vectorizer saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIJlvjftg4JI",
        "outputId": "9c44541b-66f0-4d58-a8ba-e0cf3d2b3bc9"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and vectorizer saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and the vectorizer\n",
        "loaded_model = joblib.load('svm_model.pkl')\n",
        "loaded_vectorizer = joblib.load('vectorizer.pkl')\n",
        "\n",
        "# Example new data for prediction\n",
        "new_data = [\n",
        "    \"We have some problems in api and it slows down the system.\",       # Backend\n",
        "    \"Manual guide of the installation is very bad.\",           # Documentation\n",
        "    \"customer wants to add button on the main page to show products\",   # Frontend\n",
        "    \"add warning when there is an error within the certificate\"         # Security\n",
        "    ]\n",
        "\n",
        "# Transform the new data using the loaded vectorizer\n",
        "new_data_transformed = loaded_vectorizer.transform(new_data)\n",
        "\n",
        "# Predict the class label for the new data\n",
        "new_pred = loaded_model.predict(new_data_transformed)\n",
        "\n",
        "# Print the prediction\n",
        "print(f\"Predicted class for the new input: {new_pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iScN82MdhAgY",
        "outputId": "32232808-0dd5-4350-b34e-4ae6ef06b416"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class for the new input: ['Backend' 'Documentation' 'Frontend' 'Security']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***SVM from scratch***"
      ],
      "metadata": {
        "id": "hmxrjc63SMms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np                  # for basic operations over arrays\n",
        "from scipy.spatial import distance  # to compute the Gaussian kernel\n",
        "import cvxopt                       # to solve the dual optimization problem\n",
        "import copy                         # to copy numpy arrays\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class SVM:\n",
        "    linear = lambda x, xࠤ , c=0: x @ xࠤ .T\n",
        "    polynomial = lambda x, xࠤ , Q=5: (1 + x @ xࠤ.T)**Q\n",
        "    rbf = lambda x, xࠤ , γ=10: np.exp(-γ * distance.cdist(x, xࠤ,'sqeuclidean'))\n",
        "    kernel_funs = {'linear': linear, 'polynomial': polynomial, 'rbf': rbf}\n",
        "\n",
        "    def __init__(self, kernel='rbf', C=1, k=2):\n",
        "        # set the hyperparameters\n",
        "        self.kernel_str = kernel\n",
        "        self.kernel = SVM.kernel_funs[kernel]\n",
        "        self.C = C                  # regularization parameter\n",
        "        self.k = k                  # kernel parameter\n",
        "\n",
        "        # training data and support vectors\n",
        "        self.X, y = None, None\n",
        "        self.αs = None\n",
        "\n",
        "        # for multi-class classification\n",
        "        self.multiclass = False\n",
        "        self.clfs = []\n",
        "\n",
        "    def fit(self, X, y, eval_train=False):\n",
        "      if len(np.unique(y)) > 2:\n",
        "          self.multiclass = True\n",
        "          return self.multi_fit(X, y, eval_train)\n",
        "\n",
        "      # relabel if needed\n",
        "      if set(np.unique(y)) == {0, 1}: y[y == 0] = -1\n",
        "\n",
        "\n",
        "      # ensure y has dimensions Nx1\n",
        "      self.y = y.reshape(-1, 1).astype(np.double) # Has to be a column vector\n",
        "\n",
        "\n",
        "\n",
        "      self.X = X\n",
        "      N = X.shape[0]\n",
        "\n",
        "      # compute the kernel over all possible pairs of (x, x') in the data\n",
        "      self.K = self.kernel(X, X, self.k)\n",
        "      # For 1/2 x^T P x + q^T x\n",
        "      P = cvxopt.matrix(self.y @ self.y.T * self.K)\n",
        "      q = cvxopt.matrix(-np.ones((N, 1)))\n",
        "      # For Ax = b\n",
        "      A = cvxopt.matrix(self.y.T)\n",
        "      b = cvxopt.matrix(np.zeros(1))\n",
        "      # For Gx <= h\n",
        "      G = cvxopt.matrix(np.vstack((-np.identity(N), np.identity(N))))\n",
        "      h = cvxopt.matrix(np.vstack((np.zeros((N,1)), np.ones((N,1)) * self.C)))\n",
        "\n",
        "      # Solve\n",
        "      cvxopt.solvers.options['show_progress'] = True\n",
        "      sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
        "      self.αs = np.array(sol[\"x\"])\n",
        "\n",
        "      # Maps into support vectors\n",
        "      self.is_sv = ((self.αs > 1e-3) & (self.αs <= self.C)).squeeze()\n",
        "      self.margin_sv = np.argmax((1e-3 < self.αs) & (self.αs < self.C - 1e-3))\n",
        "\n",
        "      if eval_train:\n",
        "        print(f\"Finished training with accuracy {self.evaluate(X, y)}\")\n",
        "\n",
        "    def multi_fit(self, X, y, eval_train=False):\n",
        "        self.k = len(np.unique(y))      # number of classes\n",
        "        y = np.array(y)\n",
        "        # for each pair of classes\n",
        "        for i in range(self.k):\n",
        "            # get the data for the pair\n",
        "            Xs, Ys = X, copy.copy(y)\n",
        "\n",
        "            # change the labels to -1 and 1\n",
        "            Ys[Ys!=i], Ys[Ys==i] = -1, +1\n",
        "            # fit the classifier\n",
        "            clf = SVM(kernel=self.kernel_str, C=self.C, k=self.k)\n",
        "            print('class : ,' , i)\n",
        "            clf.fit(Xs, Ys)\n",
        "            # save the classifier\n",
        "            self.clfs.append(clf)\n",
        "            print('Appended class : ' , i)\n",
        "        if eval_train:\n",
        "          print(f\"Finished training with accuracy {self.evaluate(X, y)}\")\n",
        "\n",
        "\n",
        "    def predict(self, X_t):\n",
        "        if self.multiclass: return self.multi_predict(X_t)\n",
        "        xₛ, yₛ = self.X[self.margin_sv, np.newaxis], self.y[self.margin_sv]\n",
        "        αs, y, X= self.αs[self.is_sv], self.y[self.is_sv], self.X[self.is_sv]\n",
        "\n",
        "        b = yₛ - np.sum(αs * y * self.kernel(X, xₛ, self.k), axis=0)\n",
        "        score = np.sum(αs * y * self.kernel(X, X_t, self.k), axis=0) + b\n",
        "        return np.sign(score).astype(int), score\n",
        "\n",
        "    def multi_predict(self, X):\n",
        "        # get the predictions from all classifiers\n",
        "        preds = np.zeros((X.shape[0], self.k))\n",
        "        for i, clf in enumerate(self.clfs):\n",
        "            _, preds[:, i] = clf.predict(X)\n",
        "\n",
        "        # get the argmax and the corresponding score\n",
        "        return np.argmax(preds, axis=1)\n",
        "\n",
        "    def evaluate(self, X,y):\n",
        "      outputs, _ = self.predict(X)\n",
        "      accuracy = np.sum(outputs == y) / len(y)\n",
        "      return round(accuracy, 2)\n",
        "\n",
        "\n",
        "# Initialize the SVM model\n",
        "model = SVM(C = 100)\n",
        "\n",
        "\n",
        "# Fit the model on the entire training data\n",
        "model.fit(data1, filtered_train_df['class_label'])\n",
        "\n",
        "# Predict the class labels for the testing data\n",
        "X_test_transformed = vectorizer1.transform(filtered_test_df['bug_description'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUTYooVtPulA",
        "outputId": "09551d7b-cb8b-4fe9-9664-73e24e23354d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class : , 0\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0:  3.6174e+06 -8.2146e+07  2e+08  6e-01  2e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply TruncatedSVD to reduce to 1000 dimensions\n",
        "svd = TruncatedSVD(n_components=num_features)\n",
        "X_test_transformed = svd.fit_transform(X_test_transformed)\n",
        "y_pred = model.predict(X_test_transformed)"
      ],
      "metadata": {
        "id": "7NuW2-adT6Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_test = np.array(filtered_test_df['class_label'])\n",
        "\n",
        "# Print the classification report\n",
        "# print(classification_report(filtered_test, y_pred, target_names=[0 ,1 ,2, 3]))\n",
        "\n",
        "# Print the confusion matrix\n",
        "# print(pd.crosstab(filtered_test_df['class_label'], y_pred, rownames=['Actual'], colnames=['Predicted']))\n",
        "\n",
        "# print(X_test_transformed.shape)\n",
        "# print(filtered_test_df['class_label'])\n",
        "\n",
        "# Print the accuracy\n",
        "# accuracy = model.score(X_test_transformed, filtered_test_df['class_label'])\n",
        "# print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(filtered_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "BIGSnFmr-vH3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}