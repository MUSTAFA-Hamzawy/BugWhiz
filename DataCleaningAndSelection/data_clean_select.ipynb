{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean up the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths\n",
    "directories = [\n",
    "    (\"..\\\\dataset\\\\eclipse\", \"..\\\\new_dataset\\\\eclipse\"),\n",
    "    #(\"..\\\\dataset\\\\eclipse_test\", \"..\\\\new_dataset\\\\eclipse_test\"),\n",
    "    #(\"..\\\\dataset\\\\firefox\", \"..\\\\new_dataset\\\\firefox\"),\n",
    "    #(\"..\\\\dataset\\\\netbeans\", \"..\\\\new_dataset\\\\netbeans\"),\n",
    "    #(\"..\\\\dataset\\\\openoffice\", \"..\\\\new_dataset\\\\openoffice\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse_small.csv\n",
      "eclipse_small_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each directory\n",
    "for source_dir, target_dir in directories:\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        print(file_name)\n",
    "        # Check if the file is a CSV file        \n",
    "        if file_name.endswith(\".csv\") and \"pairs\" not in file_name:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "            \n",
    "            # Check if the DataFrame is empty (end of file reached)\n",
    "            if df.empty:\n",
    "                print(\"End of file reached for:\", file_name)\n",
    "                continue\n",
    "            \n",
    "            # Create a new DataFrame with four columns\n",
    "            new_df = pd.DataFrame(columns=[\"bug_id\", \"bug_severity\", \"description\", \"priority\"])\n",
    "\n",
    "            # Populate the new DataFrame with data from the loaded DataFrame\n",
    "            new_df[\"bug_id\"] = df[\"bug_id\"]\n",
    "            # if the file is firefox, skip the bug_severity column\n",
    "            if \"firefox\" in source_dir:\n",
    "                new_df[\"bug_severity\"] = \"\"\n",
    "            else:\n",
    "                new_df[\"bug_severity\"] = df[\"bug_severity\"]\n",
    "            new_df[\"description\"] = df[\"description\"]\n",
    "            new_df[\"priority\"] = df[\"priority\"]\n",
    "\n",
    "            # Save the new DataFrame to a new CSV file in the target directory\n",
    "            new_file_path = os.path.join(target_dir, file_name.replace(\".csv\", \"_new.csv\"))\n",
    "            new_df.to_csv(new_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean up the data pairs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths\n",
    "directories = [\n",
    "    (\"..\\\\dataset\\\\eclipse\", \"..\\\\new_dataset\\\\eclipse\"),\n",
    "    (\"..\\\\dataset\\\\firefox\", \"..\\\\new_dataset\\\\firefox\"),\n",
    "    (\"..\\\\dataset\\\\netbeans\", \"..\\\\new_dataset\\\\netbeans\"),\n",
    "    (\"..\\\\dataset\\\\openoffice\", \"..\\\\new_dataset\\\\openoffice\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse.csv\n",
      "eclipse_pairs.csv\n",
      "eclipse_small.csv\n",
      "eclipse_small_pairs.csv\n",
      "firefox.csv\n",
      "firefox_pairs.csv\n",
      "netbeans-Copia.csv\n",
      "netbeans.csv\n",
      "netbeans_pairs-Copia.csv\n",
      "netbeans_pairs.csv\n",
      "openoffice.csv\n",
      "openoffice_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each directory\n",
    "for source_dir, target_dir in directories:\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        print(file_name)\n",
    "        # Check if the file is a CSV file and contains \"pairs\" in its name\n",
    "        if file_name.endswith(\".csv\") and \"pairs\" in file_name:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "            \n",
    "            # Check if the DataFrame is empty (end of file reached)\n",
    "            if df.empty:\n",
    "                print(\"End of file reached for:\", file_name)\n",
    "                continue\n",
    "            \n",
    "            # Filter out rows where the \"duplicate\" column is empty\n",
    "            df = df.dropna(subset=['duplicate'])\n",
    "            \n",
    "            # Create a new DataFrame with two columns\n",
    "            new_df = pd.DataFrame(columns=[\"issue_id\", \"duplicate\"])\n",
    "\n",
    "            # Populate the new DataFrame with data from the loaded DataFrame\n",
    "            new_df[\"issue_id\"] = df[\"issue_id\"]\n",
    "            new_df[\"duplicate\"] = df[\"duplicate\"]\n",
    "\n",
    "            # Save the new DataFrame to a new CSV file in the target directory\n",
    "            new_file_path = os.path.join(target_dir, file_name.replace(\".csv\", \"_new.csv\"))\n",
    "            new_df.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths\n",
    "directories = [\n",
    "    (\"..\\\\new_dataset\\\\eclipse\", \"..\\\\BOW_dataset\\\\eclipse\"),\n",
    "    (\"..\\\\new_dataset\\\\firefox\", \"..\\\\BOW_dataset\\\\firefox\"),\n",
    "    (\"..\\\\new_dataset\\\\netbeans\", \"..\\\\BOW_dataset\\\\netbeans\"),\n",
    "    (\"..\\\\new_dataset\\\\openoffice\", \"..\\\\BOW_dataset\\\\openoffice\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the index of a column in a CSV file\n",
    "def find_column_index(csv_file_path, column_name):\n",
    "    with open(csv_file_path, 'r') as file:\n",
    "        header = file.readline().strip().split(',')  # Read the first line (header) and split it by commas\n",
    "        try:\n",
    "            return header.index(column_name)  # Find the index of the column name\n",
    "        except ValueError:\n",
    "            return None  # Return None if the column name is not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs file loaded: eclipse_new.csv\n",
      "Original file loaded: eclipse_new_similarity.csv\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m priority_index \u001b[38;5;241m=\u001b[39m find_column_index(new_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpriority\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Find the bug severity and priority of issue_id and duplicate_id\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m issue_bug_severity \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbug_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43missue_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbug_severity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     40\u001b[0m duplicate_bug_severity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(new_data\u001b[38;5;241m.\u001b[39mloc[new_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbug_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m duplicate_id, new_data\u001b[38;5;241m.\u001b[39mcolumns[bug_severity_index]])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     42\u001b[0m issue_priority \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df\u001b[38;5;241m.\u001b[39mloc[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbug_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m issue_id, df\u001b[38;5;241m.\u001b[39mcolumns[priority_index]])[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Iterate over each directory\n",
    "for source_dir, target_dir in directories:\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        # Check if the file is a CSV file and not contains \"pairs\" in its name\n",
    "        if file_name.endswith(\".csv\") and \"pairs\" not in file_name:\n",
    "            # Load the CSV file\n",
    "            new_data = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "            print(\"Pairs file loaded:\", file_name)\n",
    "\n",
    "            # Get the corresponding new CSV file name\n",
    "            new_file_name = file_name.replace(\".csv\", \"_similarity.csv\")\n",
    "            new_file_path = os.path.join(target_dir, new_file_name)\n",
    "\n",
    "            # Check if the new file exists\n",
    "            if os.path.exists(new_file_path):\n",
    "                # Load the new CSV file\n",
    "                bug_reports_file = pd.read_csv(new_file_path)\n",
    "                print(\"Original file loaded:\", new_file_name)\n",
    "\n",
    "                # Check if the DataFrame is empty (end of file reached)\n",
    "                if new_data.empty:\n",
    "                    print(\"End of file reached for:\", file_name)\n",
    "                    continue\n",
    "\n",
    "                # Create an empty DataFrame to store results of bug_severity and priority similarity columns\n",
    "                results_df = pd.DataFrame(columns=[\"issue_id\", \"duplicate_id\", \"similarity\", \"bug_severity_similarity\", \"priority_similarity\"])\n",
    "\n",
    "                # Iterate through each pair of bug reports\n",
    "                for index, row in bug_reports_file.iterrows():\n",
    "                    issue_id = row['issue_id']\n",
    "                    duplicate_id = row['duplicate_id']\n",
    "\n",
    "                    # Find the index of bug_severity and priority columns in the source DataFrame\n",
    "                    bug_severity_index = find_column_index(new_file_path, 'bug_severity')\n",
    "                    priority_index = find_column_index(new_file_path, 'priority')\n",
    "\n",
    "                    # Find the bug severity and priority of issue_id and duplicate_id\n",
    "                    issue_bug_severity = np.array(new_data.loc[new_data['bug_id'] == issue_id, 'bug_severity'])[0]\n",
    "                    duplicate_bug_severity = np.array(new_data.loc[new_data['bug_id'] == duplicate_id, new_data.columns[bug_severity_index]])[0]\n",
    "\n",
    "                    issue_priority = np.array(df.loc[df['bug_id'] == issue_id, df.columns[priority_index]])[0]\n",
    "                    duplicate_priority = np.array(df.loc[df['bug_id'] == duplicate_id, df.columns[priority_index]])[0]\n",
    "\n",
    "                    # Calculate bug severity and priority similarity\n",
    "                    bug_severity_similarity = 1 if issue_bug_severity == duplicate_bug_severity else 0\n",
    "                    priority_similarity = 1 if issue_priority == duplicate_priority else 0\n",
    "                    \n",
    "                    # Append the results to the DataFrame\n",
    "                    results_df = results_df.append({\"issue_id\": issue_id, \"duplicate_id\": duplicate_id,\n",
    "                                                    \"similarity\": row['similarity'], \n",
    "                                                    \"bug_severity_similarity\": bug_severity_similarity, \n",
    "                                                    \"priority_similarity\": priority_similarity}, ignore_index=True)\n",
    "\n",
    "                # Save results DataFrame to the existing CSV file\n",
    "                results_df.to_csv(new_file_path, index=False)\n",
    "                print(\"Results saved to:\", new_file_name)\n",
    "            else:\n",
    "                print(\"New CSV file not found for:\", new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
