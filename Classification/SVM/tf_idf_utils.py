from typing import List, Dict, Tuple
from collections import Counter, defaultdict
from scipy.sparse import csr_matrix
import math

# Generate n-grams from a list of tokens
def generate_ngrams(tokens: List[str], n: int) -> List[str]:
    """
    Generate n-grams from a list of tokens.

    Parameters:
    tokens (List[str]): A list of tokens (words).
    n (int): The maximum number of grams.

    Returns:
    List[str]: A list of n-grams.
    """
    n_grams = []  # Initialize an empty list to store n-grams
    
    # Iterate over each n-gram size from 1 to n
    for i in range(1, n + 1):
        # Generate n-grams of size i from tokens
        for j in range(len(tokens) - i + 1):
            n_gram = ' '.join(tokens[j:j+i])  # Join tokens to form an n-gram
            n_grams.append(n_gram)  # Append the generated n-gram to the list
    
    return n_grams  # Return the list of generated n-grams

def custom_tokenize(text: str, ngram_range: Tuple[int, int] = (1, 2)) -> List[str]:
    """
    Tokenize text into words and generate n-grams.

    Parameters:
    text (str): The input text to tokenize.
    ngram_range (Tuple[int, int]): The range of n-grams to generate.

    Returns:
    List[str]: A list of n-grams.
    """
    # Tokenize the text
    tokens = text.split()  # Or use a more sophisticated tokenizer
    # Filter out single-character tokens
    tokens = [token for token in tokens if len(token) > 1]
    # Generate n-grams
    return generate_ngrams(tokens, ngram_range[1])

def compute_tf(doc: List[str]) -> Dict[str, float]:
    """
    Compute term frequency (TF) for a document.

    Parameters:
    doc (List[str]): A list of tokens (words) from a document.

    Returns:
    Dict[str, float]: A dictionary where keys are words and values are their term frequencies.
    """
    # Calculate Term Frequency (TF) for each word in the document
    tf_dict = Counter(doc)  # Count occurrences of each word in the document
    total_terms = len(doc)  # Total number of terms (words) in the document

    # Calculate TF as the ratio of occurrences of each word to the total number of terms
    tf_dict = {word: count / total_terms for word, count in tf_dict.items()}

    return tf_dict  # Return a dictionary with word as key and TF as value

def compute_idf(corpus: List[List[str]]) -> Dict[str, float]:
    """
    Compute inverse document frequency (IDF) for a corpus.

    Parameters:
    corpus (List[List[str]]): A list of documents, each document is a list of tokens (words).

    Returns:
    Dict[str, float]: A dictionary where keys are words and values are their inverse document frequencies.
    """
    idf_dict = defaultdict(int)  # Initialize a defaultdict to count document frequency
    """
     If you try to access a key that does not exist, 
     defaultdict automatically creates the key and assigns it a default value generated by the callable.
    """

    num_docs = len(corpus)  # Total number of documents in the corpus

    # Iterate through each document in the corpus
    for doc in corpus:
        # Convert the document into a set to get unique words
        for word in set(doc):
            idf_dict[word] += 1  # Increment the count for each word in which the document appears

    # Calculate IDF for each word based on its document frequency
    idf_dict = {word: math.log(num_docs / count) for word, count in idf_dict.items()}
    return idf_dict  # Return a dictionary where keys are words and values are IDF scores

    


def compute_tfidf(tf: Dict[str, float], idf: Dict[str, float], default_idf: float) -> Dict[str, float]:
    """
    Compute TF-IDF for a document.

    Parameters:
    tf (Dict[str, float]): A dictionary where keys are words and values are their term frequencies.
    idf (Dict[str, float]): A dictionary where keys are words and values are their inverse document frequencies.
    default_idf (float): The default IDF value for words not seen in the training corpus.

    Returns:
    Dict[str, float]: A dictionary where keys are words and values are their TF-IDF scores.
    """
    # terate through each (word, tf_val) pair in the tf dictionary 
    # tf_val: The TF value of the word from the tf dictionary.
    # idf.get(word, default_idf): Retrieves the IDF value for the word from the idf dictionary. 
    # If the word is not found in idf, it uses default_idf.
    tfidf = {word: tf_val * idf.get(word, default_idf) for word, tf_val in tf.items()}
    """
     If a word is not found in idf, it uses a default IDF value (default_idf).
    """
    return tfidf

def tfidf_to_sparse_matrix(tfidf_list: List[Dict[str, float]], vocab: Dict[str, int], default_idx: int) -> csr_matrix:
    """
    Convert a list of TF-IDF dictionaries to a sparse matrix.

    Parameters:
    tfidf_list (List[Dict[str, float]]): A list of dictionaries where keys are words and values are their TF-IDF scores.
    vocab (Dict[str, int]): A dictionary where keys are words and values are their indices in the matrix.
                            This helps in assigning columns in the matrix for each word.
    default_idx (int): The index for unseen words.

    Returns:
    csr_matrix: A sparse matrix of shape (number of documents, size of vocabulary + 1).
    """
    rows, cols, data = [], [], []
    # Iterate over each document's TF-IDF dictionary in tfidf_list
    for row, tfidf in enumerate(tfidf_list):
        # Iterate over each word and its TF-IDF score in the document
        for word, val in tfidf.items():
            # Get the column index for the word from vocab; use default_idx if word not found
            col = vocab.get(word, default_idx)
            # Append row index, column index, and TF-IDF score to respective lists
            rows.append(row)
            cols.append(col)
            data.append(val)
    
    # Construct a sparse CSR matrix from the collected data, rows, and columns
    return csr_matrix((data, (rows, cols)), shape=(len(tfidf_list), len(vocab) + 1))
    """
    The additional column (+1) in the sparse matrix is for handling unseen words. 
    If a word appears in a document but is not found in vocab (i.e., it's an unseen word), it will be assigned the index default_idx. 
    This index typically represents an out-of-vocabulary token or a placeholder for unseen words.
    """