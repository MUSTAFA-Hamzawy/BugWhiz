{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nimfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ-_-4bROUdX",
        "outputId": "a6d4b214-44d6-4eac-8006-d96d85d11e1a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nimfa\n",
            "  Downloading nimfa-1.4.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nimfa) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from nimfa) (1.11.4)\n",
            "Installing collected packages: nimfa\n",
            "Successfully installed nimfa-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GfR4Yo-rtJZ",
        "outputId": "ba32ed60-c8ee-465a-9420-a4a12537862f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukDBCJvXqwYc",
        "outputId": "83081b6b-e762-4a5b-afce-41108c3887b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# remove the stop words from the preprocessed data using nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0rcr0kUBqwYe"
      },
      "outputs": [],
      "source": [
        "def convert_lower_case(data):\n",
        "    return str(data).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DqVvCOicqwYe"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in symbols:\n",
        "        data = np.char.replace(data, i, ' ')\n",
        "\n",
        "    return str(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q4-p9BLIqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B2RoS_VXqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(data):\n",
        "    return re.sub(r'\\d+', '', str(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kicK-_AdqwYf"
      },
      "outputs": [],
      "source": [
        "def remove_single_characters(tokens):\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        if len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NFCFEuimqwYg"
      },
      "outputs": [],
      "source": [
        "def lemmatization(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(data)\n",
        "    data = remove_single_characters(tokens)\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "    return lemmatized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OGszj2RZqwYg"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_numbers(data)\n",
        "    data = lemmatization(data)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwwT_5_PqwYi",
        "outputId": "c269e430-8f46-457b-d3eb-27d24be68995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  for any event on my bookmarked project option ...    Backend\n",
            "1               switch to using full ln id in urlbar   Frontend\n",
            "2  consider removing hasicon property to simplify...   Frontend\n",
            "3  method to obtain current url from webbrowsered...   Frontend\n",
            "4                fix migration fails in m sql server    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_train_df = pd.read_csv('/content/preprocessed_train_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlvdLrF6qwYi",
        "outputId": "97499af5-7a25-43b5-fb0c-769f016fd4ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  rest api ability to list sub project for a pro...    Backend\n",
            "1  support selective text on right if set in gnom...   Frontend\n",
            "2  meta userstory ship v of pre populated topsite...   Frontend\n",
            "3  include updated on and passwd changed on colum...    Backend\n",
            "4         problem with email integration to m office    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_test_df = pd.read_csv('/content/preprocessed_test_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the stop words from the preprocessed data using nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoyHEvQRbQDl",
        "outputId": "0bbdccf1-58e0-4773-b12c-8e315a16044a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(data):\n",
        "    tokens = word_tokenize(data)\n",
        "    data = ' '.join([i for i in tokens if not i in stop_words])\n",
        "    return data\n",
        "\n",
        "# preprocess the first report of the training data\n",
        "print(preprocess(preprocessed_train_df['bug_description'][0]))\n",
        "\n",
        "# remove the stop words from the preprocessed data\n",
        "print(remove_stop_words(preprocess(preprocessed_train_df['bug_description'][0])))\n",
        "\n",
        "# preprocess the first report of the testing data\n",
        "print(preprocess(preprocessed_test_df['bug_description'][0]))\n",
        "\n",
        "# remove the stop words from the preprocessed data\n",
        "print(remove_stop_words(preprocess(preprocessed_test_df['bug_description'][0])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFUU3HvrsIOl",
        "outputId": "a4591fbb-14e5-43fc-98cb-756e63af18c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "event bookmarked project option sending notification non member bookmarked project\n",
            "event bookmarked project option sending notification non member bookmarked project\n",
            "rest api ability list sub project project\n",
            "rest api ability list sub project project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1XWkNBPqwYi",
        "outputId": "3fcfb993-8602-43e4-a153-d2a98d0ed6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ],
      "source": [
        "# Convert non-string values to strings in 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: str(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: str(x))\n",
        "\n",
        "# Remove stop words from 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "\n",
        "# Show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the preprocessed testing data\n",
        "print(preprocessed_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7avA5KxfqwYi",
        "outputId": "08f0ebfe-d8a3-44d0-fdc2-6370b48dd42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Training Data:\n",
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "\n",
            "Filtered Testing Data:\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ],
      "source": [
        "# keep only the reports that has class_name of Frontend, Backend, Security, Documentation\n",
        "# Filter the training data\n",
        "filtered_train_df = preprocessed_train_df[\n",
        "    (preprocessed_train_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Security') |\n",
        "    (preprocessed_train_df['class_name'] == 'Documentation')\n",
        "]\n",
        "\n",
        "# Filter the testing data\n",
        "filtered_test_df = preprocessed_test_df[\n",
        "    (preprocessed_test_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Security') |\n",
        "    (preprocessed_test_df['class_name'] == 'Documentation')\n",
        "]\n",
        "\n",
        "# Show the first 5 rows of the filtered training data\n",
        "print(\"Filtered Training Data:\")\n",
        "print(filtered_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the filtered testing data\n",
        "print(\"\\nFiltered Testing Data:\")\n",
        "print(filtered_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYJ8iDYgqwYi",
        "outputId": "9128a4c3-037a-4744-ece7-d4db3d2eda04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Backend' 'Frontend' 'Security' 'Documentation']\n",
            "['Backend' 'Frontend' 'Documentation' 'Security']\n"
          ]
        }
      ],
      "source": [
        "# print the unique class names in the training data\n",
        "print(filtered_train_df['class_name'].unique())\n",
        "\n",
        "# print the unique class names in the testing data\n",
        "print(filtered_test_df['class_name'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esX-WwIwqwYj"
      },
      "source": [
        "## Feature Exraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(filtered_train_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZB99m8gvAcs",
        "outputId": "2bcf80fe-4b40-405d-c6a9-046776692857"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6I9uy_heqwYk"
      },
      "outputs": [],
      "source": [
        "# Define the mapping of class names to the desired order\n",
        "class_name_mapping = {\n",
        "    'Backend': 1,\n",
        "    'Frontend': 0,\n",
        "    'Security': 2,\n",
        "    'Documentation': 3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext.vocab as vocab\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove = vocab.GloVe(name='6B', dim=300)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGOLFs0fbwAh",
        "outputId": "cc19954a-7ea7-4baa-abf4-c48c6b49e2ec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [01:05<00:00, 6118.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize bug reports and map tokens to GloVe embeddings\n",
        "def tokenize_and_map_to_glove(text):\n",
        "    tokens = text.split()\n",
        "    embeddings = [glove[token.lower()] for token in tokens if token.lower() in glove.stoi]\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "5zJKEeYdbxUF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "tokenized_bug_reports_train = [tokenize_and_map_to_glove(text) for text in filtered_train_df['bug_description']]\n",
        "tokenized_bug_reports_test = [tokenize_and_map_to_glove(text) for text in filtered_test_df['bug_description']]"
      ],
      "metadata": {
        "id": "B8SrjU9Gbz6Q"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate token embeddings (e.g., by averaging)\n",
        "def aggregate_embeddings(embeddings):\n",
        "    if embeddings:\n",
        "        return torch.stack(embeddings).mean(dim=0)\n",
        "    else:\n",
        "        # Return a zero vector if no embeddings are found\n",
        "        return torch.zeros(glove.vectors.shape[1])"
      ],
      "metadata": {
        "id": "DyRbm1cxb0tu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example usage:\n",
        "X_train = torch.stack([aggregate_embeddings(embeddings) for embeddings in tokenized_bug_reports_train])\n",
        "X_test = torch.stack([aggregate_embeddings(embeddings) for embeddings in tokenized_bug_reports_test])\n",
        "\n",
        "# Now you can use X_train and X_test as features for your classification model\n"
      ],
      "metadata": {
        "id": "96JuaST_g87p"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Save X_train and X_test to files\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('X_test.npy', X_test)\n"
      ],
      "metadata": {
        "id": "zgt0qs7bNP9J"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load X_train and X_test from files\n",
        "X_train = np.load('X_train.npy')\n",
        "X_test = np.load('X_test.npy')\n"
      ],
      "metadata": {
        "id": "1y5wBKOPOsDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, filtered_train_df['class_name'])\n",
        "\n",
        "# Predict class labels for the test data\n",
        "predicted_labels = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(filtered_test_df['class_name'], predicted_labels))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(filtered_test_df['class_name'], predicted_labels))\n"
      ],
      "metadata": {
        "id": "WTa3eCnuoZMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a23d94-b9dd-4910-cd09-797e90891911"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Backend       0.81      0.84      0.83      1345\n",
            "Documentation       0.56      0.24      0.33        21\n",
            "     Frontend       0.76      0.77      0.77       987\n",
            "     Security       0.60      0.26      0.36        70\n",
            "\n",
            "     accuracy                           0.79      2423\n",
            "    macro avg       0.68      0.53      0.57      2423\n",
            " weighted avg       0.78      0.79      0.78      2423\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1129    1  211    4]\n",
            " [  12    5    4    0]\n",
            " [ 216    3  760    8]\n",
            " [  33    0   19   18]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVPHOv10dZ0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}