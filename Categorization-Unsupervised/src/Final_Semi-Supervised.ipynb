{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VERwkH_G5hSy",
        "outputId": "952925f0-286a-47d7-c481-32e042eb9cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "from itertools import combinations\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GDRG-q3r5hS1"
      },
      "outputs": [],
      "source": [
        "def convert_lower_case(data):\n",
        "    return str(data).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jKHt_J7L5hS1"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in symbols:\n",
        "        data = np.char.replace(data, i, ' ')\n",
        "\n",
        "    return str(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xqpSUdua5hS1"
      },
      "outputs": [],
      "source": [
        "def remove_apostrophe(data):\n",
        "    return np.char.replace(data, \"'\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tw4pWRhy5hS1"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(data):\n",
        "    return re.sub(r'\\d+', '', str(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q3CBbYV-5hS1"
      },
      "outputs": [],
      "source": [
        "def remove_single_characters(tokens):\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        if len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ESKGJhRm5hS1"
      },
      "outputs": [],
      "source": [
        "def lemmatization(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(data)\n",
        "    data = remove_single_characters(tokens)\n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "    return lemmatized_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jqAOoaqX5hS2"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)\n",
        "    data = remove_punctuation(data)\n",
        "    data = remove_apostrophe(data)\n",
        "    data = remove_numbers(data)\n",
        "    data = lemmatization(data)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "zQIXTTSr5hS2",
        "outputId": "ba22664b-2af1-4ff8-955d-264c0a79151a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bad0d6fc1ebe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print the first report of the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ],
      "source": [
        "# print the first report of the training data\n",
        "print(train_df['report'][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_M_X52E5hS2"
      },
      "outputs": [],
      "source": [
        "# preprocess the first report of the training data\n",
        "print(preprocess(train_df['report'][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws9w0a3i5hS2",
        "outputId": "ca1e90b6-0ace-4f57-c9fc-ffcde2d32208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  for any event on my bookmarked project option ...    Backend\n",
            "1               switch to using full ln id in urlbar   Frontend\n",
            "2  consider removing hasicon property to simplify...   Frontend\n",
            "3  method to obtain current url from webbrowsered...   Frontend\n",
            "4                fix migration fails in m sql server    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_train_df = pd.read_csv('preprocessed_train_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY4RWdqH5hS3",
        "outputId": "ebb45fc8-1bb4-4af8-afe2-6bd2699f65e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     bug_description class_name\n",
            "0  rest api ability to list sub project for a pro...    Backend\n",
            "1  support selective text on right if set in gnom...   Frontend\n",
            "2  meta userstory ship v of pre populated topsite...   Frontend\n",
            "3  include updated on and passwd changed on colum...    Backend\n",
            "4         problem with email integration to m office    Backend\n"
          ]
        }
      ],
      "source": [
        "# read the preprocessed data from the new file\n",
        "preprocessed_test_df = pd.read_csv('preprocessed_test_data2.csv')\n",
        "\n",
        "# show the first 5 rows of the preprocessed training data\n",
        "print(preprocessed_test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "Q3O56IKU5hS3",
        "outputId": "425e7d71-9d29-4418-aaa3-64ff40c46d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for any event on my bookmarked project option not sending notification for non member bookmarked project\n",
            "event bookmarked project option sending notification non member bookmarked project\n",
            "rest api ability to list sub project for a project\n",
            "rest api ability list sub project project\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rest api ability to list sub project for a project'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "def remove_stop_words(data):\n",
        "    tokens = word_tokenize(data)\n",
        "    data = ' '.join([i for i in tokens if not i in stop_words])\n",
        "    return data\n",
        "\n",
        "# preprocess the first report of the training data\n",
        "print(preprocess(preprocessed_train_df['bug_description'][0]))\n",
        "\n",
        "# remove the stop words from the preprocessed data\n",
        "print(remove_stop_words(preprocess(preprocessed_train_df['bug_description'][0])))\n",
        "\n",
        "# preprocess the first report of the testing data\n",
        "print(preprocess(preprocessed_test_df['bug_description'][0]))\n",
        "\n",
        "# remove the stop words from the preprocessed data\n",
        "print(remove_stop_words(preprocess(preprocessed_test_df['bug_description'][0])))\n",
        "\n",
        "preprocessed_test_df['bug_description'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_O5GnzG5hS3",
        "outputId": "c43372c8-56e0-4c04-b45f-69fd072190aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rest api ability list sub project project\n",
            "event bookmarked project option sending notification non member bookmarked project\n"
          ]
        }
      ],
      "source": [
        "# Convert non-string values to strings in 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: str(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: str(x))\n",
        "\n",
        "# Remove stop words from 'bug_description' column\n",
        "preprocessed_train_df['bug_description'] = preprocessed_train_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "preprocessed_test_df['bug_description'] = preprocessed_test_df['bug_description'].apply(lambda x: remove_stop_words(x))\n",
        "\n",
        "print( preprocessed_test_df['bug_description'][0] )\n",
        "print( preprocessed_train_df['bug_description'][0] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_hzWk6x5hS3",
        "outputId": "4fdc846a-11ec-4ae9-bac7-bf65044d8fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Training Data:\n",
            "                                     bug_description class_name\n",
            "0  event bookmarked project option sending notifi...    Backend\n",
            "1                     switch using full ln id urlbar   Frontend\n",
            "2  consider removing hasicon property simplify st...   Frontend\n",
            "3         method obtain current url webbrowsereditor   Frontend\n",
            "4                     fix migration fails sql server    Backend\n",
            "\n",
            "Filtered Testing Data:\n",
            "                                     bug_description class_name\n",
            "0          rest api ability list sub project project    Backend\n",
            "1     support selective text right set gnome setting   Frontend\n",
            "2  meta userstory ship v pre populated topsites a...   Frontend\n",
            "3  include updated passwd changed column user api...    Backend\n",
            "4                   problem email integration office    Backend\n"
          ]
        }
      ],
      "source": [
        "# keep only the reports that has class_name of Frontend, Backend, Security, Documentation\n",
        "# Filter the training data\n",
        "filtered_train_df = preprocessed_train_df[\n",
        "    (preprocessed_train_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_train_df['class_name'] == 'Security')\n",
        "]\n",
        "\n",
        "# Filter the testing data\n",
        "filtered_test_df = preprocessed_test_df[\n",
        "    (preprocessed_test_df['class_name'] == 'Frontend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Backend') |\n",
        "    (preprocessed_test_df['class_name'] == 'Security')\n",
        "]\n",
        "\n",
        "# Show the first 5 rows of the filtered training data\n",
        "print(\"Filtered Training Data:\")\n",
        "print(filtered_train_df.head())\n",
        "\n",
        "# Show the first 5 rows of the filtered testing data\n",
        "print(\"\\nFiltered Testing Data:\")\n",
        "print(filtered_test_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoBDHCGn5hS4",
        "outputId": "efd05043-f60c-4e96-cc45-d19e91fde894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Frontend' 'Backend' 'Security']\n",
            "['Frontend' 'Backend' 'Security']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-3d5cdb24e424>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_train_df['class_label'] = filtered_train_df['class_name'].map(class_name_mapping)\n",
            "<ipython-input-37-3d5cdb24e424>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_test_df['class_label'] = filtered_test_df['class_name'].map(class_name_mapping)\n"
          ]
        }
      ],
      "source": [
        "# Define the mapping of class names to the desired order\n",
        "class_name_mapping = {\n",
        "    'Backend': 1,\n",
        "    'Frontend': 0,\n",
        "    'Security': 2\n",
        "}\n",
        "\n",
        "# Map class names in both training and testing data to the desired order\n",
        "filtered_train_df['class_label'] = filtered_train_df['class_name'].map(class_name_mapping)\n",
        "filtered_test_df['class_label'] = filtered_test_df['class_name'].map(class_name_mapping)\n",
        "\n",
        "# order them based on the number of class_label\n",
        "filtered_train_df = filtered_train_df.sort_values(by=['class_label'])\n",
        "filtered_test_df = filtered_test_df.sort_values(by=['class_label'])\n",
        "\n",
        "# Print the unique class names in the training data\n",
        "print(filtered_train_df['class_name'].unique())\n",
        "\n",
        "# Print the unique class names in the testing data\n",
        "print(filtered_test_df['class_name'].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZCtSFgw5hS4"
      },
      "source": [
        "## Feature Exraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58bC7Byg5hS5",
        "outputId": "5ac541d4-9b44-4b1e-dcf8-bd86651a8f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing n-gram range: (1, 1)\n",
            "Processing n-gram range: (1, 2)\n",
            "Processing n-gram range: (1, 3)\n",
            "Processing n-gram range: (1, 4)\n",
            "Processing n-gram range: (1, 5)\n",
            "Processing n-gram range: (1, 6)\n",
            "Processing n-gram range: (1, 7)\n",
            "Processing n-gram range: (1, 8)\n",
            "Processing n-gram range: (1, 9)\n",
            "Processing n-gram range: (1, 10)\n",
            "Processing n-gram range: (1, 11)\n",
            "Processing n-gram range: (1, 12)\n",
            "Processing n-gram range: (1, 13)\n",
            "Processing n-gram range: (1, 14)\n",
            "Shape of transformed data for n-gram range (1, 1): (1050, 2119)\n",
            "Shape of transformed data for n-gram range (1, 2): (1050, 7725)\n",
            "Shape of transformed data for n-gram range (1, 3): (1050, 12830)\n",
            "Shape of transformed data for n-gram range (1, 4): (1050, 16999)\n",
            "Shape of transformed data for n-gram range (1, 5): (1050, 20222)\n",
            "Shape of transformed data for n-gram range (1, 6): (1050, 22575)\n",
            "Shape of transformed data for n-gram range (1, 7): (1050, 24202)\n",
            "Shape of transformed data for n-gram range (1, 8): (1050, 25273)\n",
            "Shape of transformed data for n-gram range (1, 9): (1050, 25949)\n",
            "Shape of transformed data for n-gram range (1, 10): (1050, 26362)\n",
            "Shape of transformed data for n-gram range (1, 11): (1050, 26612)\n",
            "Shape of transformed data for n-gram range (1, 12): (1050, 26767)\n",
            "Shape of transformed data for n-gram range (1, 13): (1050, 26864)\n",
            "Shape of transformed data for n-gram range (1, 14): (1050, 26924)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def try_ngram_combinations(data, ngram_ranges, sample_size=200):\n",
        "    \"\"\"\n",
        "    Try different combinations of n-grams using TfidfVectorizer.\n",
        "\n",
        "    Args:\n",
        "    - data: The input data to be transformed.\n",
        "    - ngram_ranges: A list of tuples representing the n-gram ranges to try.\n",
        "    - sample_size: The number of samples to take from each class.\n",
        "\n",
        "    Returns:\n",
        "    - A list of tuples containing the transformed data and corresponding vectorizer instances for each n-gram combination.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Initialize an empty DataFrame to store the sampled data\n",
        "    sampled_data = pd.DataFrame(columns=data.columns)\n",
        "\n",
        "    # Sample the specified number of samples from each class\n",
        "    for class_name in data['class_name'].unique():\n",
        "        class_samples = data[data['class_name'] == class_name].sample(n=sample_size, random_state=42)\n",
        "        sampled_data = pd.concat([sampled_data, class_samples])\n",
        "\n",
        "    for ngrams in ngram_ranges:\n",
        "        print(f\"Processing n-gram range: {ngrams}\")\n",
        "\n",
        "        # Initialize the TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(ngram_range=ngrams)\n",
        "\n",
        "        # Fit and transform the sampled data\n",
        "        X_transformed = vectorizer.fit_transform(sampled_data['bug_description'])\n",
        "\n",
        "        # Append the transformed data and vectorizer to the results list\n",
        "        results.append((X_transformed, vectorizer))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# all n-gram ranges till (6,7)\n",
        "ngram_ranges = [(1, 1), (1, 2), (1, 3) , (1,4) , (1,5) , (1,6) , (1,7) , (1,8) , (1,9) , (1,10) ,(1,11) , (1,12) , (1,13) , (1,14) ]\n",
        "#ngram_ranges = [(2,2) , (2,3) , (2,4) , (2,5) , (2,6) , (2,7) , (2,8) , (2,9) , (2,10) , (2,11) , (2,12) , (2,13) , (2,14) , (2,15)]\n",
        "\n",
        "transformed_data = try_ngram_combinations(filtered_train_df, ngram_ranges, sample_size=350)\n",
        "\n",
        "# Print the shape of transformed data for each combination\n",
        "for data, vectorizer in transformed_data:\n",
        "    print(f\"Shape of transformed data for n-gram range {vectorizer.ngram_range}: {data.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYbTc1TS5hS5",
        "outputId": "2b3bf895-37d0-45f5-c212-5a1a259a3baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 298)\t0.46191225500985983\n",
            "  (0, 1642)\t0.40364883059940254\n",
            "  (0, 389)\t0.3822538474978055\n",
            "  (0, 1696)\t0.34798112074686754\n",
            "  (0, 1016)\t0.37376309676775177\n",
            "  (0, 424)\t0.39205569986792155\n",
            "  (0, 39)\t0.25117583800989696\n"
          ]
        }
      ],
      "source": [
        "# print the vector representation of the first report\n",
        "print(transformed_data[0][0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkznZ3o45hS5",
        "outputId": "d0caa724-497a-4969-937d-504108c64f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "['Frontend' 'Backend' 'Security']\n",
            "class_name\n",
            "Backend     7437\n",
            "Frontend    5799\n",
            "Security     367\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Print the number of unique class_name in the training data\n",
        "print(filtered_train_df['class_name'].nunique())\n",
        "\n",
        "# print their unique values\n",
        "print(filtered_train_df['class_name'].unique())\n",
        "\n",
        "# print the number of reports in each class\n",
        "print(filtered_train_df['class_name'].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "EODF3FRf5hS5"
      },
      "outputs": [],
      "source": [
        "# Create a mapping between cluster labels and class names\n",
        "cluster_class_mapping = {\n",
        "    1: 'Backend',  # Example mapping, adjust based on your actual clusters\n",
        "    0: 'Frontend',\n",
        "    2: 'Security'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "TgVdg6zx5hS5",
        "outputId": "eafcec58-5bda-4148-af44-53895f092b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained using n-gram range: (1, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [696, 2423]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f0986646985d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Perform cross-validation to evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_test_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cross-Validation Scores: {scores}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m0.28009951\u001b[0m \u001b[0;36m0.3908844\u001b[0m  \u001b[0;36m0.22784907\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [696, 2423]"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the SVM model\n",
        "model = SVC(C = 100)\n",
        "\n",
        "# Iterate over each combination of transformed data and vectorizer\n",
        "for data, vectorizer in transformed_data:\n",
        "    print(f\"Model trained using n-gram range: {vectorizer.ngram_range}\")\n",
        "\n",
        "    # Perform cross-validation to evaluate the model\n",
        "    scores = cross_val_score(model, data, filtered_test_df['class_name'], cv=5)\n",
        "    print(f\"Cross-Validation Scores: {scores}\")\n",
        "\n",
        "    # Fit the model on the entire training data\n",
        "    model.fit(data, filtered_test_df['class_name'])\n",
        "\n",
        "    # Predict the class labels for the testing data\n",
        "    X_test_transformed = vectorizer.transform(filtered_train_df['bug_description'])\n",
        "    y_pred = model.predict(X_test_transformed)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(classification_report(filtered_train_df['class_name'], y_pred, target_names=filtered_train_df['class_name'].unique()))\n",
        "\n",
        "    # Print the confusion matrix\n",
        "    print(pd.crosstab(filtered_train_df['class_name'], y_pred, rownames=['Actual'], colnames=['Predicted']))\n",
        "\n",
        "    # Print the accuracy\n",
        "    accuracy = model.score(X_test_transformed, filtered_train_df['class_name'])\n",
        "    print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s3uKdXN5hS6"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whAn8k5i5hS6",
        "outputId": "1d288b09-df56-42a4-99f5-77759ee13cc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained using n-gram range: (1, 1)\n",
            "Cross-Validation Scores: [0.84948454 0.80412371 0.83505155 0.84090909 0.82438017]\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Frontend       0.59      0.97      0.73      7437\n",
            "      Backend       0.50      0.01      0.01       174\n",
            "     Security       0.84      0.20      0.32      5799\n",
            "Documentation       0.95      0.05      0.09       367\n",
            "\n",
            "     accuracy                           0.61     13777\n",
            "    macro avg       0.72      0.31      0.29     13777\n",
            " weighted avg       0.70      0.61      0.53     13777\n",
            "\n",
            "Predicted      Backend  Documentation  Frontend  Security\n",
            "Actual                                                   \n",
            "Backend           7235              0       202         0\n",
            "Documentation      167              1         6         0\n",
            "Frontend          4629              1      1168         1\n",
            "Security           329              0        20        18\n",
            "Accuracy: 0.6113087029106482\n",
            "Model trained using n-gram range: (1, 2)\n",
            "Cross-Validation Scores: [0.84948454 0.82061856 0.82061856 0.83884298 0.82438017]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Frontend       0.58      0.98      0.73      7437\n",
            "      Backend       0.00      0.00      0.00       174\n",
            "     Security       0.87      0.18      0.30      5799\n",
            "Documentation       0.89      0.07      0.12       367\n",
            "\n",
            "     accuracy                           0.61     13777\n",
            "    macro avg       0.58      0.31      0.29     13777\n",
            " weighted avg       0.70      0.61      0.52     13777\n",
            "\n",
            "Predicted      Backend  Frontend  Security\n",
            "Actual                                    \n",
            "Backend           7296       139         2\n",
            "Documentation      168         6         0\n",
            "Frontend          4754      1044         1\n",
            "Security           329        14        24\n",
            "Accuracy: 0.6070987878347971\n",
            "Model trained using n-gram range: (1, 3)\n",
            "Cross-Validation Scores: [0.83298969 0.80824742 0.82061856 0.82231405 0.82231405]\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 3.03 GiB for an array with shape (25123, 16200) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m y_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((filtered_test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m], pseudo_labels))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Convert documents to lowercase for string columns\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m X_combined_lower \u001b[38;5;241m=\u001b[39m \u001b[43mX_combined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplymap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Convert X_combined_lower to a dense numpy array\u001b[39;00m\n\u001b[0;32m     31\u001b[0m X_combined_lower_dense \u001b[38;5;241m=\u001b[39m X_combined_lower\u001b[38;5;241m.\u001b[39mto_numpy()\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9653\u001b[0m, in \u001b[0;36mDataFrame.applymap\u001b[1;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[0;32m   9650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(x, func, ignore_na\u001b[38;5;241m=\u001b[39mignore_na)\n\u001b[0;32m   9651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(x\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values, func, ignore_na\u001b[38;5;241m=\u001b[39mignore_na)\n\u001b[1;32m-> 9653\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplymap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9557\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9559\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9561\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9566\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9567\u001b[0m )\n\u001b[1;32m-> 9568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:894\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    891\u001b[0m results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_index\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:920\u001b[0m, in \u001b[0;36mFrameApply.wrap_results\u001b[1;34m(self, results, res_index)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;66;03m# see if we can infer the results\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;129;01mand\u001b[39;00m is_sequence(results[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m--> 920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_results_for_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# dict of scalars\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# the default dtype of an empty Series will be `object`, but this\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# code can be hit by df.mean() where the result should have dtype\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# float64 even if it's an empty Series.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m constructor_sliced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor_sliced\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:988\u001b[0m, in \u001b[0;36mFrameRowApply.wrap_results_for_axis\u001b[1;34m(self, results, res_index)\u001b[0m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 988\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;66;03m# e.g. result = [[2, 3], [1.5], ['foo', 'bar']]\u001b[39;00m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;66;03m#  see test_agg_listlike_result GH#29587\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:154\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    151\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2199\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate)\u001b[0m\n\u001b[0;32m   2182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[0;32m   2183\u001b[0m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[0;32m   2184\u001b[0m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2195\u001b[0m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2199\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2200\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m BlockManager(blocks, axes, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2273\u001b[0m, in \u001b[0;36m_form_blocks\u001b[1;34m(arrays, consolidate)\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype\u001b[38;5;241m.\u001b[39mtype, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m   2271\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m-> 2273\u001b[0m values, placement \u001b[38;5;241m=\u001b[39m \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[0;32m   2275\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2312\u001b[0m, in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   2309\u001b[0m first \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2310\u001b[0m shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(arrays),) \u001b[38;5;241m+\u001b[39m first\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m-> 2312\u001b[0m stacked \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[0;32m   2314\u001b[0m     stacked[i] \u001b[38;5;241m=\u001b[39m arr\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.03 GiB for an array with shape (25123, 16200) and data type float64"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm_model = SVC(C=100)\n",
        "\n",
        "# Iterate over each combination of transformed data and vectorizer\n",
        "for data, vectorizer in transformed_data:\n",
        "    print(f\"Model trained using n-gram range: {vectorizer.ngram_range}\")\n",
        "\n",
        "    # Perform cross-validation to evaluate the SVM model\n",
        "    scores = cross_val_score(svm_model, data, filtered_test_df['class_name'], cv=5)\n",
        "    print(f\"Cross-Validation Scores: {scores}\")\n",
        "\n",
        "    # Fit the SVM model on the entire training data\n",
        "    svm_model.fit(data, filtered_test_df['class_name'])\n",
        "\n",
        "    # Predict the class labels for the unlabeled data\n",
        "    X_unlabeled = vectorizer.transform(filtered_train_df['bug_description'])\n",
        "    pseudo_labels = svm_model.predict(X_unlabeled)\n",
        "\n",
        "    # Combine the labeled and pseudo-labeled data\n",
        "    labeled_data = pd.DataFrame(data.todense(), columns=vectorizer.get_feature_names_out())\n",
        "    unlabeled_data = pd.DataFrame(X_unlabeled.todense(), columns=vectorizer.get_feature_names_out())\n",
        "    X_combined = pd.concat([labeled_data, unlabeled_data])\n",
        "    y_combined = np.hstack((filtered_test_df['class_name'], pseudo_labels))\n",
        "\n",
        "    # Convert documents to lowercase for string columns\n",
        "    X_combined_lower = X_combined.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "    # Convert X_combined_lower to a dense numpy array\n",
        "    X_combined_lower_dense = X_combined_lower.to_numpy()\n",
        "\n",
        "    # Train an unsupervised model on the combined labeled and pseudo-labeled data\n",
        "    semi_supervised_model = LabelSpreading(kernel='knn')\n",
        "    semi_supervised_model.fit(X_combined_lower_dense, y_combined)\n",
        "\n",
        "    # Evaluate the semi-supervised model on the labeled test data\n",
        "    X_test_transformed = vectorizer.transform(filtered_train_df['bug_description'])\n",
        "    y_pred = semi_supervised_model.predict(X_test_transformed)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(classification_report(filtered_train_df['class_name'], y_pred, target_names=filtered_train_df['class_name'].unique()))\n",
        "\n",
        "    # Print the confusion matrix\n",
        "    print(pd.crosstab(filtered_train_df['class_name'], y_pred, rownames=['Actual'], colnames=['Predicted']))\n",
        "\n",
        "    # Print the accuracy\n",
        "    accuracy = semi_supervised_model.score(X_test_transformed, filtered_train_df['class_name'])\n",
        "    print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMYzEB-05hS6",
        "outputId": "b7a4d446-bc2f-463a-bc9d-e146486a9cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained using n-gram range: (1, 1)\n",
            "Cross-Validation Scores (SVM): [0.84948454 0.80412371 0.83505155 0.84090909 0.82438017]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LabelPropagation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Frontend       0.84      0.90      0.87      7437\n",
            "      Backend       0.80      0.05      0.09       174\n",
            "     Security       0.84      0.82      0.83      5799\n",
            "Documentation       0.92      0.16      0.27       367\n",
            "\n",
            "     accuracy                           0.84     13777\n",
            "    macro avg       0.85      0.48      0.52     13777\n",
            " weighted avg       0.84      0.84      0.83     13777\n",
            "\n",
            "Predicted      Backend  Documentation  Frontend  Security\n",
            "Actual                                                   \n",
            "Backend           6717              1       717         2\n",
            "Documentation      122              8        44         0\n",
            "Frontend          1028              1      4767         3\n",
            "Security           156              0       152        59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LabelPropagation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (LabelPropagation): 0.838426362778544\n",
            "Model trained using n-gram range: (1, 2)\n",
            "Cross-Validation Scores (SVM): [0.84948454 0.82061856 0.82061856 0.83884298 0.82438017]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LabelPropagation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Frontend       0.84      0.90      0.87      7437\n",
            "      Backend       1.00      0.02      0.04       174\n",
            "     Security       0.84      0.83      0.83      5799\n",
            "Documentation       0.91      0.14      0.24       367\n",
            "\n",
            "     accuracy                           0.84     13777\n",
            "    macro avg       0.90      0.47      0.50     13777\n",
            " weighted avg       0.84      0.84      0.83     13777\n",
            "\n",
            "Predicted      Backend  Documentation  Frontend  Security\n",
            "Actual                                                   \n",
            "Backend           6730              0       703         4\n",
            "Documentation      131              4        39         0\n",
            "Frontend          1009              0      4789         1\n",
            "Security           162              0       155        50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LabelPropagation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (LabelPropagation): 0.8400232271176599\n",
            "Model trained using n-gram range: (1, 3)\n",
            "Cross-Validation Scores (SVM): [0.83298969 0.80824742 0.82061856 0.82231405 0.82231405]\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 1.96 GiB for an array with shape (16200, 16200) and data type float64",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m y_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((filtered_test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m], pseudo_labels))\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Train the LabelPropagation model on the combined data\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mlabel_prop_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Evaluate the LabelPropagation model on the labeled test data\u001b[39;00m\n\u001b[0;32m     40\u001b[0m X_test_transformed \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(filtered_train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbug_description\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\semi_supervised\\_label_propagation.py:480\u001b[0m, in \u001b[0;36mLabelPropagation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit a semi-supervised label propagation model to X.\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\semi_supervised\\_label_propagation.py:264\u001b[0m, in \u001b[0;36mBaseLabelPropagation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    261\u001b[0m check_classification_targets(y)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# actual graph construction (implementations should override this)\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m graph_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# label construction\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# construct a categorical distribution for classification only\u001b[39;00m\n\u001b[0;32m    268\u001b[0m classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\semi_supervised\\_label_propagation.py:453\u001b[0m, in \u001b[0;36mLabelPropagation._build_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m affinity_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    454\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m affinity_matrix\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39misspmatrix(affinity_matrix):\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\semi_supervised\\_label_propagation.py:148\u001b[0m, in \u001b[0;36mBaseLabelPropagation._get_kernel\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrbf_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rbf_kernel(X, y, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma)\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1314\u001b[0m, in \u001b[0;36mrbf_kernel\u001b[1;34m(X, Y, gamma)\u001b[0m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m     gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1314\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43meuclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m K \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mgamma\n\u001b[0;32m   1316\u001b[0m np\u001b[38;5;241m.\u001b[39mexp(K, K)  \u001b[38;5;66;03m# exponentiate K in-place\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:328\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Y_norm_squared\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m1\u001b[39m, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimensions for Y of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    325\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY_norm_squared of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m         )\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:369\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    366\u001b[0m     distances \u001b[38;5;241m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX\n\u001b[0;32m    371\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.96 GiB for an array with shape (16200, 16200) and data type float64"
          ]
        }
      ],
      "source": [
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm_model = SVC(C=100)\n",
        "\n",
        "# Initialize the LabelPropagation model\n",
        "label_prop_model = LabelPropagation()\n",
        "\n",
        "# Iterate over each combination of transformed data and vectorizer\n",
        "for data, vectorizer in transformed_data:\n",
        "    print(f\"Model trained using n-gram range: {vectorizer.ngram_range}\")\n",
        "\n",
        "    # Perform cross-validation to evaluate the SVM model\n",
        "    scores = cross_val_score(svm_model, data, filtered_test_df['class_name'], cv=5)\n",
        "    print(f\"Cross-Validation Scores (SVM): {scores}\")\n",
        "\n",
        "    # Fit the SVM model on the entire training data\n",
        "    svm_model.fit(data, filtered_test_df['class_name'])\n",
        "\n",
        "    # Predict the class labels for the unlabeled data\n",
        "    X_unlabeled = vectorizer.transform(filtered_train_df['bug_description'])\n",
        "    pseudo_labels = svm_model.predict(X_unlabeled)\n",
        "\n",
        "    # Combine the labeled and pseudo-labeled data\n",
        "    labeled_data = pd.DataFrame(data.todense(), columns=vectorizer.get_feature_names_out())\n",
        "    unlabeled_data = pd.DataFrame(X_unlabeled.todense(), columns=vectorizer.get_feature_names_out())\n",
        "    X_combined = pd.concat([labeled_data, unlabeled_data])\n",
        "    y_combined = np.hstack((filtered_test_df['class_name'], pseudo_labels))\n",
        "\n",
        "    # Train the LabelPropagation model on the combined data\n",
        "    label_prop_model.fit(X_combined, y_combined)\n",
        "\n",
        "    # Evaluate the LabelPropagation model on the labeled test data\n",
        "    X_test_transformed = vectorizer.transform(filtered_train_df['bug_description'])\n",
        "    y_pred = label_prop_model.predict(X_test_transformed)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(classification_report(filtered_train_df['class_name'], y_pred, target_names=filtered_train_df['class_name'].unique()))\n",
        "\n",
        "    # Print the confusion matrix\n",
        "    print(pd.crosstab(filtered_train_df['class_name'], y_pred, rownames=['Actual'], colnames=['Predicted']))\n",
        "\n",
        "    # Print the accuracy\n",
        "    accuracy = label_prop_model.score(X_test_transformed, filtered_train_df['class_name'])\n",
        "    print(f\"Accuracy (LabelPropagation): {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OR_PlsZ5hS6"
      },
      "outputs": [],
      "source": [
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm_model = SVC(C=100)\n",
        "\n",
        "# Initialize the LabelPropagation model\n",
        "label_prop_model = LabelPropagation()\n",
        "\n",
        "# Iterate over each combination of transformed data and vectorizer\n",
        "for data, vectorizer in transformed_data:\n",
        "    print(f\"Model trained using n-gram range: {vectorizer.ngram_range}\")\n",
        "\n",
        "    # Perform cross-validation to evaluate the SVM model\n",
        "    scores = cross_val_score(svm_model, data, filtered_test_df['class_name'], cv=5)\n",
        "    print(f\"Cross-Validation Scores (SVM): {scores}\")\n",
        "\n",
        "    # Fit the SVM model on the entire training data\n",
        "    svm_model.fit(data, filtered_test_df['class_name'])\n",
        "\n",
        "    # Predict the class labels for the unlabeled data\n",
        "    X_unlabeled = vectorizer.transform(filtered_train_df['bug_description'])\n",
        "    pseudo_labels = svm_model.predict(X_unlabeled)\n",
        "\n",
        "    # Combine the labeled and pseudo-labeled data\n",
        "    labeled_data = pd.DataFrame(data.todense(), columns=vectorizer.get_feature_names_out())\n",
        "    unlabeled_data = pd.DataFrame(X_unlabeled.todense(), columns=vectorizer.get_feature_names_out())\n",
        "    X_combined = pd.concat([labeled_data, unlabeled_data])\n",
        "    y_combined = np.hstack((filtered_test_df['class_name'], pseudo_labels))\n",
        "\n",
        "    # Train the LabelPropagation model on the combined data\n",
        "    label_prop_model.fit(X_combined, y_combined)\n",
        "\n",
        "    # Evaluate the LabelPropagation model on the labeled test data\n",
        "    X_test_transformed = vectorizer.transform(filtered_train_df['bug_description'])\n",
        "    y_pred = label_prop_model.predict(X_test_transformed)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(classification_report(filtered_train_df['class_name'], y_pred, target_names=filtered_train_df['class_name'].unique()))\n",
        "\n",
        "    # Print the confusion matrix\n",
        "    print(pd.crosstab(filtered_train_df['class_name'], y_pred, rownames=['Actual'], colnames=['Predicted']))\n",
        "\n",
        "    # Print the accuracy\n",
        "    accuracy = label_prop_model.score(X_test_transformed, filtered_train_df['class_name'])\n",
        "    print(f\"Accuracy (LabelPropagation): {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1vkURbU5hS7",
        "outputId": "8fcb1b29-d3b9-421b-f526-c629530be949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained using n-gram range: (1, 1)\n",
            "Cross-Validation Scores (SVM): [0.76190476 0.73809524 0.7047619  0.8        0.71428571]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LabelPropagation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Frontend       0.89      0.76      0.82      7437\n",
            "     Backend       0.79      0.80      0.79      5799\n",
            "    Security       0.25      0.98      0.40       367\n",
            "\n",
            "    accuracy                           0.78     13603\n",
            "   macro avg       0.64      0.85      0.67     13603\n",
            "weighted avg       0.83      0.78      0.80     13603\n",
            "\n",
            "Predicted  Backend  Frontend  Security\n",
            "Actual                                \n",
            "Backend       5646      1218       573\n",
            "Frontend       684      4614       501\n",
            "Security         2         4       361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LabelPropagation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (LabelPropagation): 0.7807836506652944\n",
            "Model trained using n-gram range: (1, 2)\n",
            "Cross-Validation Scores (SVM): [0.73809524 0.75238095 0.71904762 0.8        0.74285714]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LabelPropagation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Frontend       0.90      0.76      0.82      7437\n",
            "     Backend       0.80      0.80      0.80      5799\n",
            "    Security       0.25      0.98      0.40       367\n",
            "\n",
            "    accuracy                           0.78     13603\n",
            "   macro avg       0.65      0.85      0.67     13603\n",
            "weighted avg       0.84      0.78      0.80     13603\n",
            "\n",
            "Predicted  Backend  Frontend  Security\n",
            "Actual                                \n",
            "Backend       5666      1184       587\n",
            "Frontend       661      4650       488\n",
            "Security         3         4       360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LabelPropagation was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (LabelPropagation): 0.7848268764243181\n",
            "Model trained using n-gram range: (1, 3)\n",
            "Cross-Validation Scores (SVM): [0.73809524 0.75714286 0.72380952 0.8        0.72857143]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.semi_supervised import LabelPropagation\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm_model = SVC(C=100)\n",
        "\n",
        "# Initialize the LabelPropagation model\n",
        "label_prop_model = LabelPropagation()\n",
        "\n",
        "# Initialize a DataFrame to store the sampled data\n",
        "sampled_data = pd.DataFrame(columns=['bug_description', 'class_name'])\n",
        "\n",
        "# Sample 100 samples from each class\n",
        "for class_name in filtered_train_df['class_name'].unique():\n",
        "    class_samples = filtered_train_df[filtered_train_df['class_name'] == class_name].sample(n=350, random_state=42)\n",
        "    sampled_data = pd.concat([sampled_data, class_samples])\n",
        "\n",
        "# Iterate over each combination of transformed data and vectorizer\n",
        "for data, vectorizer in transformed_data:\n",
        "    print(f\"Model trained using n-gram range: {vectorizer.ngram_range}\")\n",
        "\n",
        "    # Perform cross-validation to evaluate the SVM model\n",
        "    scores = cross_val_score(svm_model, data, sampled_data['class_name'], cv=5)\n",
        "    print(f\"Cross-Validation Scores (SVM): {scores}\")\n",
        "\n",
        "    # Fit the SVM model on the entire training data\n",
        "    svm_model.fit(data, sampled_data['class_name'])\n",
        "\n",
        "    # Predict the class labels for the unlabeled data\n",
        "    X_unlabeled = vectorizer.transform(filtered_train_df['bug_description'])\n",
        "    pseudo_labels = svm_model.predict(X_unlabeled)\n",
        "\n",
        "    # Combine the labeled and pseudo-labeled data\n",
        "    labeled_data = pd.DataFrame(data.todense(), columns=vectorizer.get_feature_names_out())\n",
        "    unlabeled_data = pd.DataFrame(X_unlabeled.todense(), columns=vectorizer.get_feature_names_out())\n",
        "    X_combined = pd.concat([labeled_data, unlabeled_data])\n",
        "    y_combined = np.hstack((sampled_data['class_name'], pseudo_labels))\n",
        "\n",
        "    # Train the LabelPropagation model on the combined data\n",
        "    label_prop_model.fit(X_combined, y_combined)\n",
        "\n",
        "    # Evaluate the LabelPropagation model on the labeled test data\n",
        "    X_test_transformed = vectorizer.transform(filtered_train_df['bug_description'])\n",
        "    y_pred = label_prop_model.predict(X_test_transformed)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(classification_report(filtered_train_df['class_name'], y_pred, target_names=filtered_train_df['class_name'].unique()))\n",
        "\n",
        "    # Print the confusion matrix\n",
        "    print(pd.crosstab(filtered_train_df['class_name'], y_pred, rownames=['Actual'], colnames=['Predicted']))\n",
        "\n",
        "    # Print the accuracy\n",
        "    accuracy = label_prop_model.score(X_test_transformed, filtered_train_df['class_name'])\n",
        "    print(f\"Accuracy (LabelPropagation): {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7nnnyrc5hS7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}