{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    description regression group toc created autom...\n",
      "1    output column page data set editor used result...\n",
      "2    description regression failed preview chart vi...\n",
      "3    description exception thrown link label anothe...\n",
      "4    build id step reproduce start eclipse click he...\n",
      "Name: preprocessed_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read the file eclipse_new_updated.csv\n",
    "df = pd.read_csv(\"..\\\\new_dataset\\\\eclipse\\\\eclipse_small_new.csv\")\n",
    "\n",
    "\n",
    "# Print the first 5 rows of the dataframe (preprocessed_description column only)\n",
    "print(df['preprocessed_description'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bug_id                                        description dup_id  \\\n",
      "762  214940  If my memory don't fail me, content.xml and ar...     []   \n",
      "\n",
      "                              preprocessed_description  \n",
      "762  memory dont fail content xml artifact xml curr...  \n",
      "     bug_id                                        description dup_id  \\\n",
      "394  214519  It would be useful to allow SQL to be appended...     []   \n",
      "\n",
      "                              preprocessed_description  \n",
      "394  would useful allow sql appended end create tab...  \n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing data\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# print the first 1 rows of the training data\n",
    "print(X_train.head(1))\n",
    "\n",
    "# print the first 1 rows of the testing data\n",
    "print(X_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (696, 4)\n",
      "Testing data size: (174, 4)\n"
     ]
    }
   ],
   "source": [
    "# print the size of training and testing data\n",
    "print(f\"Training data size: {X_train.shape}\")\n",
    "print(f\"Testing data size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bag of words to convert the preprocessed_description into numerical data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['preprocessed_description'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['preprocessed_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary to store the test example bug_id as a key and the predicted duplicates as values\n",
    "predicted_duplicates_dict = {}\n",
    "# Make a dictionary to store the test example bug_id as a key and the corrected duplicates as values\n",
    "true_duplicates_dict = {}\n",
    "\n",
    "# Iterate over each example of the test data\n",
    "for i in range(X_test_tfidf.shape[0]):\n",
    "    predicted_duplicates_dict[X_test['bug_id'].values[i]] = []\n",
    "    true_duplicates_dict[X_test['bug_id'].values[i]] = []\n",
    "\n",
    "    # Calculate the cosine similarity between the test example and all the training examples\n",
    "    similarity = cosine_similarity(X_test_tfidf[i], X_train_tfidf)\n",
    "\n",
    "    # Get the bug_ids of the training examples that have similarity greater than or equal to 0.9\n",
    "    bug_ids = X_train['bug_id'].values[np.where(similarity >= 0.9)[1]]\n",
    "    # If bug_ids is empty, replace it with -1\n",
    "    if len(bug_ids) == 0:\n",
    "        bug_ids = [-1]\n",
    "\n",
    "    # Append the bug_ids to the predicted_duplicates list\n",
    "    predicted_duplicates_dict[X_test['bug_id'].values[i]].extend(bug_ids)\n",
    "    print(\"Predicted Duplicates for Bug ID {}: {}\".format(X_test['bug_id'].values[i], bug_ids))\n",
    "\n",
    "    # Print the bug ID of the test example\n",
    "    print(\"Test Bug ID:\", X_test['bug_id'].values[i])\n",
    "\n",
    "    # Along with the bug_id of the training example\n",
    "    print(\"Training Bug IDs:\", X_train['bug_id'].values[np.where(similarity >= 0.9)[1]])\n",
    "\n",
    "    # Get the dup_id of the test example\n",
    "    dup_ids_str = X_test['dup_id'].values[i]\n",
    "\n",
    "    # Handle cases where dup_ids is a string representation of a list\n",
    "    if dup_ids_str.startswith('[') and dup_ids_str.endswith(']'):\n",
    "        dup_ids_str = dup_ids_str[1:-1]\n",
    "        if dup_ids_str:\n",
    "            dup_ids = dup_ids_str.split(', ')\n",
    "        else:\n",
    "            dup_ids = []\n",
    "    else:\n",
    "        dup_ids = dup_ids_str.split(';')\n",
    "\n",
    "    # After splitting check if there is an empty string in the list and remove it\n",
    "    if '' in dup_ids:\n",
    "        dup_ids.remove('')\n",
    "\n",
    "    # If the dup_ids list is empty, replace it with -1\n",
    "    if len(dup_ids) == 0:\n",
    "        dup_ids = [-1]\n",
    "    else:\n",
    "        # Make the dup_ids list as integers\n",
    "        dup_ids = [int(dup_id) for dup_id in dup_ids]\n",
    "\n",
    "    # Append the dup_ids to the true_duplicates list\n",
    "    true_duplicates_dict[X_test['bug_id'].values[i]].extend(dup_ids)\n",
    "    print(\"True Duplicates for Bug ID {}: {}\".format(X_test['bug_id'].values[i], dup_ids))\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Calculate the true_positive, false_positive, true_negative, false_negative\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "\n",
    "for key in predicted_duplicates_dict.keys():\n",
    "    if -1 in predicted_duplicates_dict[key] and -1 in true_duplicates_dict[key]:\n",
    "        true_negative += 1\n",
    "    elif -1 in predicted_duplicates_dict[key] and -1 not in true_duplicates_dict[key]:\n",
    "        false_negative += 1\n",
    "    elif -1 not in predicted_duplicates_dict[key] and -1 in true_duplicates_dict[key]:\n",
    "        false_positive += 1\n",
    "    elif -1 not in predicted_duplicates_dict[key] and -1 not in true_duplicates_dict[key]:\n",
    "        for bug_id in predicted_duplicates_dict[key]:\n",
    "            if bug_id in true_duplicates_dict[key]:\n",
    "                true_positive += 1\n",
    "\n",
    "# Calculate the precision, recall, and f1 score\n",
    "precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative) if (true_positive + true_negative + false_positive + false_negative) > 0 else 0\n",
    "\n",
    "#print(\"Precision:\", precision)\n",
    "#print(\"Recall:\", recall)\n",
    "#print(\"F1 Score:\", f1)\n",
    "#print(\"Accuracy Score:\", accuracy)\n",
    "\n",
    "# Write the values to results.txt file\n",
    "with open(\"results.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy Score: \" + str(accuracy) + \"\\n\")\n",
    "    f.write(\"Precision: \" + str(precision) + \"\\n\")\n",
    "    f.write(\"Recall: \" + str(recall) + \"\\n\")\n",
    "    f.write(\"F1 Score: \" + str(f1) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
