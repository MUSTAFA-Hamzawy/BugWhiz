{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    description regression group toc created autom...\n",
      "1    output column page data set editor used result...\n",
      "2    description regression failed preview chart vi...\n",
      "3    description exception thrown link label anothe...\n",
      "4    build id step reproduce start eclipse click he...\n",
      "Name: preprocessed_description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read the file eclipse_new_updated.csv\n",
    "df = pd.read_csv(\"..\\\\new_dataset\\\\eclipse\\\\eclipse_small_new.csv\")\n",
    "\n",
    "\n",
    "# Print the first 5 rows of the dataframe (preprocessed_description column only)\n",
    "print(df['preprocessed_description'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bug_id                                        description dup_id  \\\n",
      "762  214940  If my memory don't fail me, content.xml and ar...     []   \n",
      "\n",
      "                              preprocessed_description  \n",
      "762  memory dont fail content xml artifact xml curr...  \n",
      "     bug_id                                        description dup_id  \\\n",
      "394  214519  It would be useful to allow SQL to be appended...     []   \n",
      "\n",
      "                              preprocessed_description  \n",
      "394  would useful allow sql appended end create tab...  \n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing data\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the first 1 row of the training data\n",
    "print(X_train.head(1))\n",
    "\n",
    "# Print the first 1 row of the testing data\n",
    "print(X_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (696, 4)\n",
      "Testing data size: (174, 4)\n"
     ]
    }
   ],
   "source": [
    "# Print the size of training and testing data\n",
    "print(f\"Training data size: {X_train.shape}\")\n",
    "print(f\"Testing data size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Latent Semantic Analysis\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df['preprocessed_description'])\n",
    "lsa = TruncatedSVD(n_components=100)\n",
    "X_lsa = lsa.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07362097 -0.15141954 -0.07436443 -0.10251979  0.0217279  -0.09031283\n",
      "  0.01396534 -0.17144325  0.00387314  0.20601978  0.16086724 -0.00818873\n",
      "  0.04187784 -0.00436555  0.07027703 -0.06821416  0.00393371  0.00311686\n",
      "  0.05898488  0.00574588 -0.09679636  0.10118397 -0.00454336  0.05949887\n",
      "  0.06730524  0.02513401  0.01154922 -0.00488237 -0.01227337 -0.01954594\n",
      "  0.21701561 -0.0056095   0.04790338  0.01996441 -0.01976221 -0.03331464\n",
      " -0.05086255  0.00259736  0.03020372  0.09977146 -0.06863719  0.05401869\n",
      " -0.006327    0.02225704 -0.04710008  0.01016613 -0.01962845  0.00712673\n",
      " -0.15783228  0.04741512 -0.04813148  0.0097571  -0.02319158  0.06597655\n",
      "  0.05067605 -0.08362086 -0.08307749 -0.00039901  0.02356209  0.16990979\n",
      " -0.04431828 -0.0566475  -0.04300568  0.03347948 -0.12454569 -0.00514543\n",
      "  0.02446015 -0.04915059  0.02503544  0.03187724 -0.09437575 -0.01061891\n",
      " -0.04405069  0.06047827 -0.00423513  0.1156071  -0.0754232   0.04987352\n",
      "  0.00469303  0.02919723 -0.02876616  0.09168144  0.01727132 -0.11619061\n",
      " -0.06333999 -0.01748729  0.05653232 -0.03575446 -0.02766319 -0.0060897\n",
      "  0.08942008 -0.01814263  0.0477948   0.02495175 -0.04457307  0.06169003\n",
      " -0.11325996 -0.05818708  0.00403419  0.04352631]\n"
     ]
    }
   ],
   "source": [
    "# Split the LSA transformed data into training and testing data\n",
    "X_train_lsa, X_test_lsa = train_test_split(X_lsa, test_size=0.2, random_state=42)\n",
    "# Print the first row of the LSA training data\n",
    "print(X_train_lsa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Meaning of the Numbers:\n",
    "Latent Dimensions (Topics):\n",
    "Each number corresponds to a value in one of the 100 topics identified by the LSA model. \n",
    "\n",
    "Document Representation:\n",
    "The vector [ 0.07362097, -0.15141954, ..., 0.04352631] is the representation of the first document in the latent semantic space. \n",
    "Each element of the vector indicates the extent to which the document is associated with the corresponding topic.\n",
    "\n",
    "Topic Strengths:\n",
    "The magnitude and sign (positive or negative) of each number indicate the strength and direction of the association with the corresponding latent dimension. \n",
    "Larger absolute values suggest a stronger association with that dimension.\n",
    "\n",
    "Practical Example:\n",
    "Assume X_train_lsa[0] corresponds to a document about \"machine learning algorithms.\n",
    "\n",
    "0.07362097 might indicate a weak association with the first latent topic.\n",
    "-0.15141954 might indicate a moderate negative association with the second latent topic.\n",
    "0.20601978 might indicate a strong positive association with the tenth latent topic.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary to store the test example bug_id as a key and the predicted duplicates as values\n",
    "predicted_duplicates_dict = {}\n",
    "# Make a dictionary to store the test example bug_id as a key and the corrected duplicates as values\n",
    "true_duplicates_dict = {}\n",
    "\n",
    "# Iterate over each example of the test data\n",
    "for i in range(X_test_lsa.shape[0]):\n",
    "    test_bug_id = X_test['bug_id'].values[i]\n",
    "    predicted_duplicates_dict[test_bug_id] = []\n",
    "    true_duplicates_dict[test_bug_id] = []\n",
    "\n",
    "    # Calculate the cosine similarity between the test example and all the training examples\n",
    "    similarity = cosine_similarity(X_test_lsa[i].reshape(1, -1), X_train_lsa)\n",
    "\n",
    "    # Get the bug_ids of the training examples that have similarity greater than or equal to 0.9\n",
    "    bug_ids = X_train['bug_id'].values[np.where(similarity >= 0.9)[1]]\n",
    "    # If bug_ids is empty, replace it with -1\n",
    "    if len(bug_ids) == 0:\n",
    "        bug_ids = [-1]\n",
    "\n",
    "    # Append the bug_ids to the predicted_duplicates list\n",
    "    predicted_duplicates_dict[test_bug_id].extend(bug_ids)\n",
    "    print(\"Predicted Duplicates for Bug ID {}: {}\".format(test_bug_id, bug_ids))\n",
    "\n",
    "    # Print the bug ID of the test example\n",
    "    print(\"Test Bug ID:\", test_bug_id)\n",
    "\n",
    "    # Along with the bug_id of the training example\n",
    "    print(\"Training Bug IDs:\", X_train['bug_id'].values[np.where(similarity >= 0.9)[1]])\n",
    "\n",
    "    # Get the dup_id of the test example\n",
    "    dup_ids_str = X_test['dup_id'].values[i]\n",
    "\n",
    "    # Handle cases where dup_ids is a string representation of a list\n",
    "    if dup_ids_str.startswith('[') and dup_ids_str.endswith(']'):\n",
    "        dup_ids_str = dup_ids_str[1:-1]\n",
    "        if dup_ids_str:\n",
    "            dup_ids = dup_ids_str.split(', ')\n",
    "        else:\n",
    "            dup_ids = []\n",
    "    else:\n",
    "        dup_ids = dup_ids_str.split(';')\n",
    "\n",
    "    # After splitting check if there is an empty string in the list and remove it\n",
    "    if '' in dup_ids:\n",
    "        dup_ids.remove('')\n",
    "\n",
    "    # If the dup_ids list is empty, replace it with -1\n",
    "    if len(dup_ids) == 0:\n",
    "        dup_ids = [-1]\n",
    "    else:\n",
    "        # Make the dup_ids list as integers\n",
    "        dup_ids = [int(dup_id) for dup_id in dup_ids]\n",
    "\n",
    "    # Append the dup_ids to the true_duplicates list\n",
    "    true_duplicates_dict[test_bug_id].extend(dup_ids)\n",
    "    print(\"True Duplicates for Bug ID {}: {}\".format(test_bug_id, dup_ids))\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Calculate the true_positive, false_positive, true_negative, false_negative\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "\n",
    "for key in predicted_duplicates_dict.keys():\n",
    "    if -1 in predicted_duplicates_dict[key] and -1 in true_duplicates_dict[key]:\n",
    "        true_negative += 1\n",
    "    elif -1 in predicted_duplicates_dict[key] and -1 not in true_duplicates_dict[key]:\n",
    "        false_negative += 1\n",
    "    elif -1 not in predicted_duplicates_dict[key] and -1 in true_duplicates_dict[key]:\n",
    "        false_positive += 1\n",
    "    elif -1 not in predicted_duplicates_dict[key] and -1 not in true_duplicates_dict[key]:\n",
    "        for bug_id in predicted_duplicates_dict[key]:\n",
    "            if bug_id in true_duplicates_dict[key]:\n",
    "                true_positive += 1\n",
    "\n",
    "# Calculate the precision, recall, and f1 score\n",
    "precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative) if (true_positive + true_negative + false_positive + false_negative) > 0 else 0\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Accuracy Score:\", accuracy)\n",
    "\n",
    "# Write the values to results.txt file\n",
    "with open(\"results.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy Score: \" + str(accuracy) + \"\\n\")\n",
    "    f.write(\"Precision: \" + str(precision) + \"\\n\")\n",
    "    f.write(\"Recall: \" + str(recall) + \"\\n\")\n",
    "    f.write(\"F1 Score: \" + str(f1) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
