{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths\n",
    "directories = [\n",
    "    (\"..\\\\new_dataset\\\\eclipse\", \"..\\\\new_dataset\\\\eclipse\"),\n",
    "    (\"..\\\\new_dataset\\\\eclipse_test\", \"..\\\\new_dataset\\\\eclipse_test\"),\n",
    "    (\"..\\\\new_dataset\\\\firefox\", \"..\\\\new_dataset\\\\firefox\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Creating Dictionary of each Bug_id and its Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse_new.csv\n",
      "0\n",
      "475211\n",
      "eclipse_test_new.csv\n",
      "1\n",
      "17\n",
      "firefox_new.csv\n",
      "10954\n",
      "955893\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each directory\n",
    "for source_dir, target_dir in directories:\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        print(file_name)\n",
    "\n",
    "        # Create new dictionary to store each set of duplicates together\n",
    "        # id starts from 1 and increments by 1 for each new set of duplicates\n",
    "        duplicates = {}\n",
    "\n",
    "        # Read the file\n",
    "        df = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "        \n",
    "        # get the minimum id and max id from the bug_id column by iterating over the rows\n",
    "        min_id = np.inf\n",
    "        max_id = -np.inf\n",
    "        for index, row in df.iterrows():\n",
    "            bug_id = row[\"bug_id\"]\n",
    "            if bug_id < min_id:\n",
    "                min_id = bug_id\n",
    "            if bug_id > max_id:\n",
    "                max_id = bug_id\n",
    "\n",
    "\n",
    "        # convert min_id and max_id to integers\n",
    "        min_id = int(min_id)\n",
    "        max_id = int(max_id)\n",
    "\n",
    "        print(min_id)\n",
    "        print(max_id)\n",
    "\n",
    "        # make the keys of the dictionary starting from the minimum id to the maximum id\n",
    "        # and set the value of each key to an empty list\n",
    "        for i in range(min_id, max_id + 1):\n",
    "            # check if i is in the bug_id column\n",
    "            if i in df[\"bug_id\"].values:\n",
    "                duplicates[i] = []\n",
    "\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "\n",
    "            # get the dup_id of the row\n",
    "            dup_id = row[\"dup_id\"]\n",
    "\n",
    "            # if the dup_id is not NaN or not [] or row[\"bug_id\"] is not integer\n",
    "            if not pd.isnull(dup_id) and dup_id != \"[]\":\n",
    "                # if dup_id is not integer then skip this row\n",
    "                if not dup_id.isdigit():\n",
    "                    continue\n",
    "                \n",
    "                # add the dup_id to the list of duplicates of the bug with the dup_id\n",
    "                # check if the dup_id is exists in keys of the dictionary\n",
    "                if int(dup_id) in duplicates:\n",
    "                    duplicates[int(dup_id)].append(int(row[\"bug_id\"]))\n",
    "        \n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "            # get the bug_id of the row\n",
    "            bug_id = row[\"bug_id\"]\n",
    "\n",
    "            # if the bug_id is in the duplicates dictionary\n",
    "            if bug_id in duplicates:\n",
    "                # set the dup_id of the row to the list of duplicates of the bug with the bug_id\n",
    "                df.at[index, \"dup_id\"] = duplicates[bug_id]\n",
    "\n",
    "        # Save the dataframe to a new file\n",
    "        df.to_csv(os.path.join(target_dir, file_name), index=False)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Make Sure No Nan or [] fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse_new.csv\n",
      "eclipse_test_new.csv\n",
      "firefox_new.csv\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each directory\n",
    "for source_dir, target_dir in directories:\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        print(file_name)\n",
    "\n",
    "        # Read the file\n",
    "        df = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "            # if the dup_id is not NaN or not []\n",
    "            if not pd.isnull(row[\"dup_id\"]) and row[\"dup_id\"] != \"[]\":\n",
    "                # append the bug_id to the dup_id of the same bug\n",
    "                df.at[index, \"dup_id\"] = [row[\"bug_id\"]] + eval(row[\"dup_id\"])\n",
    "\n",
    "        # Save the dataframe to a new file\n",
    "        df.to_csv(os.path.join(target_dir, file_name), index=False)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Make each ID has its Corressponding Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse_new.csv\n",
      "eclipse_test_new.csv\n",
      "firefox_new.csv\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each directory\n",
    "for source_dir, target_dir in directories:\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        print(file_name)\n",
    "\n",
    "        # Read the file\n",
    "        df = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "            # if the dup_id is not NaN or not []\n",
    "            if not pd.isnull(row[\"dup_id\"]) and row[\"dup_id\"] != \"[]\":\n",
    "\n",
    "                # Get the dup_id of the row\n",
    "                dup_id = eval(row[\"dup_id\"])\n",
    "                dup_id_copy = dup_id.copy()\n",
    "\n",
    "                # Split the dup_id into a list of integers\n",
    "                dup_id = [int(x) for x in dup_id]\n",
    "\n",
    "                # Iterate over each dup in the dup_id\n",
    "                for dup in dup_id:\n",
    "                    \n",
    "                    # if the dup exist in the bug_id column\n",
    "                    if dup in df.bug_id.values:\n",
    "\n",
    "                        # make the dup_id of the bug with the dup equal to the dup_id of the bug with the bug_id\n",
    "                        df.at[df[df.bug_id == dup].index[0], \"dup_id\"] = dup_id_copy\n",
    "\n",
    "        # Save the dataframe to a new file\n",
    "        df.to_csv(os.path.join(target_dir, file_name), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Make sure all duplicates are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse_new.csv\n",
      "eclipse_test_new.csv\n",
      "firefox_new.csv\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each directory\n",
    "for source_dir, target_dir in directories:\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        print(file_name)\n",
    "\n",
    "        # Read the file\n",
    "        df = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "            # if the dup_id is not NaN or not []\n",
    "            if not pd.isnull(row[\"dup_id\"]) and row[\"dup_id\"] != \"[]\":\n",
    "\n",
    "                # Get the dup_id of the row\n",
    "                dup_id = eval(row[\"dup_id\"])\n",
    "\n",
    "                # make sure all the elements in the dup_id are unique and not includ the bug_id of the same bug\n",
    "                dup_id = list(set(dup_id) - set([row[\"bug_id\"]]))\n",
    "\n",
    "                # update the dup_id of the row to the new dup_id\n",
    "                df.at[index, \"dup_id\"] = dup_id\n",
    "                \n",
    "                                \n",
    "        # Save the dataframe to a new file\n",
    "        df.to_csv(os.path.join(target_dir, file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Make Sure the Bug_id not existing in its Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipse_new.csv\n",
      "eclipse_test_new.csv\n",
      "firefox_new.csv\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each directory\n",
    "for source_dir, target_dir in directories:\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        print(file_name)\n",
    "\n",
    "        # Read the file\n",
    "        df = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "\n",
    "        # Iterate over each row in the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "            # if the dup_id is not NaN or not []\n",
    "            if not pd.isnull(row[\"dup_id\"]) and row[\"dup_id\"] != \"[]\":\n",
    "\n",
    "                # Get the dup_id of the row\n",
    "                dup_id = eval(row[\"dup_id\"])\n",
    "\n",
    "                # convert all the elements in the dup_id to integers\n",
    "                dup_id = [int(x) for x in dup_id]\n",
    "\n",
    "                # update the dup_id of the row to the new dup_id\n",
    "                df.at[index, \"dup_id\"] = dup_id\n",
    "                \n",
    "                                \n",
    "        # Save the dataframe to a new file\n",
    "        df.to_csv(os.path.join(target_dir, file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
