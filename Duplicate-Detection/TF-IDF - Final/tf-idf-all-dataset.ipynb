{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    km pm pr deletion indicator sync viewer subtle...\n",
      "1    setup project contains gif resource release pr...\n",
      "2    current vcm api repository adapter either pess...\n",
      "3    become synchronized project repository use dif...\n",
      "4    iresource setlocal ha problem method replaces ...\n",
      "Name: preprocessed_description, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_98920\\1341505279.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df2)\n"
     ]
    }
   ],
   "source": [
    "# Read the file eclipse_new_updated.csv\n",
    "df = pd.read_csv(\"..\\\\new_dataset\\\\eclipse\\\\eclipse_new.csv\")\n",
    "df2 = pd.read_csv(\"..\\\\new_dataset\\\\firefox\\\\firefox_new.csv\")\n",
    "\n",
    "# append the two dataframes into one\n",
    "df = df.append(df2)\n",
    "\n",
    "# Print the first 5 rows of the dataframe (preprocessed_description column only)\n",
    "print(df['preprocessed_description'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        bug_id                                        description dup_id  \\\n",
      "84372  88542.0  Is it my imagination or is the only way to cha...     []   \n",
      "\n",
      "      priority                           preprocessed_description  \n",
      "84372       P3  imagination way change data set chart go prope...  \n",
      "          bug_id                                        description dup_id  \\\n",
      "240900  264207.0  RAP 1.2 M5\\n\\nReproducable in controls demo\\n\\...     []   \n",
      "\n",
      "       priority                           preprocessed_description  \n",
      "240900       P3  rap reproducable control demo open expandbar t...  \n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing data\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# print the first 1 rows of the training data\n",
    "print(X_train.head(1))\n",
    "\n",
    "# print the first 1 rows of the testing data\n",
    "print(X_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (382667, 5)\n",
      "Testing data size: (95667, 5)\n"
     ]
    }
   ],
   "source": [
    "# print the size of training and testing data\n",
    "print(f\"Training data size: {X_train.shape}\")\n",
    "print(f\"Testing data size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdfVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_df : float, default=1.0\n",
    "        When building the vocabulary, ignore terms that have a document frequency\n",
    "        strictly higher than the given threshold (corpus-specific stop words).\n",
    "\n",
    "    min_df : int, default=1\n",
    "        When building the vocabulary, ignore terms that have a document frequency\n",
    "        strictly lower than the given threshold.\n",
    "\n",
    "    stop_words : list, default=None\n",
    "        If a list, it contains the stop words to be removed from the documents.\n",
    "\n",
    "    ngram_range : tuple, default=(1, 1)\n",
    "        The lower and upper boundary of the range of n-values for different n-grams\n",
    "        to be extracted.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_df=1.0, min_df=0.1, stop_words=None, ngram_range=(1, 1)):\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.stop_words = stop_words\n",
    "        self.ngram_range = ngram_range\n",
    "        self.vocab = {}\n",
    "        self.idf = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Learn the vocabulary and idf from the documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents : list of str\n",
    "            A list of raw documents to be vectorized.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        # Tokenize documents and calculate document frequency (DF)\n",
    "        doc_count = len(documents)\n",
    "        df = defaultdict(int)\n",
    "        \n",
    "        # Calculate document frequency (DF) for each token\n",
    "        for doc in documents:\n",
    "            tokens = self._tokenize(doc)\n",
    "            unique_tokens = set(tokens)\n",
    "            for token in unique_tokens:\n",
    "                df[token] += 1\n",
    "\n",
    "        # Filter tokens based on max_df and min_df thresholds and create vocabulary\n",
    "        self.vocab = {token: i for i, (token, count) in enumerate(df.items()) \n",
    "                      if count >= self.min_df and count <= self.max_df * doc_count}\n",
    "        \n",
    "        # Calculate inverse document frequency (IDF)\n",
    "        # ==> +1 for smoothing to avoid division by zero\n",
    "        self.idf = {token: math.log(doc_count / (count + 1)) + 1 for token, count in df.items() if token in self.vocab}\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        \"\"\"\n",
    "        Transform documents to document-term matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents : list of str\n",
    "            A list of raw documents to be vectorized.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : array of shape (n_samples, n_features)\n",
    "            The transformed document-term matrix.\n",
    "        \"\"\"\n",
    "        # Calculate TF-IDF for each document\n",
    "        tfidf_matrix = np.zeros((len(documents), len(self.vocab)))\n",
    "        \n",
    "        # Calculate TF-IDF for each document using the vocabulary and IDF values\n",
    "        for i, doc in enumerate(documents):\n",
    "            tokens = self._tokenize(doc)\n",
    "            tf = self._calculate_tf(tokens)\n",
    "            \n",
    "            # Calculate TF-IDF for each token in the document\n",
    "            for token, freq in tf.items():\n",
    "                if token in self.vocab:\n",
    "                    tfidf_matrix[i, self.vocab[token]] = freq * self.idf[token]\n",
    "        \n",
    "        return tfidf_matrix\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"\n",
    "        Learn the vocabulary and idf, return document-term matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        documents : list of str\n",
    "            A list of raw documents to be vectorized.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : array of shape (n_samples, n_features)\n",
    "            The transformed document-term matrix.\n",
    "        \"\"\"\n",
    "        return self.fit(documents).transform(documents)\n",
    "\n",
    "    def _tokenize(self, document):\n",
    "        \"\"\"\n",
    "        Tokenize the document into n-grams.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        document : str\n",
    "            A single document.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tokens : list of str\n",
    "            The list of tokens (n-grams) in the document.\n",
    "        \"\"\"\n",
    "        # Simple tokenization: split by whitespace\n",
    "        words = document.lower().split()\n",
    "        if self.stop_words:\n",
    "            words = [word for word in words if word not in self.stop_words]\n",
    "        \n",
    "        # Generate n-grams\n",
    "        tokens = []\n",
    "        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):\n",
    "            for i in range(len(words) - n + 1):\n",
    "                tokens.append(' '.join(words[i:i + n]))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def _calculate_tf(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculate term frequency (TF) for the document.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens : list of str\n",
    "            The list of tokens (n-grams) in the document.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tf : dict\n",
    "            The term frequency of each token.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate term frequency (TF)\n",
    "        tf = defaultdict(int)\n",
    "\n",
    "        # Count the frequency of each token in the document\n",
    "        for token in tokens:\n",
    "            tf[token] += 1\n",
    "        \n",
    "        # Normalize the frequency of each token by the total number of tokens\n",
    "        total_tokens = len(tokens)\n",
    "\n",
    "        # Return the normalized term frequency\n",
    "        return {token: freq / total_tokens for token, freq in tf.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF as a feature extraction technique\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['preprocessed_description'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['preprocessed_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to store the test example bug_id as a key and the predicted duplicates as values\n",
    "predicted_duplicates_dict = {}\n",
    "\n",
    "# make a dictionary to store the test example bug_id as a key and the corrected duplicates as values\n",
    "true_duplicates_dict = {}\n",
    "\n",
    "# Iterate over each example of the test data\n",
    "for i in range(X_test_tfidf.shape[0]):\n",
    "    predicted_duplicates_dict[X_test['bug_id'].values[i]] = []\n",
    "    true_duplicates_dict[X_test['bug_id'].values[i]] = []\n",
    "    \n",
    "    # Calculate the cosine similarity between the test example and all the training examples\n",
    "    similarity = cosine_similarity(X_test_tfidf[i], X_train_tfidf)\n",
    "\n",
    "    # Get the bug_ids of the training examples that have similarity greater than or equal to 0.9\n",
    "    bug_ids = X_train['bug_id'].values[np.where(similarity >= 0.9)[1]]\n",
    "    # if bug_ids is empty, replace it with -1\n",
    "    if len(bug_ids) == 0:\n",
    "        bug_ids = [-1]\n",
    "\n",
    "    # Append the bug_ids to the predicted_duplicates list\n",
    "    predicted_duplicates_dict[X_test['bug_id'].values[i]].extend(bug_ids)\n",
    "    print (\"Predicted Duplicates for Bug ID {}: {}\".format(X_test['bug_id'].values[i], bug_ids))\n",
    "\n",
    "    # Print the bug ID of the test example\n",
    "    print(\"Test Bug ID:\", X_test['bug_id'].values[i])\n",
    "\n",
    "    # along with the bug_id of the training example\n",
    "    print(\"Training Bug IDs:\", X_train['bug_id'].values[np.where(similarity >= 0.9)[1]])\n",
    "\n",
    "    # Get the dup_id of the test example and split it by semicolons\n",
    "    dup_ids = X_test['dup_id'].values[i].split(';')\n",
    "\n",
    "    # after splitting check if there is an empty string in the list and remove it\n",
    "    if '' in dup_ids:\n",
    "        dup_ids.remove('')\n",
    "\n",
    "    # if the dup_ids list is  ['[]'] replace it with -1\n",
    "    if dup_ids == ['[]']:\n",
    "        dup_ids = [-1]\n",
    "    # make the dup_ids list as integers\n",
    "    dup_ids = [int(dup_id) for dup_id in dup_ids]\n",
    "\n",
    "    # Append the dup_ids to the true_duplicates list\n",
    "    true_duplicates_dict[X_test['bug_id'].values[i]].extend(dup_ids)\n",
    "    print(\"True Duplicates for Bug ID {}: {}\".format(X_test['bug_id'].values[i], dup_ids))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# calculate the true_positive, false_positive, true_negative, false_negative\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "true_negative = 0\n",
    "false_negative = 0\n",
    "\n",
    "# There are 4 cases to consider:\n",
    "# 1. The predicted duplicate is in the true duplicates list (true positive)\n",
    "# 2. The predicted duplicate is not in the true duplicates list (false positive)\n",
    "# 3. The true duplicate is not predicted as a duplicate (false negative)\n",
    "# 4. The true duplicate is predicted as a duplicate (true negative)\n",
    "\n",
    "# Note: if predicted_duplicates_dict[X_test['bug_id'].values[i]] contains -1, and true_duplicates_dict[X_test['bug_id'].values[i]] contains -1, then it is a true negative\n",
    "# if predicted_duplicates_dict[X_test['bug_id'].values[i]] contains -1, and true_duplicates_dict[X_test['bug_id'].values[i]] contains a bug_id, then it is a false negative\n",
    "# if predicted_duplicates_dict[X_test['bug_id'].values[i]] contains a bug_id, and true_duplicates_dict[X_test['bug_id'].values[i]] contains -1, then it is a false positive\n",
    "# if predicted_duplicates_dict[X_test['bug_id'].values[i]] contains a bug_id, and true_duplicates_dict[X_test['bug_id'].values[i]] contains a bug_id, then it is a true positive\n",
    "\n",
    "for key in predicted_duplicates_dict.keys():\n",
    "    if -1 in predicted_duplicates_dict[key] and -1 in true_duplicates_dict[key]:\n",
    "        true_negative += 1\n",
    "    elif -1 in predicted_duplicates_dict[key] and -1 not in true_duplicates_dict[key]:\n",
    "        false_negative += 1\n",
    "    elif -1 not in predicted_duplicates_dict[key] and -1 in true_duplicates_dict[key]:\n",
    "        false_positive += 1\n",
    "    elif -1 not in predicted_duplicates_dict[key] and -1 not in true_duplicates_dict[key]:\n",
    "        for bug_id in predicted_duplicates_dict[key]:\n",
    "            if bug_id in true_duplicates_dict[key]:\n",
    "                true_positive += 1\n",
    "\n",
    "# calculate the precision, recall, and f1 score\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "accuracy_score = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "\n",
    "#print(\"Precision:\", precision)\n",
    "#print(\"Recall:\", recall)\n",
    "#print(\"F1 Score:\", f1)\n",
    "#print(\"Accuracy Score:\", accuracy_score)\n",
    "\n",
    "# write the results to a file\n",
    "with open(\"results.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy Score: \" + str(accuracy_score) + \"\\n\")\n",
    "    f.write(\"Precision: \" + str(precision) + \"\\n\")\n",
    "    f.write(\"Recall: \" + str(recall) + \"\\n\")\n",
    "    f.write(\"F1 Score: \" + str(f1) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
