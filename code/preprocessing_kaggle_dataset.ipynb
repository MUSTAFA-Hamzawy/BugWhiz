{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory & Construct the relative path to train.csv\n",
    "current_dir = os.getcwd() \n",
    "relative_path = os.path.join('..', 'data', 'kaggle_dataset','kaggle_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset = pd.read_csv(os.path.join(current_dir, relative_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118643, 3)\n"
     ]
    }
   ],
   "source": [
    "print(kaggle_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['owner', 'issue_title', 'description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(kaggle_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amit@chromium.org</td>\n",
       "      <td>Scrolling with some scroll mice (touchpad, etc...</td>\n",
       "      <td>Product Version      : &lt;see about:version&gt;URLs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jon@chromium.org</td>\n",
       "      <td>Proxy causes some or all network requests to fail</td>\n",
       "      <td>Product Version      : 0.2.149.27 (1583)URLs (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pfeldman@chromium.org</td>\n",
       "      <td>Web inspector button \"dock to main window\" doe...</td>\n",
       "      <td>Product Version      : chrome beta 1URLs (if a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jon@chromium.org</td>\n",
       "      <td>Habari admin interface is not rendered correctly</td>\n",
       "      <td>Product Version      : 0.2.149.27 (1583)URLs (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pkasting@chromium.org</td>\n",
       "      <td>Maximize on second larger monitor not working</td>\n",
       "      <td>Product Version      : 0.2.149.27URLs (if appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118638</th>\n",
       "      <td>navabi@chromium.org</td>\n",
       "      <td>Launch clank_qa recipes to the waterfall</td>\n",
       "      <td>We had git trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118639</th>\n",
       "      <td>bulach@chromium.org</td>\n",
       "      <td>data race in ThreadWatcherListTest</td>\n",
       "      <td>r255322 is culprithttp://build.chromium.org/p/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118640</th>\n",
       "      <td>pfeldman@chromium.org</td>\n",
       "      <td>window.console object should not be configurable</td>\n",
       "      <td>Recently sites have begun replacing window.con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118641</th>\n",
       "      <td>ernstm@chromium.org</td>\n",
       "      <td>Windows GPU bots failing on multiple tests</td>\n",
       "      <td>All Windows GPU bots are failing a variety of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118642</th>\n",
       "      <td>apavlov@chromium.org</td>\n",
       "      <td>CSS auto-complete suggestion list is incomplet...</td>\n",
       "      <td>UserAgent: Mozilla/5.0 (X11; Linux x86_64) App...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118643 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        owner  \\\n",
       "0           amit@chromium.org   \n",
       "1            jon@chromium.org   \n",
       "2       pfeldman@chromium.org   \n",
       "3            jon@chromium.org   \n",
       "4       pkasting@chromium.org   \n",
       "...                       ...   \n",
       "118638    navabi@chromium.org   \n",
       "118639    bulach@chromium.org   \n",
       "118640  pfeldman@chromium.org   \n",
       "118641    ernstm@chromium.org   \n",
       "118642   apavlov@chromium.org   \n",
       "\n",
       "                                              issue_title  \\\n",
       "0       Scrolling with some scroll mice (touchpad, etc...   \n",
       "1       Proxy causes some or all network requests to fail   \n",
       "2       Web inspector button \"dock to main window\" doe...   \n",
       "3        Habari admin interface is not rendered correctly   \n",
       "4           Maximize on second larger monitor not working   \n",
       "...                                                   ...   \n",
       "118638           Launch clank_qa recipes to the waterfall   \n",
       "118639                 data race in ThreadWatcherListTest   \n",
       "118640   window.console object should not be configurable   \n",
       "118641         Windows GPU bots failing on multiple tests   \n",
       "118642  CSS auto-complete suggestion list is incomplet...   \n",
       "\n",
       "                                              description  \n",
       "0       Product Version      : <see about:version>URLs...  \n",
       "1       Product Version      : 0.2.149.27 (1583)URLs (...  \n",
       "2       Product Version      : chrome beta 1URLs (if a...  \n",
       "3       Product Version      : 0.2.149.27 (1583)URLs (...  \n",
       "4       Product Version      : 0.2.149.27URLs (if appl...  \n",
       "...                                                   ...  \n",
       "118638                                We had git trouble   \n",
       "118639  r255322 is culprithttp://build.chromium.org/p/...  \n",
       "118640  Recently sites have begun replacing window.con...  \n",
       "118641  All Windows GPU bots are failing a variety of ...  \n",
       "118642  UserAgent: Mozilla/5.0 (X11; Linux x86_64) App...  \n",
       "\n",
       "[118643 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 118643 entries, 0 to 118642\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   owner        118643 non-null  object\n",
      " 1   issue_title  118642 non-null  object\n",
      " 2   description  118642 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(kaggle_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete duplicate data [if exist]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_dataset.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset = kaggle_dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_dataset.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118587, 3)\n"
     ]
    }
   ],
   "source": [
    "print(kaggle_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generates descriptive statistics of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>118587</td>\n",
       "      <td>118586</td>\n",
       "      <td>118586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2564</td>\n",
       "      <td>118012</td>\n",
       "      <td>116371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>estade@chromium.org</td>\n",
       "      <td>Skia image rebaseline</td>\n",
       "      <td>See the link to graphs below.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1249</td>\n",
       "      <td>38</td>\n",
       "      <td>1398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      owner            issue_title  \\\n",
       "count                118587                 118586   \n",
       "unique                 2564                 118012   \n",
       "top     estade@chromium.org  Skia image rebaseline   \n",
       "freq                   1249                     38   \n",
       "\n",
       "                           description  \n",
       "count                           118586  \n",
       "unique                          116371  \n",
       "top     See the link to graphs below.   \n",
       "freq                              1398  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generates descriptive statistics of the data\n",
    "# include=\"O\": This parameter is specifying that you want to include columns with object data type \n",
    "kaggle_dataset.describe(include = \"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "owner            2564\n",
       "issue_title    118012\n",
       "description    116371\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the number of unique values for each column in the DataFrame training_data.\n",
    "kaggle_dataset.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "number of unique titles is less than number of rows\n",
    "so there are duplicate titles\n",
    "remove rows of duplicate titles\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove duplicate titles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset = kaggle_dataset.drop_duplicates(subset='issue_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118013, 3)\n"
     ]
    }
   ],
   "source": [
    "print(kaggle_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "owner            2563\n",
       "issue_title    118012\n",
       "description    115812\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the number of unique values for each column in the DataFrame training_data.\n",
    "kaggle_dataset.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generates descriptive statistics of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owner</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>118013</td>\n",
       "      <td>118012</td>\n",
       "      <td>118012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2563</td>\n",
       "      <td>118012</td>\n",
       "      <td>115812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>estade@chromium.org</td>\n",
       "      <td>Scrolling with some scroll mice (touchpad, etc...</td>\n",
       "      <td>See the link to graphs below.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1243</td>\n",
       "      <td>1</td>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      owner  \\\n",
       "count                118013   \n",
       "unique                 2563   \n",
       "top     estade@chromium.org   \n",
       "freq                   1243   \n",
       "\n",
       "                                              issue_title  \\\n",
       "count                                              118012   \n",
       "unique                                             118012   \n",
       "top     Scrolling with some scroll mice (touchpad, etc...   \n",
       "freq                                                    1   \n",
       "\n",
       "                           description  \n",
       "count                           118012  \n",
       "unique                          115812  \n",
       "top     See the link to graphs below.   \n",
       "freq                              1396  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generates descriptive statistics of the data\n",
    "# include=\"O\": This parameter is specifying that you want to include columns with object data type \n",
    "kaggle_dataset.describe(include = \"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show the number of nulls in each column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owner          0\n",
      "issue_title    1\n",
      "description    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the number of null values in each column in the training data\n",
    "print(kaggle_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove rows with null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owner          0\n",
      "issue_title    0\n",
      "description    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove rows with missing values from the dataset\n",
    "kaggle_dataset.dropna(inplace=True)\n",
    "print(kaggle_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save cleaned data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path = os.path.join('..', 'data','kaggle_dataset' ,'cleaned_kaggle_dataset.csv')\n",
    "kaggle_dataset.to_csv(os.path.join(current_dir, relative_path), index=False) # exclude the DataFrame index from being saved to the CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of lines in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOUh0894Z2QM",
    "outputId": "8a05336a-818f-4882-b110-624020d0333e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the original file: 118644\n"
     ]
    }
   ],
   "source": [
    "file_path = 'classifier_data_0.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the original file:\", line_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the classifier_data_0 file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'classifier_data_0.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the classifier_data_0 file:\", column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of unique owners in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique owners in classifier_data_0 file: 2564\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def get_unique_owner_count(csv_file):\n",
    "    unique_owners = set()\n",
    "    with open(csv_file, 'rb') as file:\n",
    "        try:\n",
    "            filtered_lines = (line.decode('utf-8-sig') for line in file if b'\\x00' not in line)\n",
    "            csv_reader = csv.DictReader(filtered_lines)\n",
    "            for row in csv_reader:\n",
    "                unique_owners.add(row['owner'])\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "    return len(unique_owners)\n",
    "\n",
    "csv_file = 'classifier_data_0.csv'\n",
    "unique_owner_count = get_unique_owner_count(csv_file)\n",
    "print(\"Number of unique owners in classifier_data_0 file:\", unique_owner_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 5 elements in the description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 elements of the 'description' column:\n",
      "Product Version      : <see about:version>URLs (if applicable) :0.2.149.27Other browsers tested: Firefox / IEAdd OK or FAIL after other browsers where you have tested this issue:Safari 3:    Firefox 3: OK         IE 7:OKWhat steps will reproduce the problem?1. Open any webpage on compaq 6715s running vista.2. Try scrolling with the touchpad3. Scrolling down will work , but up will not.What is the expected result?The page to scroll up.What happens instead?The page doesn't move.Please provide any additional information below. Attach a screenshot if possible.Only a minor bug. \n",
      "Product Version      : 0.2.149.27 (1583)URLs (if applicable) : http://www.igoogle.com,http://code.google.com/p/chromiumOther browsers tested:Add OK or FAIL after other browsers where you have tested this issue:Safari 3:    Firefox 3: OK         IE 7: OKWhat steps will reproduce the problem?1. Load http://www.igoogle.com/ (or any other google account page)2. Click the Sign In link at top right3. Kaboom.What is the expected result?Should see the sign-in screenWhat happens instead?Don't see nuffin'.  Just a white screenPlease provide any additional information below. Attach a screenshot ifpossible.* Using Windows Vista Enterprise.* I am connecting to the internet through a proxy server (I'm at work);could possibly be that?  Not sure what else to configure since Chrome seemsto use Vista's standard Internet Options (and those are configuredcorrectly, at least as far as IE and FireFox are concerned).  I have noticed that I get a password prompt from my Proxy server whenever Istart Chrome.  Will try downloading Chrome at home tonight to see if the problem persists. \n",
      "Product Version      : chrome beta 1URLs (if applicable) :Other browsers tested:Add OK or FAIL after other browsers where you have tested this issue:     Safari 3: OK    Firefox 3: irrelevant         IE 7: irrelevantWhat steps will reproduce the problem?1. right-click on a web element2. click on \"inspect element\"3. click on \"dock to main window\"What is the expected result?To have the web-inspector get docked at the bottom of the current tab.What happens instead?Nothing.Please provide any additional information below. Attach a screenshot if possible. \n",
      "Product Version      : 0.2.149.27 (1583)URLs (if applicable) :Other browsers tested:Add OK or FAIL after other browsers where you have tested this issue:     Safari 3: OK    Firefox 3: OK         IE 7: N/AWhat steps will reproduce the problem?1. Log in to Habari's admin interface2. Notice the incorrect rendering of the rounded input boxes upon login3. Hover over the menu in the upper left corner and notice the opaque blur around it. Looks really strange.What is the expected result?Smooth graphics.What happens instead?Weird graphics.Please provide any additional information below. Attach a screenshot if possible.Screenshot attached. Black squares obviously for obfuscation. \n",
      "Product Version      : 0.2.149.27URLs (if applicable) :What steps will reproduce the problem?1. Open Chrome on the primary monitor which has a smaller resolution2. Move Chrome to the second larger resolution monitor.3. Try to maximize and will only maximize to the resolution of the first smaller monitorWhat is the expected result?Maximize to the full resolution of the monitor that it is on. \n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def print_description_elements(csv_file, num_elements=5):\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        print(f\"First {num_elements} elements of the 'description' column:\")\n",
    "        for row in csv_reader:\n",
    "            print(row['description'])\n",
    "            num_elements -= 1\n",
    "            if num_elements == 0:\n",
    "                break\n",
    "\n",
    "csv_file = 'classifier_data_0.csv'\n",
    "print_description_elements(csv_file, num_elements=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 5 elements in the title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 elements of the 'title' column:\n",
      "Scrolling with some scroll mice (touchpad, etc.) scrolls down but not up\n",
      "Proxy causes some or all network requests to fail\n",
      "Web inspector button \"dock to main window\" does nothing\n",
      "Habari admin interface is not rendered correctly\n",
      "Maximize on second larger monitor not working\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def print_title_elements(csv_file, num_elements=5):\n",
    "    with open(csv_file, 'r', encoding='utf-8-sig') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        print(f\"First {num_elements} elements of the 'title' column:\")\n",
    "        for row in csv_reader:\n",
    "            print(row['issue_title'])\n",
    "            num_elements -= 1\n",
    "            if num_elements == 0:\n",
    "                break\n",
    "\n",
    "csv_file = 'classifier_data_0.csv'\n",
    "print_title_elements(csv_file, num_elements=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows saved to 'output_csv_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def filter_non_empty_rows(input_csv, output_csv):\n",
    "    with open(input_csv, 'r', encoding='utf-8-sig') as file:\n",
    "        # reads all the lines from the file into a list.\n",
    "        lines = file.readlines()\n",
    "        # Filter out lines containing NUL characters\n",
    "        lines = [line for line in lines if '\\0' not in line]\n",
    "    # Open the output CSV file for writing\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        csv_writer = csv.writer(output_file)\n",
    "        csv_writer.writerow(['owner', 'issue_title'])  # Write header\n",
    "        \n",
    "        # iterates over each line from the filtered list of lines.\n",
    "        for line in lines:\n",
    "            row = line.strip().split(',')\n",
    "            if row[0] and row[1]:  # Check if both columns are not empty\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "input_csv_file = 'classifier_data_0.csv'\n",
    "output_csv_file = 'output_csv_file.csv'\n",
    "\n",
    "filter_non_empty_rows(input_csv_file, output_csv_file)\n",
    "print(f\"Filtered rows saved to '{output_csv_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows saved to 'output_csv_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def filter_non_empty_rows(input_csv, output_csv, delimiter=','):\n",
    "    \"\"\"\n",
    "    Filters rows from a CSV file where the first two columns are not empty \n",
    "    and saves them to another CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to the output CSV file.\n",
    "        delimiter (str, optional): Delimiter used in the CSV file (default is comma ',').\n",
    "    \"\"\"\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8-sig') as file:\n",
    "        lines = file.readlines()\n",
    "        # Filter out lines containing NUL characters\n",
    "        lines = [line for line in lines if '\\0' not in line]\n",
    "\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        csv_writer = csv.writer(output_file, delimiter=delimiter)\n",
    "\n",
    "        # Write header row if it exists in the input file \n",
    "        # (assuming the first line is the header)\n",
    "        header_row = next(csv.reader(open(input_csv, 'r', encoding='utf-8-sig'), delimiter=delimiter))\n",
    "        if header_row:\n",
    "            csv_writer.writerow(header_row)\n",
    "\n",
    "        for line in lines:\n",
    "            row = line.strip().split(delimiter)  # Split based on delimiter\n",
    "\n",
    "            # Check if the first two columns are not empty\n",
    "            if row[0] and row[1]:\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "input_csv_file = 'classifier_data_0.csv'\n",
    "output_csv_file = 'output_csv_file.csv'\n",
    "\n",
    "filter_non_empty_rows(input_csv_file, output_csv_file)\n",
    "print(f\"Filtered rows saved to '{output_csv_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the filtered file: 118567\n"
     ]
    }
   ],
   "source": [
    "file_path = 'output_csv_file.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the filtered file:\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV filtered_csv_file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'output_csv_file.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV filtered_csv_file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows saved to 'filtered_csv_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def filter_rows_by_owner_count(input_csv, output_csv, min_owner_count):\n",
    "    \"\"\"\n",
    "    Filters rows from a CSV file based on the minimum occurrence count of an owner.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to the output CSV file where filtered rows will be saved.\n",
    "        min_owner_count (int): Minimum number of times an owner must appear in the input file to be included in the output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count occurrences of each owner\n",
    "    owner_counts = Counter()\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8-sig') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            if len(row) == len(header):  # Ensure the row has the correct number of elements\n",
    "                owner_counts[row[0]] += 1  # Assuming 'owner' is the first column\n",
    "\n",
    "    # Filter rows with owners that occur at least min_owner_count times\n",
    "    filtered_rows = []\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8-sig') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        header = next(csv_reader)  # Skip header\n",
    "        filtered_rows.append(header)\n",
    "        for row in csv_reader:\n",
    "            if len(row) == len(header) and owner_counts[row[0]] >= min_owner_count:\n",
    "                filtered_rows.append(row)\n",
    "\n",
    "    # Write filtered rows to the output CSV file (including all columns)\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as file:\n",
    "        csv_writer = csv.writer(file)\n",
    "        csv_writer.writerows(filtered_rows)  # Write all rows, not just specific columns\n",
    "\n",
    "input_csv_file = 'output_csv_file.csv'\n",
    "output_csv_file = 'filtered_csv_file.csv'\n",
    "min_owner_count = 5  # Minimum count of occurrences for an owner to be kept\n",
    "\n",
    "filter_rows_by_owner_count(input_csv_file, output_csv_file, min_owner_count)\n",
    "print(f\"Filtered rows saved to '{output_csv_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV filtered_csv_file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'filtered_csv_file.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV filtered_csv_file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV classifier_data_20 file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'classifier_data_20.csv'\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV classifier_data_20 file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the filtered file: 35619\n"
     ]
    }
   ],
   "source": [
    "file_path = 'filtered_csv_file.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the filtered file:\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV deep_data file: ['id', 'issue_id', 'issue_title', 'reported_time', 'owner', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'deep_data.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV deep_data file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged files saved to 'merged_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def merge_csv_files(input_files, output_file):\n",
    "    # Open the output CSV file in write mode\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as output_csv:\n",
    "        csv_writer = csv.writer(output_csv)\n",
    "\n",
    "        # Iterate over each input file\n",
    "        for input_file in input_files:\n",
    "            # Open the input CSV file\n",
    "            with open(input_file, 'r', newline='', encoding='utf-8') as input_csv:\n",
    "                # Filter out lines containing NUL characters\n",
    "                filtered_lines = (line for line in input_csv if '\\0' not in line)\n",
    "                csv_reader = csv.reader(filtered_lines)\n",
    "                \n",
    "                # Write the rows from the input file to the output file\n",
    "                csv_writer.writerows(csv_reader)\n",
    "\n",
    "# List of input CSV files to merge\n",
    "input_files = ['filtered_csv_file.csv', 'classifier_data_5.csv', 'classifier_data_10.csv', 'classifier_data_20.csv']\n",
    "\n",
    "# Output CSV file where merged content will be written\n",
    "output_file = 'merged_file.csv'\n",
    "\n",
    "# Merge the CSV files\n",
    "merge_csv_files(input_files, output_file)\n",
    "\n",
    "print(f\"Merged files saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV file: ['owner', 'issue_title', 'description']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'merged_file.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the merged file: 377053\n"
     ]
    }
   ],
   "source": [
    "file_path = 'merged_file.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the merged file:\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered, merged, and cleaned CSV saved to: filtered_merged_cleaned_output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "def merge_filter_save_csv(input_csv, output_csv, min_word_count=10):\n",
    "    \"\"\"\n",
    "    Merges 'issue_title' and 'description' columns into a new 'Summary' column,\n",
    "    removes special characters, newlines, and hyperlinks from the Summary,\n",
    "    filters rows where the Summary has at least min_word_count words,\n",
    "    and saves the results to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to the output CSV file.\n",
    "        min_word_count (int, optional): Minimum word count for Summary column. Defaults to 10.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(input_csv, 'r', newline='') as infile, open(output_csv, 'w', newline='') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "\n",
    "        # Read header row\n",
    "        header = next(reader)\n",
    "\n",
    "        # Identify indices of 'issue_title' and 'description' columns\n",
    "        title_index = header.index('issue_title')\n",
    "        desc_index = header.index('description')\n",
    "\n",
    "        # Update header with 'Summary'\n",
    "        header.insert(header.index('description') + 1, 'Summary')  # Insert before 'description'\n",
    "        del header[desc_index]  # Remove 'description'\n",
    "        del header[title_index]  # Remove 'issue_title'\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Define regular expressions for special characters, newlines, and hyperlinks\n",
    "        special_char_pattern = r\"[^\\w\\s]\"\n",
    "        url_pattern = r\"(http|https)?://[^\\s]+?\"  # Matches URLs with optional protocol (http/https)\n",
    "\n",
    "        # Process data rows\n",
    "        for row in reader:\n",
    "            summary = row[title_index] + \" \" + row[desc_index]  # Merge text with space\n",
    "\n",
    "            # Clean the Summary text\n",
    "            clean_summary = re.sub(special_char_pattern, \"\", summary)\n",
    "            clean_summary = clean_summary.replace('\\n', ' ')  # Replace newline with space\n",
    "            clean_summary = re.sub(url_pattern, \"\", clean_summary)  # Remove hyperlinks\n",
    "\n",
    "            word_count = len(clean_summary.split())\n",
    "\n",
    "            if word_count >= min_word_count:\n",
    "                row.insert(desc_index + 1, clean_summary)  # Insert cleaned Summary before 'description'\n",
    "                del row[desc_index]  # Remove 'description'\n",
    "                del row[title_index]  # Remove 'issue_title'\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Example usage\n",
    "input_csv_file = \"merged_file.csv\"\n",
    "output_csv_file = \"filtered_merged_cleaned_output.csv\"\n",
    "\n",
    "merge_filter_save_csv(input_csv_file, output_csv_file)\n",
    "print(f\"Filtered, merged, and cleaned CSV saved to: {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV file: ['owner', 'Summary']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'merged_file_after_filteration.csv'\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "    \n",
    "    # Read the first row of the CSV file\n",
    "    column_names = next(csv_reader)\n",
    "\n",
    "print(\"Column names in the CSV file:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'merged_file_after_filteration.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_file_after_filteration.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Open the file in read mode\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Use a loop to iterate over each line in the file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     line_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of lines in the merged file after filteration:\u001b[39m\u001b[38;5;124m\"\u001b[39m, line_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'merged_file_after_filteration.csv'"
     ]
    }
   ],
   "source": [
    "file_path = 'merged_file_after_filteration.csv'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Use a loop to iterate over each line in the file\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(\"Number of lines in the merged file after filteration:\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('merged_file_after_filteration.csv')\n",
    "\n",
    "# Calculate the split indices\n",
    "total_rows = len(df)\n",
    "split_indices = [0] + [total_rows * i // 4 for i in range(1, 4)] + [total_rows]\n",
    "\n",
    "# Split the DataFrame into four parts and save each part to a separate CSV file\n",
    "for i in range(4):\n",
    "    start_index = split_indices[i]\n",
    "    end_index = split_indices[i+1]\n",
    "    df_part = df.iloc[start_index:end_index]\n",
    "    df_part.to_csv(f'merged_file_part_{i+1}.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
