{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "837wtVlaDoAm",
        "outputId": "f59d2f01-7d51-4c9d-8c4b-9b5e1ab765e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfRgNUd49jjs",
        "outputId": "bf463162-f735-4c28-f777-2a423e5d6f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.24.1\n",
            "  Downloading scikit-learn-0.24.1.tar.gz (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.1) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.1) (1.11.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.24.1) (3.5.0)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for scikit-learn \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for scikit-learn (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build scikit-learn\n",
            "\u001b[31mERROR: Could not build wheels for scikit-learn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==0.24.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dDi2yE1u6d3",
        "outputId": "fdbbb6d9-96c9-4cbe-aa37-3f9818bd4009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting predict_developers.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile predict_developers.py\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import joblib\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Remove stop words from the bug report\n",
        "def remove_stopwords(tokens, stop_words):\n",
        "    return [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Apply stemming on the bug tokens\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Get the class using the svm_classifier\n",
        "def predict_summary(summary, tfidf_vectorizer, svm_classifier):\n",
        "    # Preprocess the input summary\n",
        "    summary_tfidf = tfidf_vectorizer.transform(summary)\n",
        "\n",
        "    # Predict the class\n",
        "    predicted_class = svm_classifier.predict(summary_tfidf)\n",
        "\n",
        "    return predicted_class[0]\n",
        "\n",
        "# Define the prediction function to get top 5 classes\n",
        "def predict_top_5_classes(summary, tfidf_vectorizer, svm_classifier, label_encoder):\n",
        "    summary_tfidf = tfidf_vectorizer.transform([' '.join(summary)])\n",
        "    probabilities = svm_classifier.predict_proba(summary_tfidf)[0]\n",
        "    top_5_indices = np.argsort(probabilities)[-5:][::-1]\n",
        "    return set(top_5_indices)\n",
        "\n",
        "def Inference(bug_report, stop_words, stemmer, tfidf_vectorizer, svm_classifier, label_encoder):\n",
        "    # Apply tokenization\n",
        "    tokens = word_tokenize(bug_report)\n",
        "    # Remove stop words\n",
        "    remove_stopwords(tokens, stop_words)\n",
        "    # Apply stemming\n",
        "    stem_tokens(tokens, stemmer)\n",
        "    # Predict the top 5 classes using the svm_classifier after applying TF-IDF\n",
        "    top_5_classes = predict_top_5_classes(tokens, tfidf_vectorizer, svm_classifier, label_encoder)\n",
        "\n",
        "    return top_5_classes\n",
        "\n",
        "def main():\n",
        "    # Prepare tokenizer, stemmer and stop words**\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Load the claddifier, TF-IDF vectorizer and label encoder\n",
        "    svm_classifier = joblib.load('/content/drive/My Drive/checkpoints/svm_classifier_model_with_probability.joblib')\n",
        "    tfidf_vectorizer = joblib.load('/content/drive/My Drive/checkpoints/tfidf_vectorizer.joblib')\n",
        "    label_encoder = joblib.load('/content/drive/My Drive/checkpoints/label_encoder.joblib')\n",
        "\n",
        "    # Prepare the data\n",
        "    data = json.loads(sys.argv[1])\n",
        "    # Extract bug description\n",
        "    bug_description = data[\"bugDescription\"]\n",
        "\n",
        "    input_bug_top_5_classes = Inference(bug_description, stop_words, stemmer, tfidf_vectorizer, svm_classifier, label_encoder)\n",
        "\n",
        "    # Create a dictionary to store the results\n",
        "    developers_bugs_classes = {}\n",
        "\n",
        "    developers_with_no_bugs = set()\n",
        "    # Extract developers' data\n",
        "    for developer in data[\"developersData\"]:\n",
        "        developer_id = developer[\"developerID\"]\n",
        "        old_bugs = developer[\"oldBugsDescription\"]\n",
        "        # Check if the bug list is empty\n",
        "        if not old_bugs:\n",
        "            developers_with_no_bugs.add(developer_id)\n",
        "            continue\n",
        "\n",
        "        common_classes_count = 0\n",
        "        for bug in old_bugs:\n",
        "            predicted_top_5_classes = Inference(bug, stop_words, stemmer, tfidf_vectorizer, svm_classifier, label_encoder)\n",
        "            common_classes = input_bug_top_5_classes.intersection(predicted_top_5_classes)\n",
        "            common_classes_count += len(common_classes)\n",
        "        developers_bugs_classes[developer_id] = common_classes_count\n",
        "\n",
        "    # Sort developers by common class counts in descending order\n",
        "    sorted_developers = sorted(developers_bugs_classes.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    # Get the top developers (at most 5)\n",
        "    num_top_developers = min(5, len(sorted_developers))\n",
        "\n",
        "    # Get the top 5 developers\n",
        "    recommended_developers = [developer_id for developer_id, _ in sorted_developers[:num_top_developers]]\n",
        "\n",
        "    # If there are developers that didn't solve bugs before ==> add them at the end of the list so they try to solve bugs\n",
        "    if developers_with_no_bugs:\n",
        "      if(len(recommended_developers)==5):\n",
        "        # replace the last recommendation in the recommended develoeprs with a developer from the team that didn't solve a bug before\n",
        "        recommended_developers[4] = developers_with_no_bugs.pop()\n",
        "      else:\n",
        "        # if the recommended_developers list is less than 5 recommendations ==> add developers with no bugs till their set ends or the recommendations list reaches 5 developers\n",
        "        while(len(recommended_developers) < 5 and developers_with_no_bugs):\n",
        "          recommended_developers.append(developers_with_no_bugs.pop())\n",
        "\n",
        "    # Print the results\n",
        "    print(\"Recommended Developers:\", recommended_developers)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s96__oNmztih",
        "outputId": "77a2580e-8c2b-4747-fccd-afdbd4c26d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator SVC from version 0.24.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.24.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.24.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Input Bug Top 5 Classes: {1536, 1, 1445, 726, 440}\n",
            "Developers with no bugs: {'667a76f471631147e0b6e0deedd'}\n",
            "Developers' Bugs Classes and Common Counts: {'667a760a6da0c47fe0a327cd': 5, '667a76f471631147e0b6e0d6': 0}\n",
            "Recommended Developers: ['667a760a6da0c47fe0a327cd', '667a76f471631147e0b6e0d6', '667a76f471631147e0b6e0d', '667a76f471631147e0b6e0ddd', '667a76f471631147e0b6e0dbbd']\n"
          ]
        }
      ],
      "source": [
        "!python predict_developers.py '{\"bugDescription\": \"Maximize on second larger monitor not working \", \"developersData\": [{\"developerID\": \"667a76f471631147e0b6e0d\", \"jobTitle\": \"Backend Developer\", \"oldBugsDescription\": []},{\"developerID\": \"667a76f471631147e0b6e0ddd\", \"jobTitle\": \"Backend Developer\", \"oldBugsDescription\": []},{\"developerID\": \"667a76f471631147e0b6e0dbbd\", \"jobTitle\": \"Backend Developer\", \"oldBugsDescription\": []},{\"developerID\": \"667a76f471631147e0b6e0deedd\", \"jobTitle\": \"Backend Developer\", \"oldBugsDescription\": []},{\"developerID\": \"667a760a6da0c47fe0a327cd\", \"jobTitle\": \"Backend Developer\", \"oldBugsDescription\": [\"Maximize on second larger monitor not working\", \"the font size is very small\"]}, {\"developerID\": \"667a76f471631147e0b6e0d6\", \"jobTitle\": \"Backend Developer\", \"oldBugsDescription\": [\"Manual guide installation is not clear\"]}]}'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}