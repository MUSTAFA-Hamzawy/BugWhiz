{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords corpus if not already downloaded\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths\n",
    "directories = [\n",
    "    (\"..\\\\new_dataset\\\\eclipse\", \"..\\\\BOW_dataset\\\\eclipse\"),\n",
    "    (\"..\\\\new_dataset\\\\firefox\", \"..\\\\BOW_dataset\\\\firefox\"),\n",
    "    (\"..\\\\new_dataset\\\\netbeans\", \"..\\\\BOW_dataset\\\\netbeans\"),\n",
    "    (\"..\\\\new_dataset\\\\openoffice\", \"..\\\\BOW_dataset\\\\openoffice\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess bug description text\n",
    "def preprocess_bug_description(description):\n",
    "\n",
    "    # Convert to lowercase\n",
    "    description = description.lower()\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    description = re.sub(r'[^a-zA-Z0-9\\s]', '', description)\n",
    "\n",
    "    # Remove numbers\n",
    "    description = re.sub(r'\\b\\d+\\b', '', description)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    description = re.sub(r'\\s+', ' ', description)\n",
    "\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Bug Reports\n",
    "def preprocess_bug_reports(bug_reports):\n",
    "    preprocessed_reports = []\n",
    "    for report in bug_reports:\n",
    "        # Implement your preprocessing steps here\n",
    "        preprocessed_report = preprocess_bug_description(report)\n",
    "        preprocessed_reports.append(preprocessed_report)\n",
    "    return preprocessed_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vocabulary and Generate BoW Vectors\n",
    "def generate_bow_vectors(bug_reports):\n",
    "    # Initialize CountVectorizer to create BoW vectors\n",
    "    vectorizer = CountVectorizer()\n",
    "    # Fit the vectorizer on preprocessed bug reports to build the vocabulary\n",
    "    vectorizer.fit(bug_reports)\n",
    "    # Transform bug reports into BoW vectors\n",
    "    bow_vectors = vectorizer.transform(bug_reports)\n",
    "    return bow_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Similarity\n",
    "def calculate_similarity(bow_vectors):\n",
    "    # Calculate cosine similarity between each pair of bug reports\n",
    "    num_reports = bow_vectors.shape[0]\n",
    "    similarities = []\n",
    "    for i in range(num_reports):\n",
    "        for j in range(i+1, num_reports):\n",
    "            similarity_score = cosine_similarity(bow_vectors[i], bow_vectors[j])[0][0]\n",
    "            similarities.append(similarity_score)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get bug descriptions based on bug IDs\n",
    "def get_bug_description(bug_id, file_data):\n",
    "\n",
    "    # convert to string\n",
    "    bug_id = str(bug_id)\n",
    "    \n",
    "    # if id is not found in the dataset, return None\n",
    "    if bug_id not in file_data['bug_id'].values:\n",
    "        return None\n",
    "    \n",
    "    return file_data[file_data['bug_id'] == bug_id]['description'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the common English stopwords\n",
    "common_stop_words = stopwords.words('english')\n",
    "\n",
    "# Use the common English stopwords as custom stop words\n",
    "custom_stop_words = common_stop_words\n",
    "\n",
    "# Initialize CountVectorizer with custom stop words\n",
    "vectorizer = CountVectorizer(stop_words=custom_stop_words)\n",
    "\n",
    "# Iterate over each directory\n",
    "for source_dir, target_dir in directories:\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for file_name in os.listdir(source_dir):\n",
    "\n",
    "        # Check if the file is a CSV file and contains \"pairs\" in its name\n",
    "        if file_name.endswith(\".csv\") and \"pairs\" in file_name:\n",
    "\n",
    "            # Load the CSV file\n",
    "            pairs_data = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "            print(\"pairs file Loaded:\", file_name)\n",
    "\n",
    "            # get the first word of pairs file before the _ character\n",
    "            temp = file_name.split(\"_\")[0]\n",
    "\n",
    "            # load the file with the same name as the pairs file\n",
    "            file_name = temp + \"_new.csv\"\n",
    "            bug_reports_file = pd.read_csv(os.path.join(source_dir, file_name))\n",
    "            print(\"original file Loaded:\", file_name)\n",
    "            \n",
    "            # Check if the DataFrame is empty (end of file reached)\n",
    "            if pairs_data.empty:\n",
    "                print(\"End of file reached for:\", file_name)\n",
    "                continue\n",
    "\n",
    "            # Create an empty DataFrame to store results\n",
    "            results_df = pd.DataFrame(columns=[\"issue_id\", \"duplicate_id\", \"similarity\"])\n",
    "            \n",
    "            # Iterate through each pair of bug reports\n",
    "            for index, row in pairs_data.iterrows():\n",
    "                issue_id = row['issue_id']\n",
    "                duplicate_id = row['duplicate']\n",
    "\n",
    "                if pd.isnull(duplicate_id):\n",
    "                    continue\n",
    "\n",
    "                duplicate_ids = row['duplicate'].split(';')\n",
    "\n",
    "                # Iterate through each duplicate ID\n",
    "                for duplicate_id in duplicate_ids:\n",
    "                    duplicate_id = duplicate_id.strip()  # Remove any spaces left or right    \n",
    "                    # print(type(issue_id))\n",
    "                    # print(type(duplicate_id))\n",
    "\n",
    "                    # print(f\"Comparing Bug {issue_id} and Bug {duplicate_id}\")\n",
    "\n",
    "                    # Get descriptions of the bugs\n",
    "                    description1 = get_bug_description(issue_id, bug_reports_file)\n",
    "                    description2 = get_bug_description(duplicate_id, bug_reports_file)\n",
    "                    # print(f\"Bug {issue_id} description: {description1}\")\n",
    "                    # print(f\"Bug {duplicate_id} description: {description2}\")\n",
    "\n",
    "                    # if one of the descriptions is None, skip the comparison    \n",
    "                    if description1 is None or description2 is None:\n",
    "                        print(\"One of the descriptions is not found in the dataset. Skipping the comparison.\")\n",
    "                        continue\n",
    "\n",
    "                    # Preprocess bug descriptions\n",
    "                    preprocessed_desc1 = preprocess_bug_description(description1)\n",
    "                    preprocessed_desc2 = preprocess_bug_description(description2)\n",
    "\n",
    "                    # Check if the documents contain only stop words\n",
    "                    if all(word in vectorizer.get_stop_words() for word in preprocessed_desc1.split()) or \\\n",
    "                        all(word in vectorizer.get_stop_words() for word in preprocessed_desc2.split()):\n",
    "                        print(\"Documents contain only stop words. Skipping the comparison.\")\n",
    "                        continue\n",
    "\n",
    "                    # Create BOW representation for each bug report\n",
    "                    bow_corpus = [preprocessed_desc1, preprocessed_desc2]\n",
    "                    bow_matrix = vectorizer.fit_transform(bow_corpus)\n",
    "\n",
    "                    # Get BOW vectors for each bug report\n",
    "                    bow1 = bow_matrix[0].toarray()\n",
    "                    bow2 = bow_matrix[1].toarray()\n",
    "\n",
    "                    # Compute cosine similarity between BOW representations\n",
    "                    similarity = cosine_similarity(bow1, bow2)[0][0]\n",
    "\n",
    "                    # print(f\"Similarity between Bug {issue_id} and Bug {duplicate_id}: {similarity}\")\n",
    "    \n",
    "                    # Append results to DataFrame\n",
    "                    results_df = results_df.append({\"issue_id\": issue_id, \"duplicate_id\": duplicate_id, \"similarity\": similarity}, ignore_index=True)\n",
    "            \n",
    "            # Save results DataFrame to a new CSV file in the target directory\n",
    "            new_file_path = os.path.join(target_dir, file_name.replace(\".csv\", \"_similarity.csv\"))\n",
    "            results_df.to_csv(new_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
